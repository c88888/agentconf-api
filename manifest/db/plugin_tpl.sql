/*
Navicat MySQL Data Transfer

Source Server         : n9e
Source Server Version : 50738
Source Host           : 116.211.139.40:3306
Source Database       : n9e_v5

Target Server Type    : MYSQL
Target Server Version : 50738
File Encoding         : 65001

Date: 2024-12-12 17:04:09
*/

SET FOREIGN_KEY_CHECKS=0;

-- ----------------------------
-- Table structure for plugin_tpl
-- ----------------------------
DROP TABLE IF EXISTS `plugin_tpl`;
CREATE TABLE `plugin_tpl` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT '''unique identifier''',
  `ident` varchar(191) NOT NULL COMMENT '''identifier of plugin_tpl''',
  `toml` text NOT NULL COMMENT '''toml of plugin_tpl''',
  `readme` text NOT NULL COMMENT '''readme of plugin_tpl''',
  `note` varchar(4096) NOT NULL COMMENT '''description of plugin_tpl''',
  `created_at` bigint(20) NOT NULL DEFAULT '0' COMMENT '''create time''',
  `created_by` varchar(191) NOT NULL DEFAULT '' COMMENT '''creator''',
  `updated_at` bigint(20) NOT NULL DEFAULT '0' COMMENT '''update time''',
  `updated_by` varchar(191) NOT NULL DEFAULT '' COMMENT '''updater''',
  PRIMARY KEY (`id`),
  KEY `idx_ident` (`ident`)
) ENGINE=InnoDB AUTO_INCREMENT=100 DEFAULT CHARSET=utf8mb4;

-- ----------------------------
-- Records of plugin_tpl
-- ----------------------------
INSERT INTO `plugin_tpl` VALUES ('67', 'MySQL', '# MySql监控,确保连接账户有只读相关权限：\n#GRANT PROCESS, REPLICATION CLIENT, SELECT ON *.* TO \'monitor\'@\'127.0.0.1\';\n\n#####如不关注下文的详细说明,可参考使用下面的配置;删除#注释,修改连接和lables信息\n\n#[[instances]]\n#address = \"127.0.0.1:3306\"\n#username = \"monitor\"\n#password = \"xxxxx\"\n#extra_status_metrics = true\n#extra_innodb_metrics = true\n#gather_processlist_processes_by_state = true\n#gather_processlist_processes_by_user = true\n#gather_schema_size = true\n#gather_table_size = false\n#gather_system_table_size = true\n#gather_slave_status = true\n#labels = { instance=\"yiletoo-172.20.26.88:33066\" }\n\n###如不关注下文的详细说明,可参考使用上面的配置;删除#注释,修改连接和lables信息\n#########################################################################\n\n####如果使用了上面的示例配置,以下所有的内容可以全部删除\n######################################################\n\n# 定义mysql采集周期，单位是秒\ninterval = 15\n\n# 定义全局要执行的自定义SQL，每个instance都会执行\n# 注意这里是 [[queries]]，而不是 [[instances.queries]]，[[queries]]是全局的，[[instances.queries]]是针对某个instance的\n#[[queries]]\n#mesurement = \"lock_wait\"\n#metric_fields = [ \"total\" ]\n#timeout = \"3s\"\n#request = \'\'\'\n#SELECT count(*) as total FROM information_schema.innodb_trx WHERE trx_state=\'LOCK WAIT\'\n#\'\'\'\n\n# 定义instance， 一个instance对应一个mysql实例\n# 指定连接信息,确保连接账户有只读相关权限；\n#GRANT PROCESS, REPLICATION CLIENT, SELECT ON *.* TO \'monitor\'@\'127.0.0.1\';\n\n[[instances]]\n#如果ip地址采集不通请使用sock文件连接\n#address = \"/webser/mysql57_33066/var/mysql57_33066.sock\"\n#address = \"127.0.0.1:3306\"\n#username = \"monitor\"\n#password = \"xxxxxxxx\"\n\n# 为 mysql 实例附加一些标签，比如上面的例子中提到的 instance 标签\nlabels = { instance=\"matrix-pro:3306\" }\n\n# 是否使用 tls 等定制参数\nparameters = \"tls=false\"\n\n# 通过 show global status 监控 mysql，默认抓取一些基础指标，\n# 如果想抓取更多 global status 的指标，把下面的配置设置为 true\n#extra_status_metrics = true\n\n# 通过show global variables 监控 mysql 的全局变量，默认抓取一些常规的\n# 常规的基本够用了，扩展的部分如果也想采集，下面的配置设置为 true\nextra_innodb_metrics = true\n\n# 监控 processlist，关注较少，默认不采集\n#gather_processlist_processes_by_state = false\n#gather_processlist_processes_by_user = false\n\n# 监控各个数据库的磁盘占用大小，如果你的 DB 很大，可能会很耗时，不建议采集，用处不大\n#gather_schema_size = false\n\n# 监控所有的 table 的磁盘占用大小，如果你的 DB 很大，可能会很耗时，不建议采集，用处不大\n#gather_table_size = false\n\n# 是否采集系统表的大小，通常不用，所以默认设置为false\n#gather_system_table_size = false\n\n# 通过 show slave status 监控 slave 的情况，比较关键，所以默认采集\n#gather_slave_status = true\n\n# 超时时间\n#timeout_seconds = 3\n\n# 采集周期的倍数，比如设置为2，那么采集周期就是 interval * 2\n#interval_times = 1\n\n# tls 相关配置，可选配置\n#use_tls = false\n#tls_min_version = \"1.2\"\n#tls_ca = \"/etc/categraf/ca.pem\"\n#tls_cert = \"/etc/categraf/cert.pem\"\n#tls_key = \"/etc/categraf/key.pem\"\n# Use TLS but skip chain & host verification\n#insecure_skip_verify = true\n\n# 定义只针对当前 mysql 实例的自定义 sql\n#[[instances.queries]]\n#mesurement = \"lock_wait\"\n#metric_fields = [ \"total\" ]\n#timeout = \"3s\"\n#request = \'\'\'\n#SELECT count(*) as total FROM information_schema.innodb_trx WHERE trx_state=\'LOCK WAIT\'\n#\'\'\'', '# mysql\n\nmysql 监控采集插件，核心原理就是连到 mysql 实例，执行一些 sql，解析输出内容，整理为监控数据上报。\n\n## Configuration\n\ncategraf 的 `conf/input.mysql/mysql.toml`\n\n```toml\n[[instances]]\n# 要监控 MySQL，首先要给出要监控的MySQL的连接地址、用户名、密码\naddress = \"127.0.0.1:3306\"\nusername = \"root\"\npassword = \"1234\"\n\n# # set tls=custom to enable tls\n# parameters = \"tls=false\"\n\n# 通过 show global status监控mysql，默认抓取一些基础指标，\n# 如果想抓取更多global status的指标，把下面的配置设置为true\nextra_status_metrics = true\n\n# 通过show global variables监控mysql的全局变量，默认抓取一些常规的\n# 常规的基本够用了，扩展的部分，默认不采集，下面的配置设置为false\nextra_innodb_metrics = false\n\n# 监控processlist，关注较少，默认不采集\ngather_processlist_processes_by_state = false\ngather_processlist_processes_by_user = false\n\n# 监控各个数据库的磁盘占用大小\ngather_schema_size = false\n\n# 监控所有的table的磁盘占用大小\ngather_table_size = false\n\n# 是否采集系统表的大小，通过不用，所以默认设置为false\ngather_system_table_size = false\n\n# 通过 show slave status监控slave的情况，比较关键，所以默认采集\ngather_slave_status = true\n\n# # timeout\n# timeout_seconds = 3\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# 为mysql实例附一个instance的标签，因为通过address=127.0.0.1:3306不好区分\n# important! use global unique string to specify instance\n# labels = { instance=\"n9e-10.2.3.4:3306\" }\n\n## Optional TLS Config\n# use_tls = false\n# tls_min_version = \"1.2\"\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = true\n\n# 自定义SQL，指定SQL、返回的各个列那些是作为metric，哪些是作为label\n# [[instances.queries]]\n# mesurement = \"users\"\n# metric_fields = [ \"total\" ]\n# label_fields = [ \"service\" ]\n# # field_to_append = \"\"\n# timeout = \"3s\"\n# request = \'\'\'\n# select \'n9e\' as service, count(*) as total from n9e_v5.users\n# \'\'\'\n```\n\n## 监控多个实例\n\n大家最常问的问题是如何监控多个mysql实例，实际大家对toml配置学习一下就了解了，`[[instances]]` 部分表示数组，是可以出现多个的，举例：\n\n```toml\n[[instances]]\naddress = \"10.2.3.6:3306\"\nusername = \"root\"\npassword = \"1234\"\nlabels = { instance=\"n9e-10.2.3.6:3306\" }\n\n[[instances]]\naddress = \"10.2.6.9:3306\"\nusername = \"root\"\npassword = \"1234\"\nlabels = { instance=\"zbx-10.2.6.9:3306\" }\n\n[[instances]]\naddress = \"/tmp/mysql.sock\"\nusername = \"root\"\npassword = \"1234\"\nlabels = { instance=\"zbx-localhost:3306\" }\n```\n\n## 监控大盘\n\n夜莺内置了 mysql 相关的监控大盘，内置了至少 4 个仪表盘：\n\n### mysql_by_categraf_instance\n\n这个大盘是使用 categraf 作为采集器，使用 instance 作为大盘变量，所以上例采集配置中都有一个 instance 的标签，就是和这个大盘配合使用的。\n\n### mysql_by_categraf_ident\n\n这个大盘是使用 categraf 作为采集器，使用 ident 作为大盘变量，即在查看 mysql 监控指标的时候，先通过大盘选中宿主机器，再通过机器找到 mysql 实例。\n\n### dashboard-by-aws-rds\n\n这是网友贡献的大盘，采集的 aws 的 rds 相关的数据制作的大盘。欢迎各位网友贡献大盘，这是一个很好的共建社区的方式。把您做好的大盘导出为 JSON，提 PR 到 [这个目录](https://github.com/ccfos/nightingale/tree/main/integrations/MySQL/dashboards) 下即可。\n\n### mysql_by_exporter\n\n这是使用 mysqld_exporter 作为采集器制作的大盘。\n\n## 告警规则\n\n夜莺内置了 mysql 相关的告警规则，克隆到自己的业务组即可使用。也欢迎大家一起来通过 PR 完善修改这个内置的 [告警规则](https://github.com/ccfos/nightingale/tree/main/integrations/MySQL/alerts)。', 'mysql监控采集插件，执行SQL采集监控数据', '1730797228', 'çŽ‹æ¨(822032277)', '1733909196', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('68', 'Redis', '# # collect interval\n# interval = 15\n\n[[instances]]\n# address = \"127.0.0.1:6379\"\n# username = \"\"\n# password = \"\"\n# pool_size = 2\n\n# # Optional. Specify redis commands to retrieve values\n# commands = [\n#     {command = [\"get\", \"sample-key1\"], metric = \"custom_metric_name1\"},\n#     {command = [\"get\", \"sample-key2\"], metric = \"custom_metric_name2\"}\n# ]\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# important! use global unique string to specify instance\n# labels = { instance=\"n9e-10.2.3.4:6379\" }\n\n## Optional TLS Config\n# use_tls = false\n# tls_min_version = \"1.2\"\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = true', '# redis\n\nredis 的监控原理，就是连上 redis，执行 info 命令，解析结果，整理成监控数据上报。\n\n## Configuration\n\nredis 插件的配置在 `conf/input.redis/redis.toml` 最简单的配置如下：\n\n```toml\n[[instances]]\naddress = \"127.0.0.1:6379\"\nusername = \"\"\npassword = \"\"\nlabels = { instance=\"n9e-10.23.25.2:6379\" }\n```\n\n如果要监控多个 redis 实例，就增加 instances 即可：\n\n```toml\n[[instances]]\naddress = \"10.23.25.2:6379\"\nusername = \"\"\npassword = \"\"\nlabels = { instance=\"n9e-10.23.25.2:6379\" }\n\n[[instances]]\naddress = \"10.23.25.3:6379\"\nusername = \"\"\npassword = \"\"\nlabels = { instance=\"n9e-10.23.25.3:6379\" }\n```\n\n建议通过 labels 配置附加一个 instance 标签，便于后面复用监控大盘。\n\n## 监控大盘和告警规则\n\n夜莺内置了 redis 的告警规则和监控大盘，克隆到自己的业务组下即可使用。\n\n## redis 集群如何监控\n\n其实，redis 集群的监控，还是去监控每个 redis 实例。\n\n如果一个 redis 集群有 3 个实例，对于业务应用来讲，发起一个请求，可能随机请求到某一个实例上去了，这个是没问题的，但是对于监控 client 而言，显然是希望到所有实例上获取数据的。\n\n当然，如果多个 redis 实例组成了集群，我们希望有个标识来标识这个集群，这个时候，可以通过 labels 来实现，比如给每个实例增加一个 redis_clus 的标签，值为集群名字即可。', 'redis监控指标采集', '1730799961', 'çŽ‹æ¨(822032277)', '1732762195', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('69', 'Ping', '# Ping监控\n# # collect interval\n# interval = 15\n\n#####如不关注下文的详细说明,可参考使用下面的配置;删除#注释,修改targets\n#注意列表最后一个元素结尾没有逗号\n[[instances]]\ntargets = [\n#     \"www.baidu.com\",\n#     \"127.0.0.1\",\n#     \"10.4.5.7\"\n]\n#默认使用原生方式，如需使用系统ping命令请解开下面的注释\n# method = \"exec\"\n# binary = \"ping\"\n# binary = \"ping6\"\n\n###如不关注下文的详细说明,可参考使用上面的配置;删除#注释,修改targets\n###############################################################\n\n####如果使用了上面的示例配置,以下所有内容可以全部删除###\n######################################################\n\n## Method used for sending pings, can be either \"exec\" or \"native\".  When set\n## to \"exec\" the systems ping command will be executed.  When set to \"native\"\n## the plugin will send pings directly.\n##\n## While the default is \"native\" for backwards compatibility, new deployments\n## are encouraged to use the \"native\" method for improved compatibility and\n## performance.\n# method = \"exec\"\n\n## Specify the ping executable binary.\n# binary = \"ping\"\n\n# 如需添加标签,请修改labels\n# labels = { region=\"cloud\", product=\"n9e\" }\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n## Number of ping packets to send per interval.  Corresponds to the \"-c\"\n## option of the ping command.\n# count = 1\n\n## Time to wait between sending ping packets in seconds.  Operates like the\n## \"-i\" option of the ping command.\n# ping_interval = 1.0\n\n## If set, the time to wait for a ping response in seconds.  Operates like\n## the \"-W\" option of the ping command.\n# timeout = 3.0\n\n## Interface or source address to send ping from.  Operates like the -I or -S\n## option of the ping command.\n# interface = \"\"\n\n## Use only IPv6 addresses when resolving a hostname.\n# ipv6 = false\n\n## Number of data bytes to be sent. Corresponds to the \"-s\"\n## option of the ping command.\n# size = 56\n\n# max concurrency coroutine\n# concurrency = 50', '# ping\n\nping 监控插件，探测远端目标地址能否 ping 通，如果机器没有禁 ping，这就是一个很好用的探测机器存活的手段\n\n## Configuration\n这个插件有两种主要的操作方法：`exec` 和 `native`.推荐使用 `native` 方法，因为它具有更好的系统兼容性和性能.\n为了向后兼容和更精准的response_ms,`native` 方法是默认的.\n使用 `method = \"exec\"`,将会调用系统ping程序来发送ping packets.\n\n要探测的机器配置到 targets 中，targets 是个数组，可以配置多个，当然也可以拆成多个 `[[instances]]` 配置段，比如：\n\n```\n[[instances]]\ntargets = [ \"10.4.5.6\" ]\nlabels = { region=\"cloud\", product=\"n9e\" }\n\n[[instances]]\ntargets = [ \"10.4.5.7\" ]\nlabels = { region=\"cloud\", product=\"zbx\" }\n```\n\n上例中是 ping 两个地址，为了信息更丰富，附加了 region 和 product 标签\n\n## File Limit\n\n```sh\nsystemctl edit categraf\n```\n\nIncrease the number of open files:\n\n```ini\n[Service]\nLimitNOFILE=8192\n```\n\nRestart Categraf:\n\n```sh\nsystemctl restart categraf\n```\n\n### Linux Permissions\n\nOn most systems, ping requires `CAP_NET_RAW` capabilities or for Categraf to be run as root.\n\nWith systemd:\n\n```sh\nsystemctl edit categraf\n```\n\n```ini\n[Service]\nCapabilityBoundingSet=CAP_NET_RAW\nAmbientCapabilities=CAP_NET_RAW\n```\n\n```sh\nsystemctl restart categraf\n```\n\nWithout systemd:\n\n```sh\nsetcap cap_net_raw=eip /usr/bin/categraf\n```\n\nReference [`man 7 capabilities`][man 7 capabilities] for more information about\nsetting capabilities.\n\n[man 7 capabilities]: http://man7.org/linux/man-pages/man7/capabilities.7.html\n\n### Other OS Permissions\n\nWhen using `method = \"native\"`, you will need permissions similar to the executable ping program for your OS.\n\n## 监控大盘和告警规则\n\n夜莺内置了告警规则和监控大盘，克隆到自己的业务组下即可使用。\n\n## Example Output\n\n```text\nping_maximum_response_ms agent_hostname=zy-fat product=n9e region=cloud target=10.0.24.136 0.036\nping_packets_transmitted agent_hostname=zy-fat product=n9e region=cloud target=10.0.24.136 1\nping_packets_received agent_hostname=zy-fat product=n9e region=cloud target=10.0.24.136 1\nping_average_response_ms agent_hostname=zy-fat product=n9e region=cloud target=10.0.24.136 0.036\nping_minimum_response_ms agent_hostname=zy-fat product=n9e region=cloud target=10.0.24.136 0.036\nping_standard_deviation_ms agent_hostname=zy-fat product=n9e region=cloud target=10.0.24.136 0\nping_result_code agent_hostname=zy-fat product=n9e region=cloud target=10.0.24.136 0\nping_percent_packet_loss agent_hostname=zy-fat product=n9e region=cloud target=10.0.24.136 0\nping_ttl agent_hostname=zy-fat product=n9e region=cloud target=10.0.24.136 64\nping_minimum_response_ms agent_hostname=zy-fat product=n9e region=cloud target=www.baidu.com 20.935\nping_average_response_ms agent_hostname=zy-fat product=n9e region=cloud target=www.baidu.com 20.935\nping_standard_deviation_ms agent_hostname=zy-fat product=n9e region=cloud target=www.baidu.com 0\nping_result_code agent_hostname=zy-fat product=n9e region=cloud target=www.baidu.com 0\nping_packets_transmitted agent_hostname=zy-fat product=n9e region=cloud target=www.baidu.com 1\nping_packets_received agent_hostname=zy-fat product=n9e region=cloud target=www.baidu.com 1\nping_ttl agent_hostname=zy-fat product=n9e region=cloud target=www.baidu.com 50\nping_percent_packet_loss agent_hostname=zy-fat product=n9e region=cloud target=www.baidu.com 0\nping_maximum_response_ms agent_hostname=zy-fat product=n9e region=cloud target=www.baidu.com 20.935\n```', 'ping 监控插件，探测远端目标地址能否 ping 通', '1730800231', 'çŽ‹æ¨(822032277)', '1733972372', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('70', 'Exec', '# # collect interval\n# interval = 15\n\n[[instances]]\n# # commands, support glob\ncommands = [\n#     \"/webser/app/categraf/scripts/*.sh\"\n]\n\n# # timeout for each command to complete\n# timeout = 5\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# # choices: influx prometheus falcon\n# # influx stdout example: mesurement,labelkey1=labelval1,labelkey2=labelval2 field1=1.2,field2=2.3\n# data_format = \"influx\"', '# 应用场景\n```\n应用于input插件库exec目录之外的特殊或自定义实现指定业务的监控。\n监控脚本采集到监控数据之后通过相应的格式输出到stdout，categraf截获stdout内容，解析之后传给服务端，\n脚本的输出格式支持3种：influx、falcon、prometheus，通过 exec.toml 的 `data_format` 配置告诉 Categraf。\ndata_format有3个值，其用法为：\n```\n\n## influx\n\ninflux 格式的内容规范：\n```\nmesurement,labelkey1=labelval1,labelkey2=labelval2 field1=1.2,field2=2.3\n```\n- 首先mesurement，表示一个类别的监控指标，比如 connections；\n- mesurement后面是逗号，逗号后面是标签，如果没有标签，则mesurement后面不需要逗号\n- 标签是k=v的格式，多个标签用逗号分隔，比如region=beijing,env=test\n- 标签后面是空格\n- 空格后面是属性字段，多个属性字段用逗号分隔\n- 属性字段是字段名=值的格式，在categraf里值只能是数字\n  最终，mesurement和各个属性字段名称拼接成metric名字\n\n## falcon\nOpen-Falcon的格式如下，举例：\n\n```json\n[\n    {\n        \"endpoint\": \"test-endpoint\",\n        \"metric\": \"test-metric\",\n        \"timestamp\": 1658490609,\n        \"step\": 60,\n        \"value\": 1,\n        \"counterType\": \"GAUGE\",\n        \"tags\": \"idc=lg,loc=beijing\",\n    },\n    {\n        \"endpoint\": \"test-endpoint\",\n        \"metric\": \"test-metric2\",\n        \"timestamp\": 1658490609,\n        \"step\": 60,\n        \"value\": 2,\n        \"counterType\": \"GAUGE\",\n        \"tags\": \"idc=lg,loc=beijing\",\n    }\n]\n```\ntimestamp、step、counterType，这三个字段在categraf处理的时候会直接忽略掉，endpoint会放到labels里上报。\n\n## prometheus\nprometheus 格式大家不陌生了，比如我这里准备一个监控脚本，输出 prometheus 的格式数据：\n```shell\n#!/bin/sh\n\necho \'# HELP demo_http_requests_total Total number of http api requests\'\necho \'# TYPE demo_http_requests_total counter\'\necho \'demo_http_requests_total{api=\"add_product\"} 4633433\'\n```\n其中 `#` 注释的部分，其实会被 categraf 忽略，不要也罢，prometheus 协议的数据具体的格式，请大家参考 prometheus 官方文档\n\n\n# 部署场景\n一般在复合型用途或独立的虚拟机启用此插件。\n\n# 前置条件\n```\n1.需使用人解读每个脚本或程序的逻辑，其脚本或程序顶部有大概作用的描述。\n```\n\n# 配置场景\n本配置启用或数据定义如下功能：\n增加自定义标签，可通过自定义标签筛选数据及更加精确的告警推送。\n响应超时时间为5秒。\ncommands字段正确应用脚本所在位置。\n\n# 修改exec.toml文件配置\n```\n[root@aliyun input.exec]# vi exec.toml\n\n# # collect interval\n# interval = 15\n\n[[instances]]\n# # commands, support glob\ncommands = [\n     \"/opt/categraf/scripts/*/collect_*.sh\"\n     #\"/opt/categraf/scripts/*/collect_*.py\"\n     #\"/opt/categraf/scripts/*/collect_*.go\"\n     #\"/opt/categraf/scripts/*/collect_*.lua\"\n     #\"/opt/categraf/scripts/*/collect_*.java\"\n     #\"/opt/categraf/scripts/*/collect_*.bat\"\n     #\"/opt/categraf/scripts/*/collect_*.cmd\"\n     #\"/opt/categraf/scripts/*/collect_*.ps1\"\n]\n\n# # timeout for each command to complete\n# timeout = 5\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# # mesurement,labelkey1=labelval1,labelkey2=labelval2 field1=1.2,field2=2.3\ndata_format = \"influx\"\n```\n\n#  测试配置\n```\n以cert/collect_cert_expiretime.sh为例：\nsh /opt/categraf/cert/collect_cert_expiretime.sh 出现：\ncert,cloud=huaweicloud,region=huabei-beijing-4,azone=az1,product=cert,domain_name=www.baidu.com expire_days=163\ncert,cloud=huaweicloud,region=huabei-beijing-4,azone=az1,product=cert,domain_name=www.weibo.com expire_days=85\ncert,cloud=huaweicloud,region=huabei-beijing-4,azone=az1,product=cert,domain_name=www.csdn.net expire_days=281\n```\n\n# 重启服务\n```\n重启categraf服务生效\nsystemctl daemon-reload && systemctl restart categraf && systemctl status categraf\n\n查看启动日志是否有错误\njournalctl -f -n 500 -u categraf | grep \"E\\!\" | grep \"W\\!\"\n```\n\n# 检查数据呈现\n如图：\n![image](https://user-images.githubusercontent.com/12181410/220940504-04c47faa-790a-42c1-b3dd-1510ae55c217.png)\n\n# 告警规则\n```\n脚本作用不同，规则就不同，先略过。\n```\n\n# 监控图表\n```\n脚本作用不同，规则就不同，先略过。\n```\n\n# 故障自愈\n```\n脚本作用不同，规则就不同，先略过。\n```\n', '运行指定命令或脚本自定义实现指定业务的监控', '1731052453', 'çŽ‹æ¨(822032277)', '1732759673', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('71', 'RabbitMQ', '# As of 3.8.0, RabbitMQ ships with built-in Prometheus & Grafana support.\n# Support for Prometheus metric collector ships in the rabbitmq_prometheus plugin.\n# The plugin exposes all RabbitMQ metrics on a dedicated TCP port, in Prometheus text format.\n#\n# enable prometheus plugin:\n# `rabbitmq-plugins enable rabbitmq_prometheus`\n# `curl http://localhost:15692/metrics`\n# \n# then use categraf prometheus plugin scrape http://localhost:15692/metrics instead of this rabbitmq plugin\n\n# # collect interval\n# interval = 15\n\n[[instances]]\n# # Management Plugin url\n# url = \"http://localhost:15672\"\n# username = \"guest\"\n# password = \"guest\"\n\n## Optional TLS Config\n# use_tls = false\n# tls_min_version = \"1.2\"\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = true\n\n## Optional request timeouts\n##\n## ResponseHeaderTimeout, if non-zero, specifies the amount of time to wait\n## for a server\'s response headers after fully writing the request.\n# header_timeout = \"3s\"\n##\n## client_timeout specifies a time limit for requests made by this client.\n## Includes connection time, any redirects, and reading the response body.\n# client_timeout = \"4s\"\n\n## A list of nodes to gather as the rabbitmq_node measurement. If not\n## specified, metrics for all nodes are gathered.\n# nodes = [\"rabbit@node1\", \"rabbit@node2\"]\n\n## A list of exchanges to gather as the rabbitmq_exchange measurement. If not\n## specified, metrics for all exchanges are gathered.\n# exchanges = [\"categraf\"]\n\n## Metrics to include and exclude. Globs accepted.\n## Note that an empty array for both will include all metrics\n## Currently the following metrics are supported: \"exchange\", \"federation\", \"node\", \"overview\", \"queue\"\n# metric_include = []\n# metric_exclude = []\n\n## Queues to include and exclude. Globs accepted.\n## Note that an empty array for both will include all queues\n# queue_name_include = []\n# queue_name_exclude = []\n\n## Federation upstreams to include and exclude specified as an array of glob\n## pattern strings.  Federation links can also be limited by the queue and\n## exchange filters.\n# federation_upstream_include = []\n# federation_upstream_exclude = []\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# important! use global unique string to specify instance\n# labels = { instance=\"rabbitmq-001\" }', '# RabbitMQ\n\n高版本（3.8以上版本）的 RabbitMQ，已经内置支持了暴露 Prometheus 协议的监控数据。所以，直接使用 categraf 的 prometheus 插件即可采集。开启 RabbitMQ Prometheus 访问：\n\n```bash\nrabbitmq-plugins enable rabbitmq_prometheus\n```\n\n启用成功的话，rabbitmq 默认会在 15692 端口起监听，访问 `http://localhost:15692/metrics` 即可看到符合 prometheus 协议的监控数据。\n\n如果低于 3.8 的版本，还是需要使用 categraf 的 rabbitmq 插件来采集监控数据。\n\n## 告警规则\n\n夜莺内置了 RabbitMQ 的告警规则，克隆到自己的业务组下即可使用。\n\n## 仪表盘\n\n夜莺内置了 RabbitMQ 的仪表盘，克隆到自己的业务组下即可使用。`rabbitmq_v3.8_gt` 是大于等于 3.8 版本的仪表盘，`rabbitmq_v3.8_lt` 是小于 3.8 版本的仪表盘。\n\n![20230802082542](https://download.flashcat.cloud/ulric/20230802082542.png)', 'RabbitMQ监控指标采集', '1732011995', 'admin888', '1732762205', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('72', 'AliYun', '# # collect interval\n# interval = 60\n[[instances]]\n# # endpoint region 参考 https://help.aliyun.com/document_detail/28616.html#section-72p-xhs-6qt\n# region=\"cn-beijing\"\n# endpoint=\"metrics.cn-hangzhou.aliyuncs.com\"\n# access_key_id=\"your-access-key-id\"\n# access_key_secret=\"your-access-key-secret\"\n# interval_times=4\n# delay=\"10m\"\n# period=\"60s\"\n# # namespace 参考 https://help.aliyun.com/document_detail/163515.htm?spm=a2c4g.11186623.0.0.44d65c58mhgNw3\n# namespaces=[\"acs_ecs_dashboard\"]\n# [[instances.metric_filters]]\n# # metric name 参考 https://help.aliyun.com/document_detail/163515.htm?spm=a2c4g.11186623.0.0.401d15c73Z0dZh\n# # 参考页面中的Metric Id 填入下面的metricName ,页面中包含中文的Metric Name对应接口中的Description\n# metric_names=[\"cpu_cores\",\"vm.TcpCount\"]\n# namespace=\"\"\n# ratelimit=25\n# catch_ttl=\"1h\"\n# timeout=\"5s\"', '# aliyun plugin\n\n## 简介\n\n使用[categraf](https://github.com/flashcatcloud/categraf)中[aliyun](https://github.com/flashcatcloud/categraf/tree/main/inputs/aliyun)插件拉取阿里云云监控的数据（通过 OpenAPI）。\n\n## 授权\n\n获取凭证 [https://usercenter.console.aliyun.com/#/manage/ak](https://usercenter.console.aliyun.com/#/manage/ak)\nRAM 用户授权。RAM 用户调用云监控 API 前，需要所属的阿里云账号将权限策略授予对应的 RAM 用户，参见 [RAM 用户权限](https://help.aliyun.com/document_detail/43170.html?spm=a2c4g.11186623.0.0.30c841feqsoAAn)。\n可以在 [授权页面](https://ram.console.aliyun.com/permissions) 新增授权，选择对应的用户，授予云监控只读权限 `AliyunCloudMonitorReadOnlyAccess`, 并为授予权限的用户创建accessKey 即可。\n\n## Categraf中conf/input.aliyun/cloud.toml配置文件：\n\n```toml\n# # categraf采集周期，阿里云指标的粒度一般是60秒，建议设置不要少于60秒\ninterval = 120\n[[instances]]\n## 阿里云资源所处的region\n## endpoint region 参考 https://help.aliyun.com/document_detail/28616.html#section-72p-xhs-6qt\nregion=\"cn-beijing\"\nendpoint=\"metrics.cn-hangzhou.aliyuncs.com\"\n## 填入你的access_key_id\naccess_key_id=\"\"\n## 填入你的access_key_secret\naccess_key_secret=\"\"\n\n## 可能无法获取当前最新指标，这个指标是指监控指标的截止时间距离现在多久\ndelay=\"50m\"\n## 阿里云指标的最小粒度，60s 是推荐值，再小了部分指标不支持\nperiod=\"60s\"\n## 指标所属的namespace ,为空，则表示所有空间指标都要采集\n## namespace 参考 https://help.aliyun.com/document_detail/163515.htm?spm=a2c4g.11186623.0.0.44d65c58mhgNw3\nnamespaces=[\"acs_ecs_dashboard\"]\n## 过滤某个namespace下的一个或多个指标\n## metric name 参考 https://help.aliyun.com/document_detail/163515.htm?spm=a2c4g.11186623.0.0.401d15c73Z0dZh\n## 参考页面中的Metric Id 填入下面的metricName ,页面中包含中文的Metric Name对应接口中的Description\n[[instances.metric_filters]]\nnamespace=\"\"\nmetric_names=[\"cpu_cores\",\"vm.TcpCount\", \"cpu_idle\"]\n\n# 阿里云查询指标接口的QPS是50， 这里默认设置为一半\nratelimit=25\n# 查询指定namesapce指标后, namespace/metric_name等meta信息会缓存起来，catch_ttl 是指标的缓存时间\ncatch_ttl=\"1h\"\n# 每次请求阿里云endpoint的超时时间\ntimeout=\"5s\"\n```\n\n## 效果图\n\n### aliyun ecs\n\n![ecs](http://download.flashcat.cloud/uPic/R6LOcO.jpg)\n\n### aliyun rds\n\n![rds](http://download.flashcat.cloud/uPic/rds.png)\n\n### aliyun redis\n\n![redis](http://download.flashcat.cloud/uPic/redis.png)\n\n### aliyun slb\n\n![slb](http://download.flashcat.cloud/uPic/slb.png)\n\n### aliyun waf\n\n![waf](http://download.flashcat.cloud/uPic/waf.png)', '拉取阿里云云监控的数据（通过 OpenAPI）', '1732759885', '王杨(822032277)', '1732759885', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('73', 'Dns_Query', '# # collect interval\n# interval = 15\n\n[[instances]]\n# # append some labels for series\n# labels = { region=\"cloud\", product=\"n9e\" }\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# # \nauto_detect_local_dns_server  = false\n\n## servers to query\n# servers = [\"8.8.8.8\"]\nservers = []\n\n## Network is the network protocol name.\n# network = \"udp\"\n\n## Domains or subdomains to query.\n# domains = [\".\"]\n\n## Query record type.\n## Possible values: A, AAAA, CNAME, MX, NS, PTR, TXT, SOA, SPF, SRV.\n# record_type = \"A\"\n\n## Dns server port.\n# port = 53\n\n## Query timeout in seconds.\n# timeout = 2', '# 应用场景\n一般用于对DNS服务器的响应监测，帮助运维快速定位网络问题。\n\n# 部署场景\n不需要每台虚拟机都启用此插件，建议是独立或复合的某一台虚拟机启用此插件。\n\n# 配置场景\n```\n本配置启用或数据定义如下功能：\n使用本机DNS查询域名解析质量。\n使用外部DNS查询域名解析质量。\n使用不同记录类型进行DNS查询。\n每种查询都设置超时时间5秒。\n增加自定义标签，可通过自定义标签筛选数据及更加精确的告警推送。\n在domains字段处增加自己想要被DNS查询的域名，一般填写公司业务系统的域名或第三方依赖的业务系统。\n```\n\n# 修改dns_query.toml文件配置\n\n``` 以下文件内容配置作为参考\n[root@aliyun input.dns_query]# cat dns_query.toml\n# # collect interval\n# interval = 15\n\n[[instances]]\n# # append some labels for series\nlabels = { cloud=\"huaweicloud\", region=\"huabei-beijing-4\",azone=\"az1\", product=\"n9e\" }\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# #\nauto_detect_local_dns_server  = true\n\n### A record\n\n## servers to query\nservers = [\"223.5.5.5\",\"114.114.114.114\",\"119.29.29.29\"]\n\n## Network is the network protocol name.\n# network = \"udp\"\n\n## Domains or subdomains to query.\ndomains = [\"www.huaweicloud.com\", \"www.baidu.com\", \"www.tapd.cn\"]\n\n## Query record type.\n## Possible values: A, AAAA, CNAME, MX, NS, PTR, TXT, SOA, SPF, SRV.\nrecord_type = \"A\"\n\n## Dns server port.\n# port = 53\n\n## Query timeout in seconds.\ntimeout = 5\n\n\n### CNAME record\n\n[[instances]]\n# # append some labels for series\nlabels = { cloud=\"huaweicloud\", region=\"huabei-beijing-4\",azone=\"az1\", product=\"n9e\" }\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# #\nauto_detect_local_dns_server  = false\n\n## servers to query\nservers = [\"223.5.5.5\",\"114.114.114.114\",\"119.29.29.29\"]\n\n## Network is the network protocol name.\n# network = \"udp\"\n\n## Domains or subdomains to query.\ndomains = [\"www.huaweicloud.com\", \"www.baidu.com\", \"www.tapd.cn\"]\n\n## Query record type.\n## Possible values: A, AAAA, CNAME, MX, NS, PTR, TXT, SOA, SPF, SRV.\nrecord_type = \"CNAME\"\n\n## Dns server port.\n# port = 53\n\n## Query timeout in seconds.\ntimeout = 5\n\n\n### NS record\n\n[[instances]]\n# # append some labels for series\nlabels = { cloud=\"huaweicloud\", region=\"huabei-beijing-4\",azone=\"az1\", product=\"n9e\" }\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# #\nauto_detect_local_dns_server  = false\n\n## servers to query\nservers = [\"223.5.5.5\",\"114.114.114.114\",\"119.29.29.29\"]\n\n## Network is the network protocol name.\n# network = \"udp\"\n\n## Domains or subdomains to query.\ndomains = [\"www.huaweicloud.com\", \"www.baidu.com\", \"www.tapd.cn\"]\n\n## Query record type.\n## Possible values: A, AAAA, CNAME, MX, NS, PTR, TXT, SOA, SPF, SRV.\nrecord_type = \"NS\"\n\n## Dns server port.\n# port = 53\n\n## Query timeout in seconds.\ntimeout = 5\n```\n\n# 测试配置\n```\n./categraf --test --inputs dns_query\n....... A记录同理就省略\n20:51:34 dns_query_rcode_value agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud domain=www.tapd.cn product=n9e record_type=CNAME region=huabei-beijing-4 server=119.29.29.29 0\n20:51:34 dns_query_result_code agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud domain=www.tapd.cn product=n9e record_type=CNAME region=huabei-beijing-4 server=119.29.29.29 0\n20:51:34 dns_query_query_time_ms agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud domain=www.tapd.cn product=n9e record_type=CNAME region=huabei-beijing-4 server=119.29.29.29 33.500371\n\n20:51:34 dns_query_rcode_value agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud domain=www.baidu.com product=n9e record_type=CNAME region=huabei-beijing-4 server=119.29.29.29 0\n20:51:34 dns_query_result_code agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud domain=www.baidu.com product=n9e record_type=CNAME region=huabei-beijing-4 server=119.29.29.29 0\n20:51:34 dns_query_query_time_ms agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud domain=www.baidu.com product=n9e record_type=CNAME region=huabei-beijing-4 server=119.29.29.29 34.328242\n\n20:51:34 dns_query_rcode_value agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud domain=www.huaweicloud.com product=n9e record_type=CNAME region=huabei-beijing-4 server=119.29.29.29 0\n20:51:34 dns_query_result_code agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud domain=www.huaweicloud.com product=n9e record_type=CNAME region=huabei-beijing-4 server=119.29.29.29 0\n20:51:34 dns_query_query_time_ms agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud domain=www.huaweicloud.com product=n9e record_type=CNAME region=huabei-beijing-4 server=119.29.29.29\n.....\n\n```\n# 重启服务\n```\n重启categraf服务生效\nsystemctl daemon-reload && systemctl restart categraf && systemctl status categraf\n\n查看启动日志是否有错误\njournalctl -f -n 500 -u categraf | grep \"E\\!\" | grep \"W\\!\"\n```\n\n# 检查数据呈现\n等待1-2分钟后数据就会在图表中展示出来，如图：\n![image](https://user-images.githubusercontent.com/12181410/220353480-e17a7822-7ccc-4fdf-b18b-a0be84cd5550.png)\n\n# 监控告警规则配置\n```\n个人经验仅供参考，一般DNS解析延迟时间：\n超过2000毫秒，为P2级别，启用企业微信应用推送告警，3分钟内恢复发出恢复告警。\n超过5000毫秒，为P1级别，启用电话语音告警&企业微信应用告警，3分钟内恢复发出恢复告警。\n\n为什么会这么考量设计？\n在用到DNS监控时，一般公司业务是遍布全国的，然而全国各个地区在解析DNS存在各种场景因素导致的DNS问题（如DNS被劫持、片区DNS服务器故障等），所以需要以高级别对待。\n从收到告警到恢复告警设置3分钟的意图是防止期间是短暂时间有问题,同时也给SLA(99.99%)给足处理时长。\n```\n\n# 监控图表配置\n```\n先略过\n```\n', '对DNS服务器的响应监测', '1732760185', '王杨(822032277)', '1732764723', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('74', 'Mtail', '[[instances]]\n# progs = \"/path/to/prog1\" # prog dir1\n# logs = [\"/path/to/a.log\", \"path/to/b.log\"]\n# override_timezone = \"Asia/Shanghai\"\n# emit_metric_timestamp = \"true\" #string type\n\n# [[instances]]\n# progs = \"/path/to/prog2\" # prog dir2\n# logs = [\"/path/to/logdir/\"]\n# override_timezone = \"Asia/Shanghai\"\n# emit_metric_timestamp = \"true\" # string type', '# mtail插件\n\n## 简介\n功能：提取日志内容，转换为监控metrics\n\n+ 输入： 日志\n+ 输出： metrics 按照mtail语法输出, 仅支持counter、gauge、histogram\n+ 处理： 本质是golang的正则提取+表达式计算\n\n## 启动\n编辑mtail.toml文件, 一般每个instance需要指定不同的progs参数（不同的progs文件或者目录）,否则指标会相互干扰。\n**注意**: 如果不同instance使用相同progs, 可以通过给每个instance增加labels做区分，\n```\nlabels = { k1=v1 }\n```\n或\n```\n[instances.labels]\nk1=v1\n```\n\n1. conf/inputs.mtail/mtail.toml中指定instance\n```toml\n\n[[instances]]\n## 指定mtail prog的目录\nprogs = \"/path/to/prog1\"\n## 指定mtail要读取的日志\nlogs = [\"/path/to/a.log\", \"path/to/b.log\"] \n## 指定时区\n# override_timezone = \"Asia/Shanghai\" \n## metrics是否带时间戳，注意，这里是\"true\"\n# emit_metric_timestamp = \"true\" \n\n```\n2. 在/path/to/prog1 目录下编写规则文件\n```\ngauge xxx_errors\n/ERROR.*/ {\n    xxx_errros++\n}\n```\n\n3. 一个tab中执行 `categraf --test --inputs mtail`，用于测试 \n4. 另一个tab中，\"/path/to/a.log\" 或者 \"path/to/b.log\" 追加一行 ERROR，看看categraf的输出\n5. 测试通过后，启动categraf\n\n### 输入\nlogs参数指定要处理的日志源, 支持模糊匹配, 支持多个log文件。\n\n### 处理规则\n`progs`指定具体的规则文件目录(或文件)\n\n\n## 处理规则与语法\n\n### 处理流程\n```python \nfor line in lines:\n  for regex in regexes:\n    if match:\n      do something\n```\n\n### 语法\n\n``` golang\nexported variable \n\npattern { \n  action statements\n} \n\ndef decorator { \n  pattern and action statements\n}\n```\n\n#### 定义指标名称\n前面也提过，指标仅支持 counter gauge histogram 三种类型。\n一个🌰\n```mtail\ncounter lines\n/INFO.*/ {\n    lines++\n}\n```\n\n注意，定义的名称只支持 C类型的命名方式(字母/数字/下划线)，**如果想使用\"-\" 要使用\"as\"导出别名**。例如，\n```mtail\ncounter lines_total as \"line-count\"\n```\n这样获取到的就是line-count这个指标名称了\n\n#### 匹配与计算（pattern/action)\n\n```mtail\nPATTERN {\nACTION\n}\n```\n\n例子\n```mtail\n/foo/ {\n  ACTION1\n}\n\nvariable > 0 {\n  ACTION2\n}\n\n/foo/ && variable > 0 {\n  ACTION3\n}\n```\n支持RE2正则匹配\n```mtail\nconst PREFIX /^\\w+\\W+\\d+ /\n\nPREFIX {\n  ACTION1\n}\n\nPREFIX + /foo/ {\n  ACTION2\n}\n```\n\n这样，ACTION1 是匹配以小写字符+大写字符+数字+空格的行，ACTION2 是匹配小写字符+大写字符+数字+空格+foo开头的行。\n\n#### 关系运算符\n+ `<` 小于 `<=` 小于等于\n+ `>` 大于 `>=` 大于等于\n+ `==` 相等 `!=` 不等\n+ `=~` 匹配(模糊) `!~` 不匹配(模糊)\n+ `||` 逻辑或 `&&` 逻辑与 `!` 逻辑非\n \n#### 数学运算符\n+ `|` 按位或\n+ `&` 按位与\n+ `^` 按位异或\n+ `+ - * /` 四则运算\n+ `<<` 按位左移\n+ `>>` 按位右移\n+ `**` 指数运算 \n+ `=` 赋值\n+ `++` 自增运算\n+ `--` 自减运算\n+ `+=` 加且赋值\n\n#### 支持else与otherwise\n```mtail\n/foo/ {\nACTION1\n} else {\nACTION2\n}\n```\n支持嵌套\n```mtail\n/foo/ {\n  /foo1/ {\n     ACTION1\n  }\n  /foo2/ {\n     ACTION2\n  }\n  otherwise {\n     ACTION3\n  }\n}\n```\n\n支持命名与非命名提取\n\n```mtail\n/(?P<operation>\\S+) (\\S+) \\[\\S+\\] (\\S+) \\(\\S*\\) \\S+ (?P<bytes>\\d+)/ {\n  bytes_total[$operation][$3] += $bytes\n}\n```\n增加常量label \n```mtail\n# test.mtail\n# 定义常量label env\nhidden text env\n# 给label 赋值 这样定义是global范围;\n# 局部添加，则在对应的condition中添加\nenv=\"production\"\ncounter line_total by logfile,env\n/^(?P<date>\\w+\\s+\\d+\\s+\\d+:\\d+:\\d+)/ {\n    line_total[getfilename()][env]++\n}\n```\n获取到的metrics中会添加上`env=production`的label 如下：\n```mtail\n# metrics\nline_total{env=\"production\",logfile=\"/path/to/xxxx.log\",prog=\"test.mtail\"} 4 1661165941788\n```\n\n如果要给metrics增加变量label，必须要使用命名提取。例如\n```python\n# 日志内容\n192.168.0.1 GET /foo\n192.168.0.2 GET /bar\n192.168.0.1 POST /bar\n```\n\n``` mtail\n# test.mtail\ncounter my_http_requests_total by log_file, verb \n/^/ +\n/(?P<host>[0-9A-Za-z\\.:-]+) / +\n/(?P<verb>[A-Z]+) / +\n/(?P<URI>\\S+).*/ +\n/$/ {\n    my_http_requests_total[getfilename()][$verb]++\n}\n```\n\n```python\n# metrics\nmy_http_requests_total{logfile=\"xxx.log\",verb=\"GET\",prog=\"test.mtail\"} 4242\nmy_http_requests_total{logfile=\"xxx.log\",verb=\"POST\",prog=\"test.mtail\"} 42\n```\n\n命名提取的变量可以在条件中使用\n```mtail\n/(?P<x>\\d+)/ && $x > 1 {\nnonzero_positives++\n}\n```\n\n#### 时间处理\n不显示处理，则默认使用系统时间\n\n默认emit_metric_timestamp=\"false\" （注意是字符串）\n```\nhttp_latency_bucket{prog=\"histo.mtail\",le=\"1\"} 0\nhttp_latency_bucket{prog=\"histo.mtail\",le=\"2\"} 0\nhttp_latency_bucket{prog=\"histo.mtail\",le=\"4\"} 0\nhttp_latency_bucket{prog=\"histo.mtail\",le=\"8\"} 0\nhttp_latency_bucket{prog=\"histo.mtail\",le=\"+Inf\"} 0\nhttp_latency_sum{prog=\"histo.mtail\"} 0\nhttp_latency_count{prog=\"histo.mtail\"} 0\n```\n\n参数 emit_metric_timestamp=\"true\" (注意是字符串)\n```\nhttp_latency_bucket{prog=\"histo.mtail\",le=\"1\"} 1 1661152917471\nhttp_latency_bucket{prog=\"histo.mtail\",le=\"2\"} 2 1661152917471\nhttp_latency_bucket{prog=\"histo.mtail\",le=\"4\"} 2 1661152917471\nhttp_latency_bucket{prog=\"histo.mtail\",le=\"8\"} 2 1661152917471\nhttp_latency_bucket{prog=\"histo.mtail\",le=\"+Inf\"} 2 1661152917471\nhttp_latency_sum{prog=\"histo.mtail\"} 3 1661152917471\nhttp_latency_count{prog=\"histo.mtail\"} 4 1661152917471\n```\n\n使用日志的时间\n```\nAug 22 15:28:32 GET /api/v1/pods latency=2s code=200\nAug 22 15:28:32 GET /api/v1/pods latency=1s code=200\nAug 22 15:28:32 GET /api/v1/pods latency=0s code=200\n```\n\n```\nhistogram http_latency buckets 1, 2, 4, 8\n/^(?P<date>\\w+\\s+\\d+\\s+\\d+:\\d+:\\d+)/ {\n        strptime($date, \"Jan 02 15:04:05\")\n	/latency=(?P<latency>\\d+)/ {\n		http_latency=$latency\n	}\n}\n```\n\n日志提取的时间，一定要注意时区问题，有一个参数 `override_timezone` 可以控制时区选择，否则默认使用UTC转换。\n比如我启动时指定 `override_timezone=Asia/Shanghai`, 这个时候日志提取的时间会当做东八区时间 转换为timestamp， 然后再从timestamp转换为各时区时间时 就没有问题了,如图。\n![timestamp](https://cdn.jsdelivr.net/gh/flashcatcloud/categraf@main/inputs/mtail/timestamp.png)\n如果不带 `override_timezone=Asia/Shanghai`, 则默认将`Aug 22 15:34:32` 当做UTC时间，转换为timestamp。 这样再转换为本地时间时，会多了8个小时, 如图。\n![timestamp](https://cdn.jsdelivr.net/gh/flashcatcloud/categraf@main/inputs/mtail/timezone.png)\n', '提取日志内容，转换为监控metrics', '1732760387', '王杨(822032277)', '1732760387', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('75', 'PHPFPM', '# # collect interval\n# interval = 15\n\n[[instances]]\n## An array of Nginx stub_status URI to gather stats.\nurls = [\n## HTTP: the URL must start with http:// or https://, ie:\n#    \"http://localhost/status\",\n#    \"https://www.baidu.com/phpfpm-status\",\n## fcgi: the URL must start with fcgi:// or cgi://, and port must be present, ie:\n#    \"fcgi://127.0.0.1:9001\",\n#    \"cgi://192.168.0.1:9000/status\",\n## Unix socket: path to fpm socket, ie:\n#    \"/run/php/php7.2-fpm.sock\",\n##    or using a custom fpm status path:\n#    \"/var/run/php5-fpm.sock:/fpm-custom-status-path\",\n##    glob patterns are also supported:\n#    \"/var/run/php*.sock\"\n]\n\n## append some labels for series\n# labels = { region=\"cloud\", product=\"n9e\" }\n\n## interval = global.interval * interval_times\n# interval_times = 1\n\n## Set response_timeout (default 5 seconds),HTTP urls only\nresponse_timeout = \"5s\"\n\n## Whether to follow redirects from the server (defaults to false),HTTP urls only\n# follow_redirects = false\n\n## Optional HTTP Basic Auth Credentials,HTTP urls only\n#username = \"admin\"\n#password = \"admin\"\n\n## Optional headers,HTTP urls only\n# headers = [\"X-From\", \"categraf\", \"X-Xyz\", \"abc\"]\n\n## Optional TLS Config,only http\n# use_tls = false\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = false', '# PHP-FPM\n\n*PHP-FPM* (PHP FastCGI Process Manager) 监控采集插件。\n\n该插件需要更改phpfpm的配置文件，开启 *pm.status_path*配置项\n```\npm.status_path = /status\n```\n\n\n## Configuration\n\n### 注意事项：\n1. 如下配置 仅生效于HTTP的url\n    - response_timeout\n    - username & password\n    - headers\n    - TLS config\n2. 如果使用 Unix socket，需要保证 categraf 和 socket path 在同一个主机上，且 categraf 运行用户拥有读取该 path 的权限。\n## 监控大盘和告警规则\n\n待更新...', '(PHP FastCGI Process Manager) 监控采集插件', '1732760539', '王杨(822032277)', '1732760574', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('76', 'TomCat', '# # collect interval\n# interval = 15\n\n# Gather metrics from the Tomcat server status page.\n[[instances]]\n## URL of the Tomcat server status\n# url = \"http://127.0.0.1:8080/manager/status/all?XML=true\"\nurl = \"\"\n\n## HTTP Basic Auth Credentials\n# username = \"tomcat\"\n# password = \"s3cret\"\n\n## Request timeout\n# timeout = \"5s\"\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# important! use global unique string to specify instance\n# labels = { instance=\"192.168.1.2:8080\", url=\"-\" }\n\n## Optional TLS Config\n# use_tls = false\n# tls_min_version = \"1.2\"\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = true\n', '# tomcat\n\ntomcat 采集器，是读取 tomcat 的管理侧接口 `/manager/status/all` 这个接口需要鉴权。修改 `tomcat-users.xml` ，增加下面的内容：\n\n```xml\n<role rolename=\"admin-gui\" />\n<user username=\"tomcat\" password=\"s3cret\" roles=\"manager-gui\" />\n```\n\n此外，还需要注释文件**webapps/manager/META-INF/context.xml**的以下内容，\n```xml\n  <Valve className=\"org.apache.catalina.valves.RemoteAddrValve\"\n         allow=\"127\\.\\d+\\.\\d+\\.\\d+|::1|0:0:0:0:0:0:0:1\" />\n```\n\n否则 tomcat 会报以下错误，导致 tomcat 采集器无法采集到数据。\n\n```html\n403 Access Denied\nYou are not authorized to view this page.\n\nBy default the Manager is only accessible from a browser running on the same machine as Tomcat. If you wish to modify this restriction, you\'ll need to edit the Manager\'s context.xml file.\n```\n\n## Configuration\n\n配置文件在 `conf/input.tomcat/tomcat.toml`\n\n```toml\n[[instances]]\n## URL of the Tomcat server status\nurl = \"http://127.0.0.1:8080/manager/status/all?XML=true\"\n\n## HTTP Basic Auth Credentials\nusername = \"tomcat\"\npassword = \"s3cret\"\n\n## Request timeout\n# timeout = \"5s\"\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# important! use global unique string to specify instance\n# labels = { instance=\"192.168.1.2:8080\", url=\"-\" }\n\n## Optional TLS Config\n# use_tls = false\n# tls_min_version = \"1.2\"\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = true\n```\n\n## 监控大盘\n\n夜莺内置了 tomcat 仪表盘，克隆到自己的业务组下使用即可。', 'tomcat 采集器', '1732762101', '王杨(822032277)', '1732762101', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('77', 'SQLServer', '# # collect interval\n# interval = 15\n\n[[instances]]\n\n## Specify instances to monitor with a list of connection strings.\n## All connection parameters are optional.\n## By default, the host is localhost, listening on default port, TCP 1433.\n##   for Windows, the user is the currently running AD user (SSO).\n##   See https://github.com/denisenkom/go-mssqldb for detailed connection\n##   parameters, in particular, tls connections can be created like so:\n##   \"encrypt=true;certificate=<cert>;hostNameInCertificate=<SqlServer host fqdn>\"\n# servers = [\"Server=server.xxx.com;Port=1433;User Id=monitor;Password=xxxxxx;app name=categraf;log=1;\"]\n# servers = [ ]\n\n## Authentication method\n## valid methods: \"connection_string\", \"AAD\"\n# auth_method = \"connection_string\"\n\n## \"database_type\" enables a specific set of queries depending on the database type. If specified, it replaces azuredb = true/false and query_version = 2\n## In the config file, the sql server plugin section should be repeated each with a set of servers for a specific database_type.\n## Possible values for database_type are - \"SQLServer\" or \"AzureSQLDB\" or \"AzureSQLManagedInstance\" or \"AzureSQLPool\"\n\ndatabase_type = \"SQLServer\"\n\n## A list of queries to include. If not specified, all the below listed queries are used.\ninclude_query = []\n\n## A list of queries to explicitly ignore.\nexclude_query = [\"SQLServerAvailabilityReplicaStates\", \"SQLServerDatabaseReplicaStates\"]\n\n## Queries enabled by default for database_type = \"SQLServer\" are -\n## SQLServerPerformanceCounters, SQLServerWaitStatsCategorized, SQLServerDatabaseIO, SQLServerProperties, SQLServerMemoryClerks,\n## SQLServerSchedulers, SQLServerRequests, SQLServerVolumeSpace, SQLServerCpu, SQLServerAvailabilityReplicaStates, SQLServerDatabaseReplicaStates,\n## SQLServerRecentBackups\n\n\n\n## Following are old config settings\n## You may use them only if you are using the earlier flavor of queries, however it is recommended to use\n## the new mechanism of identifying the database_type there by use it\'s corresponding queries\n\n## Optional parameter, setting this to 2 will use a new version\n## of the collection queries that break compatibility with the original\n## dashboards.\n## Version 2 - is compatible from SQL Server 2012 and later versions and also for SQL Azure DB\n# query_version = 2\n\n## Toggling this to true will emit an additional metric called \"sqlserver_telegraf_health\".\n## This metric tracks the count of attempted queries and successful queries for each SQL instance specified in \"servers\".\n## The purpose of this metric is to assist with identifying and diagnosing any connectivity or query issues.\n## This setting/metric is optional and is disabled by default.\n# health_metric = false\n\n## Possible queries accross different versions of the collectors\n## Queries enabled by default for specific Database Type\n\n## database_type =  SQLServer by default collects the following queries\n## - SQLServerPerformanceCounters\n## - SQLServerWaitStatsCategorized\n## - SQLServerDatabaseIO\n## - SQLServerProperties\n## - SQLServerMemoryClerks\n## - SQLServerSchedulers\n## - SQLServerRequests\n## - SQLServerVolumeSpace\n## - SQLServerCpu\n## - SQLServerRecentBackups\n## and following as optional (if mentioned in the include_query list)\n## - SQLServerAvailabilityReplicaStates\n## - SQLServerDatabaseReplicaStates\n\n## Version 2 by default collects the following queries\n## Version 2 is being deprecated, please consider using database_type.\n## - PerformanceCounters\n## - WaitStatsCategorized\n## - DatabaseIO\n## - ServerProperties\n## - MemoryClerk\n## - Schedulers\n## - SqlRequests\n## - VolumeSpace\n## - Cpu\n\n## Version 1 by default collects the following queries\n## Version 1 is deprecated, please consider using database_type.\n## - PerformanceCounters\n## - WaitStatsCategorized\n## - CPUHistory\n## - DatabaseIO\n## - DatabaseSize\n## - DatabaseStats\n## - DatabaseProperties\n## - MemoryClerk\n## - VolumeSpace\n## - PerformanceMetrics', '# sqlserver\n\nforked from telegraf/sqlserver. 这个插件的作用是获取sqlserver的监控指标，这里去掉了Azure相关部分监控，只保留了本地部署sqlserver情况。\n\n# 使用\n按照下面方法创建监控账号，用于读取监控数据\nUSE master;\n\nCREATE LOGIN [categraf] WITH PASSWORD = N\'mystrongpassword\';\n\nGRANT VIEW SERVER STATE TO [categraf];\n\nGRANT VIEW ANY DEFINITION TO [categraf];\nData Source=10.19.1.1;Initial Catalog=hc;User ID=sa;Password=mystrongpassword;', 'sqlserver监控指标采集', '1732762184', '王杨(822032277)', '1732762184', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('78', 'Nginx', '# # collect interval\n# interval = 15\n\n[[instances]]\n## An array of Nginx stub_status URI to gather stats.\nurls = [\n    \"https://nginx.domains.com\"\n]\n\n## append some labels for series\nlabels = { cloud=\"my-cloud\", region=\"my-region\",azone=\"az1\", product=\"my-product\" }\n\n## interval = global.interval * interval_times\n# interval_times = 1\n\n## Set response_timeout (default 5 seconds)\nresponse_timeout = \"5s\"\n\n## Whether to follow redirects from the server (defaults to false)\n# follow_redirects = false\n\n## Optional HTTP Basic Auth Credentials\n#username = \"admin\"\n#password = \"admin\"\n\n## Optional headers\n# headers = [\"X-From\", \"categraf\", \"X-Xyz\", \"abc\"]\n\n## Optional TLS Config\n# use_tls = false\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = false', '- 该插件依赖 **nginx** 的 **http_stub_status_module**\n\n# 应用场景\n一般用于业务系统做对外或对外路由映射时使用代理服务，是运维最常见且最重要的代理工具。\n\n# 部署场景\n需要在装有nginx服务的虚拟机启用此插件。\n\n\n# 前置条件\n```\n条件1：nginx服务需要启用http_stub_status_module模块\n\n推荐源码编译方式安装模块，如不清楚要安装哪些模块，可参考：\ncd /opt/nginx-1.20.1 && ./configure \\\n--prefix=/usr/share/nginx \\\n--sbin-path=/usr/sbin/nginx \\\n--modules-path=/usr/lib64/nginx/modules \\\n--conf-path=/etc/nginx/nginx.conf \\\n--error-log-path=/var/log/nginx/error.log \\\n--http-log-path=/var/log/nginx/access.log \\\n--http-client-body-temp-path=/var/lib/nginx/tmp/client_body \\\n--http-proxy-temp-path=/var/lib/nginx/tmp/proxy \\\n--http-fastcgi-temp-path=/var/lib/nginx/tmp/fastcgi \\\n--http-uwsgi-temp-path=/var/lib/nginx/tmp/uwsgi \\\n--http-scgi-temp-path=/var/lib/nginx/tmp/scgi \\\n--pid-path=/var/run/nginx.pid \\\n--lock-path=/run/lock/subsys/nginx \\\n--user=nginx \\\n--group=nginx \\\n--with-compat \\\n--with-threads \\\n--with-http_addition_module \\\n--with-http_auth_request_module \\\n--with-http_dav_module \\\n--with-http_flv_module \\\n--with-http_gunzip_module \\\n--with-http_gzip_static_module \\\n--with-http_mp4_module \\\n--with-http_random_index_module \\\n--with-http_realip_module \\\n--with-http_secure_link_module \\\n--with-http_slice_module \\\n--with-http_ssl_module \\\n--with-http_stub_status_module \\\n--with-http_sub_module \\\n--with-http_v2_module \\\n--with-mail \\\n--with-mail_ssl_module \\\n--with-stream \\\n--with-stream_realip_module \\\n--with-stream_ssl_module \\\n--with-stream_ssl_preread_module \\\n--with-select_module \\\n--with-poll_module \\\n--with-file-aio \\\n--with-http_xslt_module=dynamic \\\n--with-http_image_filter_module=dynamic \\\n--with-http_perl_module=dynamic \\\n--with-stream=dynamic \\\n--with-mail=dynamic \\\n--with-http_xslt_module=dynamic \\\n--add-module=/etc/nginx/third-modules/nginx_upstream_check_module \\\n--add-module=/etc/nginx/third-modules/ngx_devel_kit-0.3.0 \\\n--add-module=/etc/nginx/third-modules/lua-nginx-module-0.10.13 \\\n--add-module=/etc/nginx/third-modules/nginx-module-vts \\\n--add-module=/etc/nginx/third-modules/ngx-fancyindex-0.5.2\n\n# 根据cpu核数\nmake -j2\nmake install\n\n注意：第三方模块nginx_upstream_check_module lua-nginx-module nginx-module-vts 都是相关插件所必备的依赖。\n```\n\n```\n条件2：nginx启用stub_status配置。\n\n[root@aliyun conf.d]# cat nginx.domains.com.conf\nserver {\n    listen 80;\n    listen 443 ssl;\n    server_name nginx.domains.com;\n    include /etc/nginx/ssl_conf/domains.com.conf;\n\n    location / {\n        stub_status on;\n	    include /etc/nginx/ip_whitelist.conf;\n    }\n\n    access_log /var/log/nginx/nginx.domains.com.access.log main;\n    error_log /var/log/nginx/nginx.domains.com.error.log warn;\n}\n\n浏览器访问https://nginx.domains.com出现：\nActive connections: 5\nserver accepts handled requests\n 90837 90837   79582\nReading: 0 Writing: 1 Waiting: 4\n\nNginx状态解释：\nActive connections Nginx正处理的活动连接数5个\nserver Nginx启动到现在共处理了90837个连接。\naccepts Nginx启动到现在共成功创建90837次握手。\nhandled requests Nginx总共处理了79582次请求。\nReading Nginx读取到客户端的 Header 信息数。\nWriting Nginx返回给客户端的 Header 信息数。\nWaiting Nginx已经处理完正在等候下一次请求指令的驻留链接，Keep-alive启用情况下，这个值等于active-（reading + writing）。\n请求丢失数=(握手数-连接数)可以看出,本次状态显示没有丢失请求。\n\n```\n\n# 配置场景\n```\n本配置启用或数据定义如下功能：\n增加自定义标签，可通过自定义标签筛选数据及更加精确的告警推送。\n响应超时时间为5秒。\nurls字段填写条件2所定义好的域名。\n```\n\n# 修改nginx.toml文件配置\n```\n[root@aliyun input.nginx]# cat nginx.toml\n# # collect interval\n# interval = 15\n\n[[instances]]\n## An array of Nginx stub_status URI to gather stats.\nurls = [\n    \"https://nginx.domains.com\"\n]\n\n## append some labels for series\nlabels = { cloud=\"my-cloud\", region=\"my-region\",azone=\"az1\", product=\"my-product\" }\n\n## interval = global.interval * interval_times\n# interval_times = 1\n\n## Set response_timeout (default 5 seconds)\nresponse_timeout = \"5s\"\n\n## Whether to follow redirects from the server (defaults to false)\n# follow_redirects = false\n\n## Optional HTTP Basic Auth Credentials\n#username = \"admin\"\n#password = \"admin\"\n\n## Optional headers\n# headers = [\"X-From\", \"categraf\", \"X-Xyz\", \"abc\"]\n\n## Optional TLS Config\n# use_tls = false\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = false\n\n```\n\n# 测试配置\n```\n./categraf --test --inputs nginx\n\n21:46:46 nginx_waiting agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud port=443 product=nginx region=huabei-beijing-4 server=nginx.devops.press 0\n21:46:46 nginx_active agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud port=443 product=nginx region=huabei-beijing-4 server=nginx.devops.press 1\n21:46:46 nginx_accepts agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud port=443 product=nginx region=huabei-beijing-4 server=nginx.devops.press 90794\n21:46:46 nginx_handled agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud port=443 product=nginx region=huabei-beijing-4 server=nginx.devops.press 90794\n21:46:46 nginx_requests agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud port=443 product=nginx region=huabei-beijing-4 server=nginx.devops.press 79458\n21:46:46 nginx_reading agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud port=443 product=nginx region=huabei-beijing-4 server=nginx.devops.press 0\n21:46:46 nginx_writing agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud port=443 product=nginx region=huabei-beijing-4 server=nginx.devops.press 1\n\n```\n# 重启服务\n```\n重启categraf服务生效\nsystemctl daemon-reload && systemctl restart categraf && systemctl status categraf\n\n查看启动日志是否有错误\njournalctl -f -n 500 -u categraf | grep \"E\\!\" | grep \"W\\!\"\n```\n\n# 检查数据呈现\n等待1-2分钟后数据就会在图表中展示出来，如图：\n![image](https://user-images.githubusercontent.com/12181410/220639442-5d02a9ec-f0ae-48f5-91f0-4c7839b747b5.png)\n\n\n# 监控告警规则配置\n\n个人经验仅供参考：\n\n- 超过2000毫秒，为P2级别，启用企业微信应用推送告警，3分钟内恢复发出恢复告警。\n- 超过5000毫秒，为P1级别，启用电话语音告警&企业微信应用告警，3分钟内恢复发出恢复告警。\n\n\n# 监控图表配置\n\nhttps://github.com/flashcatcloud/categraf/blob/main/inputs/nginx_vts/dashboards.json\n\n# 故障自愈配置\n```\n先略过\n```', '依赖 nginx的http_stub_status_module', '1732762574', '王杨(822032277)', '1732762947', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('79', 'Nginx_Upstream_Check', '# # collect interval\n# interval = 15\n\n[[instances]]\n# 这个配置最关键，是要给出获取 status 信息的接口地址\ntargets = [\n    \"https://nginx-upstream.domains.com/?format=json\"\n]\n\n# 标签这个配置请注意\n# 如果 Categraf 和 Nginx 是在一台机器上，target 可能配置的是 127.0.0.1\n# 如果 Nginx 有多台机器，每台机器都有 Categraf 来采集本机的 Nginx 的 Status 信息\n# 可能会导致时序数据标签相同，不易区分，当然，Categraf 会自带 ident 标签，该标签标识本机机器名\n# 如果大家觉得 ident 标签不够用，可以用下面 labels 配置，附加 instance、region 之类的标签\n\n# # append some labels for series\nlabels = { cloud=\"my-cloud\", region=\"my-region\",azone=\"az1\", product=\"my-product\" }\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n## Set http_proxy (categraf uses the system wide proxy settings if it\'s is not set)\n# http_proxy = \"http://localhost:8888\"\n\n## Interface to use when dialing an address\n# interface = \"eth0\"\n\n## HTTP Request Method\n# method = \"GET\"\n\n## Set timeout (default 5 seconds)\n# timeout = \"5s\"\n\n## Whether to follow redirects from the server (defaults to false)\n# follow_redirects = false\n\n## Optional HTTP Basic Auth Credentials\n# username = \"username\"\n# password = \"pa$$word\"\n\n## Optional headers\n# headers = [\"X-From\", \"categraf\", \"X-Xyz\", \"abc\"]\n\n## Optional TLS Config\n# use_tls = false\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = false', '# 应用场景\n一般用于业务系统做对外或对外路由映射时使用代理服务，是运维最常见且最重要的代理工具。\n\n# 部署场景\n需要在装有nginx服务的虚拟机启用此插件。\n\n# 采集原理\n\n- 该采集插件是读取 [nginx_upstream_check](https://github.com/yaoweibin/nginx_upstream_check_module) 的状态输出。[nginx_upstream_check](https://github.com/yaoweibin/nginx_upstream_check_module) 可以周期性检查 upstream 中的各个 server 是否存活，如果检查失败，就会标记为 `down`，如果检查成功，就标记为 `up`。\n\n# 注意事项\n- 由于 TSDB 通常无法处理字符串，所以 Categraf 会做转换，将 `down` 转换为 2， `up` 转换为 1，其他状态转换为 0，使用 `nginx_upstream_check_status_code` 这个指标来表示，所以，我们可能需要这样的告警规则：\n\n# 前置条件\n## 条件1：nginx服务需要启用nginx_upstream_check_module模块\n```\n推荐源码编译方式安装模块，如不清楚要安装哪些模块，可参考：\ncd /opt/nginx-1.20.1 && ./configure \\\n--prefix=/usr/share/nginx \\\n--sbin-path=/usr/sbin/nginx \\\n--modules-path=/usr/lib64/nginx/modules \\\n--conf-path=/etc/nginx/nginx.conf \\\n--error-log-path=/var/log/nginx/error.log \\\n--http-log-path=/var/log/nginx/access.log \\\n--http-client-body-temp-path=/var/lib/nginx/tmp/client_body \\\n--http-proxy-temp-path=/var/lib/nginx/tmp/proxy \\\n--http-fastcgi-temp-path=/var/lib/nginx/tmp/fastcgi \\\n--http-uwsgi-temp-path=/var/lib/nginx/tmp/uwsgi \\\n--http-scgi-temp-path=/var/lib/nginx/tmp/scgi \\\n--pid-path=/var/run/nginx.pid \\\n--lock-path=/run/lock/subsys/nginx \\\n--user=nginx \\\n--group=nginx \\\n--with-compat \\\n--with-threads \\\n--with-http_addition_module \\\n--with-http_auth_request_module \\\n--with-http_dav_module \\\n--with-http_flv_module \\\n--with-http_gunzip_module \\\n--with-http_gzip_static_module \\\n--with-http_mp4_module \\\n--with-http_random_index_module \\\n--with-http_realip_module \\\n--with-http_secure_link_module \\\n--with-http_slice_module \\\n--with-http_ssl_module \\\n--with-http_stub_status_module \\\n--with-http_sub_module \\\n--with-http_v2_module \\\n--with-mail \\\n--with-mail_ssl_module \\\n--with-stream \\\n--with-stream_realip_module \\\n--with-stream_ssl_module \\\n--with-stream_ssl_preread_module \\\n--with-select_module \\\n--with-poll_module \\\n--with-file-aio \\\n--with-http_xslt_module=dynamic \\\n--with-http_image_filter_module=dynamic \\\n--with-http_perl_module=dynamic \\\n--with-stream=dynamic \\\n--with-mail=dynamic \\\n--with-http_xslt_module=dynamic \\\n--add-module=/etc/nginx/third-modules/nginx_upstream_check_module \\\n--add-module=/etc/nginx/third-modules/ngx_devel_kit-0.3.0 \\\n--add-module=/etc/nginx/third-modules/lua-nginx-module-0.10.13 \\\n--add-module=/etc/nginx/third-modules/nginx-module-vts \\\n--add-module=/etc/nginx/third-modules/ngx-fancyindex-0.5.2\n\n# 根据cpu核数\nmake -j2\nmake install\n\n注意：第三方模块nginx_upstream_check_module lua-nginx-module nginx-module-vts 都是相关插件所必备的依赖。\n```\n\n## 条件2：nginx启用check_status配置\n```\n[root@aliyun categraf]# cat /etc/nginx/conf.d/nginx-upstream.domains.com.conf\nserver {\n    listen 80;\n    listen 443 ssl;\n    server_name nginx-upstream.domains.com;\n    include /etc/nginx/ssl_conf/domains.com.conf;\n\n    location / {\n        check_status;\n        include /etc/nginx/ip_whitelist.conf;\n    }\n\n    access_log /var/log/nginx/nginx-upstream.domains.com.access.log main;\n    error_log /var/log/nginx/nginx-upstream.domains.com.error.log warn;\n}\n```\n浏览器访问https://nginx-upstream.domains.com?format=json出现：\n![image](https://user-images.githubusercontent.com/12181410/220912157-57f485de-6b4e-4ca4-869d-871244aabde1.png)\n\n浏览器访问https://nginx-upstream.domains.com出现：\n![image](https://user-images.githubusercontent.com/12181410/220909354-fc8ba53d-2384-41d3-8def-4447a104fb3c.png)\n\n## 条件3：在需要启用upstream监控的域名配置下进行配置\n例如：\n```\n[root@aliyun upstream_conf]# cat upstream_n9e.conf\nupstream n9e {\n    server 127.0.0.1:18000 weight=10 max_fails=2 fail_timeout=5s;\n\n    check interval=3000 rise=2 fall=5 timeout=1000 type=tcp default_down=false port=18000;\n    check_http_send \"HEAD / HTTP/1.0\\r\\n\\r\\n\";\n    check_http_expect_alive http_2xx http_3xx;\n}\n\n[root@aliyun upstream_conf]# cat upstream_n9e_server_api.conf\nupstream n9e-server-api {\n    server 127.0.0.1:19000 weight=10 max_fails=2 fail_timeout=5s;\n\n    check interval=3000 rise=2 fall=5 timeout=1000 type=tcp default_down=false port=19000;\n    check_http_send \"HEAD / HTTP/1.0\\r\\n\\r\\n\";\n    check_http_expect_alive http_2xx http_3xx;\n}\n\n[root@aliyun upstream_conf]# cat upstream_vm.conf\nupstream vm {\n    server 127.0.0.1:8428 weight=10 max_fails=2 fail_timeout=5s;\n    keepalive 20;\n\n    check interval=3000 rise=2 fall=5 timeout=1000 type=tcp default_down=false port=8428;\n    check_http_send \"HEAD / HTTP/1.0\\r\\n\\r\\n\";\n    check_http_expect_alive http_2xx http_3xx;\n}\n\n```\n\n# 配置场景\n```\n本配置启用或数据定义如下功能：\n增加自定义标签，可通过自定义标签筛选数据及更加精确的告警推送。\n响应超时时间为5秒。\nurls字段填写条件2所定义好的域名。\n```\n\n# 修改nginx.toml文件配置\n```\n[root@aliyun conf]# cat input.nginx_upstream_check/nginx_upstream_check.toml\n\n# # collect interval\n# interval = 15\n\n[[instances]]\n# 这个配置最关键，是要给出获取 status 信息的接口地址\ntargets = [\n    \"https://nginx-upstream.domains.com/?format=json\"\n]\n\n# 标签这个配置请注意\n# 如果 Categraf 和 Nginx 是在一台机器上，target 可能配置的是 127.0.0.1\n# 如果 Nginx 有多台机器，每台机器都有 Categraf 来采集本机的 Nginx 的 Status 信息\n# 可能会导致时序数据标签相同，不易区分，当然，Categraf 会自带 ident 标签，该标签标识本机机器名\n# 如果大家觉得 ident 标签不够用，可以用下面 labels 配置，附加 instance、region 之类的标签\n\n# # append some labels for series\nlabels = { cloud=\"my-cloud\", region=\"my-region\",azone=\"az1\", product=\"my-product\" }\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n## Set http_proxy (categraf uses the system wide proxy settings if it\'s is not set)\n# http_proxy = \"http://localhost:8888\"\n\n## Interface to use when dialing an address\n# interface = \"eth0\"\n\n## HTTP Request Method\n# method = \"GET\"\n\n## Set timeout (default 5 seconds)\n# timeout = \"5s\"\n\n## Whether to follow redirects from the server (defaults to false)\n# follow_redirects = false\n\n## Optional HTTP Basic Auth Credentials\n# username = \"username\"\n# password = \"pa$$word\"\n\n## Optional headers\n# headers = [\"X-From\", \"categraf\", \"X-Xyz\", \"abc\"]\n\n## Optional TLS Config\n# use_tls = false\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = false\n```\n\n# 测试配置\n```\n./categraf --test --inputs nginx_upstream_check\n\n```\n# 重启服务\n```\n重启categraf服务生效\nsystemctl daemon-reload && systemctl restart categraf && systemctl status categraf\n\n查看启动日志是否有错误\njournalctl -f -n 500 -u categraf | grep \"E\\!\" | grep \"W\\!\"\n```\n\n# 检查数据呈现\n等待1-2分钟后数据就会在图表中展示出来，如图：\n![image](https://user-images.githubusercontent.com/12181410/220914337-f97f6fd5-4763-4174-b64c-131aecf6664f.png)\n\n\n# 监控告警规则配置\n```\n一般查看后端是否异常为关键检查对象，nginx_upstream_check_status_code返回1代表正常，返回2代表异常（实际测试可从上图看出）。\nnginx_upstream_check_status_code!=1则视为异常需立即告警，级别为一级告警，执行频率为60秒，持续时长为60秒，留观时长2分钟，重复发送频率5分钟，最大发送次数0次，使用企业微信应用及电话语音通道将告警内容发送给系统运维组，此规则运用到周一到周日全天。\n```\n\n# 监控图表配置\nhttps://github.com/flashcatcloud/categraf/blob/main/inputs/nginx_upstream_check/dashboards.json\n\n# 故障自愈配置\n```\n先略过\n```', '采集nginx_upstream_check的状态输出', '1732762648', '王杨(822032277)', '1732762923', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('80', 'ElasticSearch', '# # collect interval\n# interval = 15\n\n############################################################################\n# !!! uncomment [[instances]] to enable this plugin\n[[instances]]\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# append some labels to metrics\n# labels = { cluster=\"cloud-n9e-es\" }\n\n## specify a list of one or more Elasticsearch servers\n# servers = [\"http://localhost:9200\"]\nservers = []\n\n## Timeout for HTTP requests to the elastic search server(s)\nhttp_timeout = \"10s\"\n\n# either /_nodes/stats or /_nodes/_local/stats depending on this setting\nlocal = false\n\n## Set cluster_health to true when you want to obtain cluster health stats\ncluster_health = true\n\n## Adjust cluster_health_level when you want to obtain detailed health stats\n## The options are\n##  - indices (default)\n##  - cluster\ncluster_health_level = \"cluster\"\n\n## Set cluster_stats to true when you want to obtain cluster stats.\ncluster_stats = true\n\n## Indices to collect; can be one or more indices names or _all\n## Use of wildcards is allowed. Use a wildcard at the end to retrieve index names that end with a changing value, like a date.\n# indices_include = [\"zipkin*\"]\n\n## use \"shards\" or blank string for indices level\nindices_level = \"\"\n\n## node_stats is a list of sub-stats that you want to have gathered. Valid options\n## are \"indices\", \"os\", \"process\", \"jvm\", \"thread_pool\", \"fs\", \"transport\", \"http\",\n## \"breaker\". Per default, all stats are gathered.\nnode_stats = [\"jvm\", \"breaker\", \"process\", \"os\", \"fs\", \"indices\", \"thread_pool\", \"transport\"]\n\n## HTTP Basic Authentication username and password.\nusername = \"elastic\"\npassword = \"password\"\n\n## Optional TLS Config\n# use_tls = false\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = true\n\n## Sets the number of most recent indices to return for indices that are configured with a date-stamped suffix.\n## Each \'indices_include\' entry ending with a wildcard (*) or glob matching pattern will group together all indices that match it, and \n## sort them by the date or number after the wildcard. Metrics then are gathered for only the \'num_most_recent_indices\' amount of most \n## recent indices.\nnum_most_recent_indices = 1', '# elasticsearch plugin\n\nElasticSearch 通过 HTTP JSON 的方式暴露了自身的监控指标，通过 categraf 的 [elasticsearch](https://github.com/flashcatcloud/categraf/tree/main/inputs/elasticsearch) 插件抓取。\n\n如果是小规模集群，设置 `local=false`，从集群中某一个节点抓取数据，即可拿到整个集群所有节点的监控数据。如果是大规模集群，建议设置 `local=true`，在集群的每个节点上都部署抓取器，抓取本地 elasticsearch 进程的监控数据。\n\n\n## 配置示例\n\ncategraf 配置文件：`conf/input.elasticsearch/elasticsearch.toml`\n\n```yaml\n[[instances]]\nservers = [\"http://192.168.11.177:9200\"]\nhttp_timeout = \"10s\"\nlocal = false\ncluster_health = true\ncluster_health_level = \"cluster\"\ncluster_stats = true\nindices_level = \"\"\nnode_stats = [\"jvm\", \"breaker\", \"process\", \"os\", \"fs\", \"indices\", \"thread_pool\", \"transport\"]\nusername = \"elastic\"\npassword = \"xxxxxxxx\"\nnum_most_recent_indices = 1\nlabels = { service=\"es\" }\n```\n\n## 仪表盘效果\n\n夜莺内置仪表盘中已经内置了 Elasticsearch 的仪表盘，导入即可使用。', 'elasticsearch监控指标采集', '1732762849', '王杨(822032277)', '1732762849', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('81', 'HAProxy', '[[instances]]\n# URI on which to scrape HAProxy.\n# e.g. \n# uri = \"http://localhost:5000/baz?stats;csv\"\n# uri = \"http://user:pass@haproxy.example.com/haproxy?stats;csv\"\n# uri = \"unix:/run/haproxy/admin.sock\"\nuri = \"\"\n\n# Flag that enables SSL certificate verification for the scrape URI\nssl_verify = false\n\n# Comma-separated list of exported server metrics. See http://cbonte.github.io/haproxy-dconv/configuration-1.5.html#9.1\nserver_metric_fields = \"\"\n\n# Comma-separated list of exported server states to exclude. See https://cbonte.github.io/haproxy-dconv/1.8/management.html#9.1, field 17 status\nserver_exclude_states = \"\"\n\n# Timeout for trying to get stats from HAProxy.\ntimeout = \"5s\"\n\n# Flag that enables using HTTP proxy settings from environment variables ($http_proxy, $https_proxy, $no_proxy)\nproxy_from_env = false', '# HAProxy\n\nforked from [haproxy_exporter](https://github.com/prometheus/haproxy_exporter)\n\nNote: since HAProxy 2.0.0, the official source includes a Prometheus exporter module that can be built into your binary with a single flag during build time and offers an exporter-free Prometheus endpoint.\n\n\nhaproxy configurations for `/stats`:\n\n```\nfrontend stats\n    bind *:8404\n    stats enable\n    stats uri /stats\n    stats refresh 10s\n```\n', 'haproxy监控指标采集', '1732762902', '王杨(822032277)', '1732762902', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('82', 'PostgreSQL', '# Read metrics from one or many postgresql servers\n# # collect interval\n# interval = 15\n\n[[instances]]\n  ## specify address via a url matching:\n  ##   postgres://[pqgotest[:password]]@localhost[/dbname]?sslmode=[disable|verify-ca|verify-full]\n  ## or a simple string:\n  ##   host=localhost user=pqgotest password=... sslmode=... dbname=app_production\n  ##\n  ## All connection parameters are optional.\n  ##\n  ## Without the dbname parameter, the driver will default to a database\n  ## with the same name as the user. This dbname is just for instantiating a\n  ## connection with the server and doesn\'t restrict the databases we are trying\n  ## to grab metrics for.\n  ##\n  # address = \"host=localhost user=postgres sslmode=disable\"\n\n  ## A custom name for the database that will be used as the \"server\" tag in the\n  ## measurement output. If not specified, a default one generated from\n  ## the connection address is used.\n  # outputaddress = \"db01\"\n\n  ## connection configuration.\n  ## maxlifetime - specify the maximum lifetime of a connection.\n  ## default is forever (0s)\n  # max_lifetime = \"0s\"\n\n  ## A  list of databases to explicitly ignore.  If not specified, metrics for all\n  ## databases are gathered.  Do NOT use with the \'databases\' option.\n  # ignored_databases = [\"postgres\", \"template0\", \"template1\"]\n\n  ## A list of databases to pull metrics about. If not specified, metrics for all\n  ## databases are gathered.  Do NOT use with the \'ignored_databases\' option.\n  # databases = [\"app_production\", \"testing\"]\n\n  ## Whether to use prepared statements when connecting to the database.\n  ## This should be set to false when connecting through a PgBouncer instance\n  ## with pool_mode set to transaction.\n  #prepared_statements = true\n  # [[instances.metrics]]\n  # mesurement = \"sessions\"\n  # label_fields = [ \"status\", \"type\" ]\n  # metric_fields = [ \"value\" ]\n  # timeout = \"3s\"\n  # request = \'\'\'\n  # SELECT status, type, COUNT(*) as value FROM v$session GROUP BY status, type\n  # \'\'\'\n', '# PostgreSQL\n\ncategraf 作为一个 client 连上 pg，采集相关指标，首先要确保用户授权。举例：\n\n```\ncreate user categraf with password \'categraf\';\nalter user categraf set default_transaction_read_only=on;\ngrant usage on schema public to categraf;\ngrant select on all tables in schema public to categraf ;\n```\n\n## 配置文件示例\n\n```toml\n[[instances]]\naddress = \"host=192.168.11.181 port=5432 user=postgres password=123456789 sslmode=disable\"\n## specify address via a url matching:\n##   postgres://[pqgotest[:password]]@localhost[/dbname]?sslmode=[disable|verify-ca|verify-full]\n## or a simple string:\n##   host=localhost user=pqgotest password=... sslmode=... dbname=app_production\n##\n## All connection parameters are optional.\n##\n## Without the dbname parameter, the driver will default to a database\n## with the same name as the user. This dbname is just for instantiating a\n## connection with the server and doesn\'t restrict the databases we are trying\n## to grab metrics for.\n##\n# address = \"host=localhost user=postgres sslmode=disable\"\n\n## A custom name for the database that will be used as the \"server\" tag in the\n## measurement output. If not specified, a default one generated from\n## the connection address is used.\n# outputaddress = \"db01\"\n\n## connection configuration.\n## maxlifetime - specify the maximum lifetime of a connection.\n## default is forever (0s)\n# max_lifetime = \"0s\"\n\n## A  list of databases to explicitly ignore.  If not specified, metrics for all\n## databases are gathered.  Do NOT use with the \'databases\' option.\n# ignored_databases = [\"postgres\", \"template0\", \"template1\"]\n\n## A list of databases to pull metrics about. If not specified, metrics for all\n## databases are gathered.  Do NOT use with the \'ignored_databases\' option.\n# databases = [\"app_production\", \"testing\"]\n\n## Whether to use prepared statements when connecting to the database.\n## This should be set to false when connecting through a PgBouncer instance\n## with pool_mode set to transaction.\n# prepared_statements = true\n#\n# [[instances.metrics]]\n# mesurement = \"sessions\"\n# label_fields = [ \"status\", \"type\" ]\n# metric_fields = [ \"value\" ]\n# timeout = \"3s\"\n# request = \'\'\'\n# SELECT status, type, COUNT(*) as value FROM v$session GROUP BY status, type\n# \'\'\'\n```\n\n## 仪表盘\n\n夜莺内置了 Postgres 的仪表盘，克隆到自己的业务组下即可使用。\n\n![20230802073729](https://download.flashcat.cloud/ulric/20230802073729.png)\n\n## 告警规则\n\n夜莺内置了 Postgres 的告警规则，克隆到自己的业务组下即可使用。\n\n![20230802073753](https://download.flashcat.cloud/ulric/20230802073753.png)', 'PostgreSQL监控指标采集', '1732763069', '王杨(822032277)', '1732763069', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('83', 'SMART', '# Read metrics from storage devices supporting S.M.A.R.T.\n[[instances]]\n    ## Optionally specify the path to the smartctl executable\n    # path_smartctl = \"/usr/bin/smartctl\"\n\n    ## Optionally specify the path to the nvme-cli executable\n    # path_nvme = \"/usr/bin/nvme\"\n\n    ## Optionally specify if vendor specific attributes should be propagated for NVMe disk case\n    ## [\"auto-on\"] - automatically find and enable additional vendor specific disk info\n    ## [\"vendor1\", \"vendor2\", ...] - e.g. \"Intel\" enable additional Intel specific disk info\n    #  enable_extensions = [\"auto-on\"]\n\n    ## On most platforms used cli utilities requires root access.\n    ## Setting \'use_sudo\' to true will make use of sudo to run smartctl or nvme-cli.\n    ## Sudo must be configured to allow the telegraf user to run smartctl or nvme-cli\n    ## without a password.\n    # use_sudo = true\n\n    ## Skip checking disks in this power mode. Defaults to\n    ## \"standby\" to not wake up disks that have stopped rotating.\n    ## See --nocheck in the man pages for smartctl.\n    ## smartctl version 5.41 and 5.42 have faulty detection of\n    ## power mode and might require changing this value to\n    ## \"never\" depending on your disks.\n    # nocheck = \"standby\"\n\n    ## Gather all returned S.M.A.R.T. attribute metrics and the detailed\n    ## information from each drive into the \'smart_attribute\' measurement.\n    # attributes = true\n\n    ## Optionally specify devices to exclude from reporting if disks auto-discovery is performed.\n    # excludes = [ \"/dev/pass6\" ]\n\n    ## Optionally specify devices and device type, if unset\n    ## a scan (smartctl --scan and smartctl --scan -d nvme) for S.M.A.R.T. devices will be done\n    ## and all found will be included except for the excluded in excludes.\n    # devices = [ \"/dev/ada0 -d atacam\", \"/dev/nvme0\"]\n    # devices = [\"dev/nvme0 -d nvme\", \"/dev/nvme0\"]\n\n    ## Timeout for the cli command to complete.\n      timeout = \"30s\"\n\n    ## Optionally call smartctl and nvme-cli with a specific concurrency policy.\n    ## By default, smartctl and nvme-cli are called in separate threads (goroutines) to gather disk attributes.\n    ## Some devices (e.g. disks in RAID arrays) may have access limitations that require sequential reading of\n    ## SMART data - one individual array drive at the time. In such case please set this configuration option\n    ## to \"sequential\" to get readings for all drives.\n    ## valid options: concurrent, sequential\n    # read_method = \"concurrent\"\n', '# S.M.A.R.T. 插件\n\n从[telegraf](https://github.com/influxdata/telegraf/blob/master/plugins/inputs/smart/README.md) fork，略作改动\n\nGet metrics using the command line utility `smartctl` for\nS.M.A.R.T. (Self-Monitoring, Analysis and Reporting Technology) storage\ndevices. SMART is a monitoring system included in computer hard disk drives\n(HDDs) and solid-state drives (SSDs) that detects and reports on various\nindicators of drive reliability, with the intent of enabling the anticipation of\nhardware failures.  See smartmontools (<https://www.smartmontools.org/>).\n\nSMART information is separated between different measurements: `smart_device` is\nused for general information, while `smart_attribute` stores the detailed\nattribute information if `attributes = true` is enabled in the plugin\nconfiguration.\n\nIf no devices are specified, the plugin will scan for SMART devices via the\nfollowing command:\n\n```sh\nsmartctl --scan\n```\n\nMetrics will be reported from the following `smartctl` command:\n\n```sh\nsmartctl --info --attributes --health -n <nocheck> --format=brief <device>\n```\n\nThis plugin supports _smartmontools_ version 5.41 and above, but v. 5.41 and\nv. 5.42 might require setting `nocheck`, see the comment in the sample\nconfiguration.  Also, NVMe capabilities were introduced in version 6.5.\n\nTo enable SMART on a storage device run:\n\n```sh\nsmartctl -s on <device>\n```\n\n## NVMe vendor specific attributes\n\nFor NVMe disk type, plugin can use command line utility `nvme-cli`. It has a\nfeature to easy access a vendor specific attributes.  This plugin supports\nnmve-cli version 1.5 and above (<https://github.com/linux-nvme/nvme-cli>).  In\ncase of `nvme-cli` absence NVMe vendor specific metrics will not be obtained.\n\nVendor specific SMART metrics for NVMe disks may be reported from the following\n`nvme` command:\n\n```sh\nnvme <vendor> smart-log-add <device>\n```\n\nNote that vendor plugins for `nvme-cli` could require different naming\nconvention and report format.\n\nTo see installed plugin extensions, depended on the nvme-cli version, look at\nthe bottom of:\n\n```sh\nnvme help\n```\n\nTo gather disk vendor id (vid) `id-ctrl` could be used:\n\n```sh\nnvme id-ctrl <device>\n```\n\nAssociation between a vid and company can be found there:\n<https://pcisig.com/membership/member-companies>.\n\nDevices affiliation to being NVMe or non NVMe will be determined thanks to:\n\n```sh\nsmartctl --scan\n```\n\nand:\n\n```sh\nsmartctl --scan -d nvme\n```\n\n\n## Configuration\n\n```toml @示例\n# Read metrics from storage devices supporting S.M.A.R.T.\n[[instances]]\n## Optionally specify the path to the smartctl executable\n# path_smartctl = \"/usr/bin/smartctl\"\n\n## Optionally specify the path to the nvme-cli executable\n# path_nvme = \"/usr/bin/nvme\"\n\n## Optionally specify if vendor specific attributes should be propagated for NVMe disk case\n## [\"auto-on\"] - automatically find and enable additional vendor specific disk info\n## [\"vendor1\", \"vendor2\", ...] - e.g. \"Intel\" enable additional Intel specific disk info\n# enable_extensions = [\"auto-on\"]\n\n## On most platforms used cli utilities requires root access.\n## Setting \'use_sudo\' to true will make use of sudo to run smartctl or nvme-cli.\n## Sudo must be configured to allow the categraf user to run smartctl or nvme-cli\n## Sudo must be configured to allow the categraf user to run smartctl or nvme-cli\n## without a password.\nuse_sudo = true\n\n## Skip checking disks in this power mode. Defaults to\n## \"standby\" to not wake up disks that have stopped rotating.\n## See --nocheck in the man pages for smartctl.\n## smartctl version 5.41 and 5.42 have faulty detection of\n## power mode and might require changing this value to\n## \"never\" depending on your disks.\n# nocheck = \"standby\"\n\n## Gather all returned S.M.A.R.T. attribute metrics and the detailed\n## information from each drive into the \'smart_attribute\' measurement.\nattributes = true\n\n## Optionally specify devices to exclude from reporting if disks auto-discovery is performed.\n# excludes = [ \"/dev/pass6\" ]\n\n## Optionally specify devices and device type, if unset\n## a scan (smartctl --scan and smartctl --scan -d nvme) for S.M.A.R.T. devices will be done\n## and all found will be included except for the excluded in excludes.\n# devices = [ \"/dev/ada0 -d atacam\", \"/dev/nvme0\"]\n# devices = [\"dev/nvme0 -d nvme\", \"/dev/nvme0\"]\n\n## Timeout for the cli command to complete.\ntimeout = \"30s\"\n\n## Optionally call smartctl and nvme-cli with a specific concurrency policy.\n## By default, smartctl and nvme-cli are called in separate threads (goroutines) to gather disk attributes.\n## Some devices (e.g. disks in RAID arrays) may have access limitations that require sequential reading of\n## SMART data - one individual array drive at the time. In such case please set this configuration option\n## to \"sequential\" to get readings for all drives.\n## valid options: concurrent, sequential\n# read_method = \"concurrent\"\n```\n\n## Permissions\n采集需要sudo权限\n\n## Metrics\n\n- smart_device:\n  - tags:\n    - capacity\n    - device\n    - enabled\n    - model\n    - serial_no\n    - wwn\n  - fields:\n    - exit_status\n    - health_ok\n    - media_wearout_indicator\n    - percent_lifetime_remain\n    - read_error_rate\n    - seek_error\n    - temp_c\n    - udma_crc_errors\n    - wear_leveling_count\n\n- smart_attribute:\n  - tags:\n    - capacity\n    - device\n    - enabled\n    - fail\n    - flags\n    - id\n    - model\n    - name\n    - serial_no\n    - wwn\n  - fields:\n    - exit_status\n    - threshold\n    - value\n    - worst\n    - critical_warning\n    - temperature_celsius\n    - available_spare\n    - available_spare_threshold\n    - percentage_used\n    - data_units_read\n    - data_units_written\n    - host_read_commands\n    - host_write_commands\n    - controller_busy_time\n    - power_cycle_count\n    - power_on_hours\n    - unsafe_shutdowns\n    - media_and_data_integrity_errors\n    - error_information_log_entries\n    - warning_temperature_time\n    - critical_temperature_time\n    - program_fail_count\n    - erase_fail_count\n    - wear_leveling_count\n    - end_to_end_error_detection_count\n    - crc_error_count\n    - media_wear_percentage\n    - host_reads\n    - timed_workload_timer\n    - thermal_throttle_status\n    - retry_buffer_overflow_count\n    - pll_lock_loss_count\n\n### Flags\n\nThe interpretation of the tag `flags` is:\n\n- `K` auto-keep\n- `C` event count\n- `R` error rate\n- `S` speed/performance\n- `O` updated online\n- `P` prefailure warning\n\n### Exit Status\n\nThe `exit_status` field captures the exit status of the used cli utilities\ncommand which is defined by a bitmask. For the interpretation of the bitmask see\nthe man page for smartctl or nvme-cli.\n\n## Device Names\n\nDevice names, e.g., `/dev/sda`, are _not persistent_, and may be\nsubject to change across reboots or system changes. Instead, you can use the\n_World Wide Name_ (WWN) or serial number to identify devices. On Linux block\ndevices can be referenced by the WWN in the following location:\n`/dev/disk/by-id/`.\n\n## Troubleshooting\n\nIf you expect to see more SMART metrics than this plugin shows, be sure to use a\nproper version of smartctl or nvme-cli utility which has the functionality to\ngather desired data. Also, check your device capability because not every SMART\nmetrics are mandatory. For example the number of temperature sensors depends on\nthe device specification.\n\nIf this plugin is not working as expected for your SMART enabled device,\nplease run these commands and include the output in a bug report:\n\nFor non NVMe devices (from smartctl version >= 7.0 this will also return NVMe\ndevices by default):\n\n```sh\nsmartctl --scan\n```\n\nFor NVMe devices:\n\n```sh\nsmartctl --scan -d nvme\n```\n\nRun the following command replacing your configuration setting for NOCHECK and\nthe DEVICE (name of the device could be taken from the previous command):\n\n```sh\nsmartctl --info --health --attributes --tolerance=verypermissive --nocheck NOCHECK --format=brief -d DEVICE\n```\n\nIf you try to gather vendor specific metrics, please provide this command\nand replace vendor and device to match your case:\n\n```sh\nnvme VENDOR smart-log-add DEVICE\n```\n\nIf you have specified devices array in configuration file, and categraf only\nshows data from one device, you should change the plugin configuration to\nsequentially gather disk attributes instead of collecting it in separate threads\n(goroutines). To do this find in plugin configuration read_method and change it\nto sequential:\n\n```toml\n    ## Optionally call smartctl and nvme-cli with a specific concurrency policy.\n    ## By default, smartctl and nvme-cli are called in separate threads (goroutines) to gather disk attributes.\n    ## Some devices (e.g. disks in RAID arrays) may have access limitations that require sequential reading of\n    ## SMART data - one individual array drive at the time. In such case please set this configuration option\n    ## to \"sequential\" to get readings for all drives.\n    ## valid options: concurrent, sequential\n    read_method = \"sequential\"\n```\n\n## Example Output\n\n```text\nsmart_device_health_ok agent_hostname=1.2.3.4 device=nvme0 model=INTEL_SSDPE2KX040T8 serial_no=PHLJ830200CH4P0DGN 1\nsmart_device_temp_c agent_hostname=1.2.3.4 device=nvme0 model=INTEL_SSDPE2KX040T8 serial_no=PHLJ830200CH4P0DGN 53\nsmart_attribute_program_fail_count agent_hostname=1.2.3.4 device=nvme0 model= name=Program_Fail_Count serial_no=PHLJ830200CH4P0DGN 0\nsmart_attribute_erase_fail_count agent_hostname=1.2.3.4 device=nvme0 model= name=Erase_Fail_Count serial_no=PHLJ830200CH4P0DGN 0\nsmart_attribute_wear_leveling_count agent_hostname=1.2.3.4 device=nvme0 model= name=Wear_Leveling_Count serial_no=PHLJ830200CH4P0DGN 34360328200\n```', 'SMART插件', '1732763178', '王杨(822032277)', '1732763178', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('84', 'HTTP_Response', '## collect interval\n# interval = 15\n\n## Set the mapping of extra tags in batches\n[mappings]\n# \"http://localhost\" = { \"job\" = \"local\" }\n# \"https://www.baidu.com\" = { \"job\" = \"baidu\" }\n\n[[instances]]\ntargets = [\n#     \"http://localhost\",\n#     \"https://www.baidu.com\"\n]\n\n## append some labels for series\n# labels = { region=\"cloud\", product=\"n9e\" }\n\n## interval = global.interval * interval_times\n# interval_times = 1\n\n## Set http_proxy (categraf uses the system wide proxy settings if it\'s is not set)\n# http_proxy = \"http://localhost:8888\"\n\n## Interface to use when dialing an address\n# interface = \"eth0\"\n\n## HTTP Request Method\n# method = \"GET\"\n\n## Set response_timeout (default 5 seconds)\n# response_timeout = \"5s\"\n\n## Whether to follow redirects from the server (defaults to false)\n# follow_redirects = false\n\n## Optional HTTP Basic Auth Credentials\n# username = \"username\"\n# password = \"pa$$word\"\n\n## Optional headers\n# headers = [\"Header-Key-1\", \"Header-Value-1\", \"Header-Key-2\", \"Header-Value-2\"]\n\n## Optional HTTP Request Body\n# body = \'\'\'\n# {\'fake\':\'data\'}\n# \'\'\'\n\n## Optional substring or regular expression match in body of the response(substring case sensitive).\n## When both of the following parameters are enabled, one of them can be satisfied.\n# expect_response_substring = \"ok\"\n# expect_response_regular_expression = \"green|yellow\"\n\n## Optional expected response status codes.\n## \"expect_response_status_codes\" Supports adding multiple codes by delimiter(\"|\" or \",\").\n## When both of the following parameters are enabled, one of them can be satisfied.\n# expect_response_status_code = 0\n# expect_response_status_codes = \"200|301\"\n\n## Optional TLS Config\n# use_tls = false\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = false\n', '# http_response plugin\n\nHTTP 探测插件，用于检测 HTTP 地址的连通性、延迟、HTTPS 证书过期时间。因为 Prometheus 生态的时序库只能存储 float64 类型的值，所以 HTTP 地址探测的结果也是 float64 类型的值，但是这个值的含义是不同的，具体含义如下：\n\n```\nSuccess          = 0\nConnectionFailed = 1\nTimeout          = 2\nDNSError         = 3\nAddressError     = 4\nBodyMismatch     = 5\nCodeMismatch     = 6\n```\n\n如果一切正常，这个值是 0，如果有异常，这个值是 1-6 之间的值，具体含义如上。这个值对应的指标名字是 `http_response_result_code`。\n\n## Configuration\n\ncategraf 的 `conf/input.http_response/http_response.toml`。最核心的配置就是 targets 配置，配置目标地址，比如想要监控两个地址：\n\n```toml\n[[instances]]\ntargets = [\n    \"http://localhost:8080\",\n    \"https://www.baidu.com\"\n]\n```\n\ninstances 下面的所有 targets 共享同一个 `[[instances]]` 下面的配置，比如超时时间，HTTP方法等，如果有些配置不同，可以拆成多个不同的 `[[instances]]`，比如：\n\n```toml\n[[instances]]\ntargets = [\n    \"http://localhost:8080\",\n    \"https://www.baidu.com\"\n]\nmethod = \"GET\"\n\n[[instances]]\ntargets = [\n    \"http://localhost:9090\"\n]\nmethod = \"POST\"\n```\n\n完整的带有注释的配置如下：\n\n```toml\n[[instances]]\ntargets = [\n#     \"http://localhost\",\n#     \"https://www.baidu.com\"\n]\n\n# # append some labels for series\n# labels = { region=\"cloud\", product=\"n9e\" }\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n## Set http_proxy (categraf uses the system wide proxy settings if it\'s is not set)\n# http_proxy = \"http://localhost:8888\"\n\n## Interface to use when dialing an address\n# interface = \"eth0\"\n\n## HTTP Request Method\n# method = \"GET\"\n\n## Set response_timeout (default 5 seconds)\n# response_timeout = \"5s\"\n\n## Whether to follow redirects from the server (defaults to false)\n# follow_redirects = false\n\n## Optional HTTP Basic Auth Credentials\n# username = \"username\"\n# password = \"pa$$word\"\n\n## Optional headers\n# headers = [\"Header-Key-1\", \"Header-Value-1\", \"Header-Key-2\", \"Header-Value-2\"]\n\n## Optional HTTP Request Body\n# body = \'\'\'\n# {\'fake\':\'data\'}\n# \'\'\'\n\n## Optional substring match in body of the response (case sensitive)\n# expect_response_substring = \"ok\"\n\n## Optional expected response status code.\n# expect_response_status_code = 0\n\n## Optional TLS Config\n# use_tls = false\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = false\n```\n\n## dashboard and monitors\n\n夜莺提供了内置大盘和内置告警规则，克隆到自己的业务组下即可使用。', 'HTTP 探测插件', '1732763313', '王杨(822032277)', '1732763313', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('85', 'Net_Response', '# # collect interval\n# interval = 15\n\n[mappings]\n# \"127.0.0.1:22\"= {region=\"local\",ssh=\"test\"}\n# \"127.0.0.1:22\"= {region=\"local\",ssh=\"redis\"}\n\n[[instances]]\ntargets = [\n#     \"127.0.0.1:22\",\n#     \"localhost:6379\",\n#     \":9090\"\n]\n\n# # append some labels for series\n# labels = { region=\"cloud\", product=\"n9e\" }\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n## Protocol, must be \"tcp\" or \"udp\"\n## NOTE: because the \"udp\" protocol does not respond to requests, it requires\n## a send/expect string pair (see below).\n# protocol = \"tcp\"\n\n## Set timeout\n# timeout = \"1s\"\n\n## Set read timeout (only used if expecting a response)\n# read_timeout = \"1s\"\n\n## The following options are required for UDP checks. For TCP, they are\n## optional. The plugin will send the given string to the server and then\n## expect to receive the given \'expect\' string back.\n## string sent to the server\n# send = \"ssh\"\n## expected string in answer\n# expect = \"ssh', '# net_response\n\n网络探测插件，通常用于监控本机某个端口是否在监听，或远端某个端口是否能连通。因为 Prometheus 生态的时序库只能存储 float64 类型的值，所以网络探测插件探测的结果也是 float64 类型的值，但是这个值的含义是不同的，具体含义如下：\n\n```\n- 0: Success\n- 1: Timeout\n- 2: ConnectionFailed\n- 3: ReadFailed\n- 4: StringMismatch\n```\n\n如果一切正常，这个值是 0，如果有异常，这个值是 1-4 之间的值，具体含义如上。这个值对应的指标名字是 `net_response_result_code`。\n\n## Configuration\n\ncategraf 的 `conf/input.net_response/net_response.toml`。最核心的配置就是 targets 部分，指定探测的目标，下面的例子：\n\n```toml\n[[instances]]\ntargets = [\n    \"10.2.3.4:22\",\n    \"localhost:6379\",\n    \":9090\"\n]\n```\n\n- `10.2.3.4:22` 表示探测 10.2.3.4 这个机器的 22 端口是否可以连通\n- `localhost:6379` 表示探测本机的 6379 端口是否可以连通\n- `:9090` 表示探测本机的 9090 端口是否可以连通\n\n监控数据或告警事件中只是一个 IP 和端口，接收告警的人看到了，可能不清楚只是哪个业务的模块告警了，可以附加一些更有价值的信息放到标签里，比如：\n\n```toml\nlabels = { region=\"cloud\", product=\"n9e\" }\n```\n\n标识了这是 cloud 这个 region，n9e 这个产品，这俩标签会附到时序数据上，告警的时候自然也会报出来。\n\n完整配置样例如下：\n\n```toml\n[mappings]\n# \"127.0.0.1:22\"= {region=\"local\",ssh=\"test\"}\n# \"127.0.0.1:22\"= {region=\"local\",ssh=\"redis\"}\n\n[[instances]]\ntargets = [\n#     \"127.0.0.1:22\",\n#     \"localhost:6379\",\n#     \":9090\"\n]\n\n# # append some labels for series\n# labels = { region=\"cloud\", product=\"n9e\" }\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n## Protocol, must be \"tcp\" or \"udp\"\n## NOTE: because the \"udp\" protocol does not respond to requests, it requires\n## a send/expect string pair (see below).\n# protocol = \"tcp\"\n\n## Set timeout\n# timeout = \"1s\"\n\n## Set read timeout (only used if expecting a response)\n# read_timeout = \"1s\"\n\n## The following options are required for UDP checks. For TCP, they are\n## optional. The plugin will send the given string to the server and then\n## expect to receive the given \'expect\' string back.\n## string sent to the server\n# send = \"ssh\"\n## expected string in answer\n# expect = \"ssh\"\n```\n\n## 监控大盘和告警规则\n\n夜莺内置了仪表盘和告警规则，克隆到自己的业务组即可使用。', '监控本机或远端某个端口是否在监听或联通', '1732763419', '王杨(822032277)', '1732763441', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('86', 'Procstat', '# # collect interval\n# interval = 15\n\n# [[instances]]\n# # executable name (ie, pgrep <search_exec_substring>)\n# search_exec_substring = \"nginx\"\n\n# # pattern as argument for pgrep (ie, pgrep -f <search_cmdline_substring>)\n# search_cmdline_substring = \"n9e server\"\n\n# # windows service name\n# search_win_service = \"\"\n\n# # search process with specific user, option with exec_substring or cmdline_substring\n# search_user = \"\"\n\n# # append some labels for series\n# labels = { region=\"cloud\", product=\"n9e\" }\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# # mode to use when calculating CPU usage. can be one of \'solaris\' or \'irix\'\n# mode = \"irix\"\n\n# sum of threads/fd/io/cpu/mem, min of uptime/limit\ngather_total = true\n\n# will append pid as tag\ngather_per_pid = false\n\n#  gather jvm metrics only when jstat is ready\n# gather_more_metrics = [\n#     \"threads\",\n#     \"fd\",\n#     \"io\",\n#     \"uptime\",\n#     \"cpu\",\n#     \"mem\",\n#     \"limit\",\n#     \"jvm\"\n# ]', '# 进程监控\n\n使用 categraf procstat 插件。\n\n## 配置文件\n\n位置：categraf 的 `conf/input.procstat/procstat.toml`\n\n样例配置：\n\n```toml\n[[instances]]\n# # executable name (ie, pgrep <search_exec_substring>)\nsearch_exec_substring = \"nginx\"\n\n# # pattern as argument for pgrep (ie, pgrep -f <search_cmdline_substring>)\n# search_cmdline_substring = \"n9e server\"\n\n# # windows service name\n# search_win_service = \"\"\n\n# # search process with specific user, option with exec_substring or cmdline_substring\n# search_user = \"\"\n\n# # append some labels for series\n# labels = { region=\"cloud\", product=\"n9e\" }\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# # mode to use when calculating CPU usage. can be one of \'solaris\' or \'irix\'\n# mode = \"irix\"\n\n# sum of threads/fd/io/cpu/mem, min of uptime/limit\ngather_total = true\n\n# will append pid as tag\ngather_per_pid = false\n\n#  gather jvm metrics only when jstat is ready\n# gather_more_metrics = [\n#     \"threads\",\n#     \"fd\",\n#     \"io\",\n#     \"uptime\",\n#     \"cpu\",\n#     \"mem\",\n#     \"limit\",\n#     \"jvm\"\n# ]\n```\n\n机器上有很多进程，要监控进程是否存活以及进程的资源占用，首先得告诉 categraf，要监控的进程是啥。所以，本插件一开始的几个配置，就是做进程过滤的，用来告诉 categraf 要监控的进程是哪些。\n\n- search_exec_substring 配置一个查询字符串，相当于执行 `pgrep <search_exec_substring>`\n- search_cmdline_substring 配置一个查询字符串，相当于执行 `pgrep -f <search_cmdline_substring>`\n- search_win_service 配置一个 windows 服务名，相当于执行 `sc query <search_win_service>`\n\n上例默认是采集 nginx。默认只会采集一个指标：procstat_lookup_count，表示通过这些过滤条件，查询到的进程的数量。那显然，如果 `procstat_lookup_count <= 0` 就说明进程不存在了。\n\n## CPU 利用率计算\n\n在计算 CPU 利用率的时候有两种模式：irix（默认）、solaris。如果是 irix 模式，CPU 利用率会出现大于 100% 的情况，如果是 solaris 模式，会考虑 CPU 核数，所以 CPU 利用率不会大于 100%。\n\n## 采集更多指标\n\n`gather_more_metrics` 默认没有打开，即不会采集进程资源利用情况。如果想要采集，就打开 `gather_more_metrics` 这个配置即可。其中最为特殊的是 `jvm`，如果想要采集 jvm 指标，需要先安装好 jstat，然后再打开 `jvm` 这个配置。\n\n## gather_total\n\n比如进程名字是 mysql 的进程，同时可能运行了多个，我们想知道这个机器上的所有 mysql 的进程占用的总的 cpu、mem、fd 等，就设置 gather_total = true，当然，对于 uptime 和 limit 的采集，gather_total 的时候是取的多个进程的最小值。\n\n## gather_per_pid\n\n还是拿 mysql 举例，一个机器上可能同时运行了多个，我们可能想知道每个 mysql 进程的资源占用情况，此时就要启用 gather_per_pid 的配置，设置为 true，此时会采集每个进程的资源占用情况，并附上 pid 作为标签来区分\n\n## 告警规则\n\n夜莺内置了进程监控的告警规则，克隆到自己的业务组下即可使用。\n\n## 仪表盘\n\n夜莺内置了进程监控的仪表盘，克隆到自己的业务组下即可使用。', '进程监控', '1732763490', '王杨(822032277)', '1732763490', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('87', 'IPMI', '# Read metrics from the bare metal servers via freeipmi\n[[instances]]\n# target指定是本地采集还是远程采集\n#target=\"localhost\"\n# 指定采集的用户名和密码，这里务必保证ipmi命令能获取正确输出，不是网上查到一个用户名 密码就可以。\n#user = \"user\"\n#pass = \"1234\"\n\n# ipmi协议版本，支持1.5 和 2.0\n#driver = \"LAN_2_0\"\n\n# 指定特权用户名\n#privilege = \"user\"\n\n## session-timeout, ms\n#timeout = 100000\n\n# 支持的采集器  bmc, bmc-watchdog, ipmi, chassis, dcmi, sel，sm-lan-mode\n# 默认使用 bmc, ipmi, chassis和dcmi，建议保持下列配置便于仪表盘更好的展示\ncollectors = [ \"bmc\", \"ipmi\", \"chassis\", \"sel\", \"dcmi\"]\n\n# 不关注的传感器，指定id 排除掉\n#exclude_sensor_ids = [ 2, 29, 32, 50, 52, 55 ]\n\n# 如果你想使用定制化的参数覆盖内置的命令，可以修改以下内容； 建议保持注释\n#[instances.collector_cmd]\n#ipmi = \"sudo\"\n#sel = \"sudo\"\n#[instances.default_args]\n#ipmi = [ \"--bridge-sensors\" ]\n#[instances.custom_args]\n#ipmi = [ \"--bridge-sensors\" ]\n#sel = [ \"ipmi-sel\" ]\n', 'ipmi插件是从ipmi exporter迁移过来。 基本原理是通过执行ipmi的一系列命令并将命令输出转换为指标，如果ipmi没有配置好，是无法采集到指标的，请务必将ipmi配置好。\n\ncategraf的ipmi插件配置举例如下：\n```toml\n# Read metrics from the bare metal servers via freeipmi\n[[instances]]\n# target指定是本地采集还是远程采集\n#target=\"localhost\"\n# 指定采集的用户名和密码，这里务必保证ipmi命令能获取正确输出，不是网上查到一个用户名 密码就可以。\n#user = \"user\"\n#pass = \"1234\"\n\n# ipmi协议版本，支持1.5 和 2.0 \n#driver = \"LAN_2_0\"\n\n# 指定特权用户名\n#privilege = \"user\"\n\n## session-timeout, ms\n#timeout = 100000\n\n# 支持的采集器  bmc, bmc-watchdog, ipmi, chassis, dcmi, sel，sm-lan-mode\n# 默认使用 bmc, ipmi, chassis和dcmi，建议保持下列配置便于仪表盘更好的展示\ncollectors = [ \"bmc\", \"ipmi\", \"chassis\", \"sel\", \"dcmi\"]\n\n# 不关注的传感器，指定id 排除掉\n#exclude_sensor_ids = [ 2, 29, 32, 50, 52, 55 ]\n\n# 如果你想使用定制化的参数覆盖内置的命令，可以修改以下内容； 建议保持注释\n#[instances.collector_cmd]\n#ipmi = \"sudo\"\n#sel = \"sudo\"\n#[instances.default_args]\n#ipmi = [ \"--bridge-sensors\" ]\n#[instances.custom_args]\n#ipmi = [ \"--bridge-sensors\" ]\n#sel = [ \"ipmi-sel\" ]\n```', 'ipmi监控指标采集', '1732763646', '王杨(822032277)', '1732763646', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('88', 'SNMP', '#SNMP不同厂商设备有不同的私有 oid，以下以Cisco设备为例\n[[instances]]\n\nagents = [\"udp://127.0.0.1\"]\n\ntimeout = \"5s\"\n\nversion = 2\n\n## Path to mib files\n## Used by the gosmi translator.\n## To add paths when translating with netsnmp, use the MIBDIRS environment variable\n##path = [\"/usr/share/snmp/DCN\"]\n##translator = \"gosmi\"\n\ncommunity = \"public\"\n\nagent_host_tag = \"DCN\"\n\nretries = 3\n\nmax_repetitions = 100\n\n##运行时间\n[[instances.field]]\noid = \"1.3.6.1.2.1.1.3.0\"\nname = \"sys_uptime\"\nconversion = \"float(2)\"\n\n[[instances.field]]\noid = \"1.3.6.1.4.1.6339.100.1.11.10.0\"\nname = \"cpu_usage\"\n\n[[instances.field]]\noid = \"1.3.6.1.4.1.6339.100.1.11.6.0\"\nname = \"mem_max\"\n\n[[instances.field]]\noid = \"1.3.6.1.4.1.6339.100.1.11.7.0\"\nname = \"mem_use\"\n\n#端口总和\n[[instances.field]]\nname = \"TotalPorts\"\noid = \"1.3.6.1.4.1.6339.100.3.1.0\"\n\n##设备名称\n[[instances.field]]\noid = \"1.3.6.1.2.1.1.5.0\"\nname = \"sys_name\"\nis_tag = true\n\n##产品型号\n[[instances.field]]\nname = \"sys_pm\"\noid = \"1.3.6.1.4.1.6339.100.25.1.1.1.0\"\nis_tag = true\n\n#本机IP\n[[instances.field]]\nname = \"LocalIP\"\noid = \"1.3.6.1.2.1.4.20.1.1\"\nis_tag = true\n\n#接口表信息\n[[instances.table]]\nname = \"interface\"\ninherit_tags = [\"sys_name\",\"sys_pm\",\"LocalIP\"]\n\n#各个端口\n[[instances.table.field]]\nname = \"ifDescr\"\noid = \"1.3.6.1.2.1.2.2.1.2\"\nis_tag = true\n\n[[instances.table.field]]\nname = \"ifSpeed\"\noid = \"1.3.6.1.2.1.2.2.1.5\"\nconversion = \"float(6)\"\n#is_tag = true\n\n[[instances.table.field]]\nname = \"ifOperStatus\"\noid = \"1.3.6.1.2.1.2.2.1.8\"\n#is_tag = true\n\n[[instances.table.field]]\nname = \"ifOutOctets\"\noid = \"1.3.6.1.2.1.2.2.1.16\"\n\n[[instances.table.field]]\nname = \"ifInOctets\"\noid = \"1.3.6.1.2.1.2.2.1.10\"\n\n\n#聚合状态\n#oid = \"1.3.6.1.4.1.6339.100.14.2.1.4.1.1.*\"\n\n#聚合端口\n#oid = \"1.3.6.1.4.1.6339.100.14.3.1.2\"\n', '# snmp\n\n> 监控网络设备，主要是通过 SNMP 协议，Categraf、Telegraf、Datadog-Agent、snmp_exporter 都提供了这个能力。\n\nCategraf 从 v0.2.13 版本开始把 Telegraf 的 snmp 插件集成了进来，推荐大家采用这个插件来监控网络设备。这个插件的核心逻辑是：要采集什么指标，直接配置对应的 oid 即可，而且可以把一些 oid 采集到的数据当做时序数据的标签，非常非常灵活。\n\n当然，弊端也有，因为 SNMP 体系里有大量的私有 oid，比如不同的设备获取 CPU、内存利用率的 oid 都不一样，这就需要为不同的型号的设备采用不同的配置，维护起来比较麻烦，需要大量的积累。这里我倡议大家把不同的设备型号的采集配置积累到 [这里](https://github.com/flashcatcloud/categraf/tree/main/inputs/snmp)，每个型号一个文件夹，长期积累下来，那将是利人利己的好事。不知道如何提 PR 的可以联系我们。\n\n另外，也不用太悲观，针对网络设备而言，大部分监控数据的采集都是通用 oid 就可以搞定的，举个例子：\n\n```toml\ninterval = 120\n\n[[instances]]\nagents = [\"udp://172.30.15.189:161\"]\n\ninterval_times = 1\ntimeout = \"5s\"\nversion = 2\ncommunity = \"public\"\nagent_host_tag = \"switch_ip\"\nretries = 1\n\n[[instances.field]]\noid = \"RFC1213-MIB::sysUpTime.0\"\nname = \"uptime\"\n\n[[instances.field]]\noid = \"RFC1213-MIB::sysName.0\"\nname = \"source\"\nis_tag = true\n\n[[instances.table]]\noid = \"IF-MIB::ifTable\"\nname = \"interface\"\ninherit_tags = [\"source\"]\n\n[[instances.table.field]]\noid = \"IF-MIB::ifDescr\"\nname = \"ifDescr\"\nis_tag = true\n\n```\n\n上面的样例是 v2 版本的配置，如果是 v3 版本，校验方式举例：\n\n```toml\nversion = 3\nsec_name = \"managev3user\"\nauth_protocol = \"SHA\"\nauth_password = \"example.Demo.c0m\"\n```\n\n另外，snmp 的采集，建议大家部署单独的 Categraf 来做，因为不同监控对象采集频率可能不同，比如边缘交换机，我们 5min 采集一次就够了，核心交换机可以配置的频繁一些，比如 60s 或者 120s。\n\n> 注意：如果采集的过于频繁，有些老款的交换机可能会被打挂，或者被限流，被限流的结果就是图上看到的是断点。\n\n## 扩展阅读\n\n- [SNMP(简单网络管理协议)简介](https://flashcat.cloud/blog/snmp-introduction/)\n- [SNMP命令相关参数介绍](https://flashcat.cloud/blog/snmp-command-arguments/)\n- [通过 Categraf SNMP 插件采集监控数据](https://flashcat.cloud/blog/snmp-metrics-collect-by-categraf/)\n\n## 排错\n\n要想通过 categraf 采集到 snmp 数据，首先要保证 categraf 所在的机器能够连通网络设备，可以通过 snmpget 命令来做测试：\n\n```bash\nsnmpget -v2c -c public 172.30.15.189 RFC1213-MIB::sysUpTime.0\n```\n\n如果 snmpget 都跑不通，就得先解决这个问题，比如是 snmpd 没有启动，或者防火墙限制了 snmp 的访问，还是 snmpget 命令没有安装，等等。这些问题，gpt 和 google 都可以解决，这里不再赘述。', '监控网络设备等', '1732763965', '王杨(822032277)', '1732763965', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('89', 'Kafka', '# # collect interval\n# interval = 15\n\n############################################################################\n# !!! uncomment [[instances]] to enable this plugin\n[[instances]]\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# append some labels to metrics\n# cluster is a preferred tag with the cluster name. If none is provided, the first of kafka_uris will be used\nlabels = { cluster=\"kafka-cluster-01\" }\n\n# log level only for kafka exporter\nlog_level = \"error\"\n\n# Address (host:port) of Kafka server.\n# kafka_uris = [\"127.0.0.1:9092\",\"127.0.0.1:9092\",\"127.0.0.1:9092\"]\nkafka_uris = []\n\n# Connect using SASL/PLAIN\n# Default is false\n# use_sasl = false\n\n# Only set this to false if using a non-Kafka SASL proxy\n# Default is true\n# use_sasl_handshake = false\n\n# SASL user name\n# sasl_username = \"username\"\n\n# SASL user password\n# sasl_password = \"password\"\n\n# The SASL SCRAM SHA algorithm sha256 or sha512 as mechanism\n# sasl_mechanism = \"\"\n\n# Connect using TLS\n# use_tls = false\n\n# The optional certificate authority file for TLS client authentication\n# ca_file = \"\"\n\n# The optional certificate file for TLS client authentication\n# cert_file = \"\"\n\n# The optional key file for TLS client authentication\n# key_file = \"\"\n\n# If true, the server\'s certificate will not be checked for validity. This will make your HTTPS connections insecure\n# insecure_skip_verify = true\n\n# Kafka broker version\n# Default is 2.0.0\n# kafka_version = \"2.0.0\"\n\n# if you need to use a group from zookeeper\n# Default is false\n# use_zookeeper_lag = false\n\n# Address array (hosts) of zookeeper server.\n# zookeeper_uris = []\n\n# Metadata refresh interval\n# Default is 1m\n# metadata_refresh_interval = \"1m\"\n\n# Whether show the offset/lag for all consumer group, otherwise, only show connected consumer groups, default is true\n# Default is true\n# offset_show_all = true\n\n# If true, all scrapes will trigger kafka operations otherwise, they will share results. WARN: This should be disabled on large clusters\n# Default is false\n# allow_concurrency = false\n\n# Maximum number of offsets to store in the interpolation table for a partition\n# Default is 1000\n# max_offsets = 1000\n\n# How frequently should the interpolation table be pruned, in seconds.\n# Default is 30\n# prune_interval_seconds = 30\n\n# Regex filter for topics to be monitored\n# Default is \".*\"\n# topics_filter_regex = \".*\"\n\n# Regex filter for consumer groups to be monitored\n# Default is \".*\"\n# groups_filter_regex = \".*\"\n\n# if rename  kafka_consumergroup_uncommitted_offsets to kafka_consumergroup_lag\n# Default is false\n# rename_uncommit_offset_to_lag = false\n\n\n# if disable calculating lag rate\n# Default is false\n# disable_calculate_lag_rate = false', '# kafka plugin\n\nKafka 的核心指标，其实都是通过 JMX 的方式暴露的。对于 JMX 暴露的指标，使用 jolokia 或者使用 jmx_exporter 那个 jar 包来采集即可，不需要本插件。\n\n本插件主要是采集的消费者延迟数据，这个数据无法通过 Kafka 服务端的 JMX 拿到。\n\n本插件 fork 自 [https://github.com/davidmparrott/kafka_exporter](https://github.com/davidmparrott/kafka_exporter)（以下简称 davidmparrott 版本），davidmparrott 版本 fork 自 [https://github.com/danielqsj/kafka_exporter](https://github.com/danielqsj/kafka_exporter)（以下简称 danielqsj 版本）。\n\ndanielqsj 版本作为原始版本, github 版本也相对活跃, prometheus 生态使用较多。davidmparrott 版本与 danielqsj 版本相比, 有以下 metric 名字不同：\n\n| davidmparrott 版本  | danielqsj 版本 |\n| ---- | ---- |\n| kafka_consumergroup_uncommit_offsets  | kafka_consumergroup_lag |\n| kafka_consumergroup_uncommit_offsets_sum  | kafka_consumergroup_lag_sum |\n| kafka_consumergroup_uncommitted_offsets_zookeeper | kafka_consumergroup_lag_zookeeper |\n\n如果想使用 danielqsj 版本的 metric, 在 `[[instances]]` 中进行如下配置:\n\n```toml\nrename_uncommit_offset_to_lag = true\n```\n\ndavidmparrott 版本比 danielqsj 版本多了以下 metric，这些指标是对延迟速率做了预估计算：\n\n- kafka_consumer_lag_millis\n- kafka_consumer_lag_interpolation\n- kafka_consumer_lag_extrapolation\n\n为什么要计算速率？因为 lag 很大，但是消费很快，是不会积压的，而 lag 很小，消费很慢，仍然会积压，所以，通过 lag 大小是没法判断积压风险的。通过计算历史消费速率，来判断积压风险会更为合理。要计算这个速率，需要占用较多内存，可以通过如下配置关闭这个计算逻辑：\n\n```toml\ndisable_calculate_lag_rate = true\n```\n\n## 采集配置\n\ncategraf 配置文件：`conf/input.kafka/kafka.toml`。配置样例如下：\n\n```toml\n[[instances]]\nlog_level = \"error\"\nkafka_uris = [\"192.168.0.250:9092\"]\nlabels = { cluster=\"kafka-cluster-01\", service=\"kafka\" }\n```\n\n完整的带有注释的配置如下：\n\n```toml\n[[instances]]\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# append some labels to metrics\n# cluster is a preferred tag with the cluster name. If none is provided, the first of kafka_uris will be used\nlabels = { cluster=\"kafka-cluster-01\" }\n\n# log level only for kafka exporter\nlog_level = \"error\"\n\n# Address (host:port) of Kafka server.\n# kafka_uris = [\"127.0.0.1:9092\",\"127.0.0.1:9092\",\"127.0.0.1:9092\"]\nkafka_uris = []\n\n# Connect using SASL/PLAIN\n# Default is false\n# use_sasl = false\n\n# Only set this to false if using a non-Kafka SASL proxy\n# Default is true\n# use_sasl_handshake = false\n\n# SASL user name\n# sasl_username = \"username\"\n\n# SASL user password\n# sasl_password = \"password\"\n\n# The SASL SCRAM SHA algorithm sha256 or sha512 as mechanism\n# sasl_mechanism = \"\"\n\n# Connect using TLS\n# use_tls = false\n\n# The optional certificate authority file for TLS client authentication\n# ca_file = \"\"\n\n# The optional certificate file for TLS client authentication\n# cert_file = \"\"\n\n# The optional key file for TLS client authentication\n# key_file = \"\"\n\n# If true, the server\'s certificate will not be checked for validity. This will make your HTTPS connections insecure\n# insecure_skip_verify = true\n\n# Kafka broker version\n# Default is 2.0.0\n# kafka_version = \"2.0.0\"\n\n# if you need to use a group from zookeeper\n# Default is false\n# use_zookeeper_lag = false\n\n# Address array (hosts) of zookeeper server.\n# zookeeper_uris = []\n\n# Metadata refresh interval\n# Default is 1m\n# metadata_refresh_interval = \"1m\"\n\n# Whether show the offset/lag for all consumer group, otherwise, only show connected consumer groups, default is true\n# Default is true\n# offset_show_all = true\n\n# If true, all scrapes will trigger kafka operations otherwise, they will share results. WARN: This should be disabled on large clusters\n# Default is false\n# allow_concurrency = false\n\n# Maximum number of offsets to store in the interpolation table for a partition\n# Default is 1000\n# max_offsets = 1000\n\n# How frequently should the interpolation table be pruned, in seconds.\n# Default is 30\n# prune_interval_seconds = 30\n\n# Regex filter for topics to be monitored\n# Default is \".*\"\n# topics_filter_regex = \".*\"\n\n# Regex filter for consumer groups to be monitored\n# Default is \".*\"\n# groups_filter_regex = \".*\"\n\n# if rename  kafka_consumergroup_uncommitted_offsets to kafka_consumergroup_lag\n# Default is false\n# rename_uncommit_offset_to_lag = false\n\n\n# if disable calculating lag rate\n# Default is false\n# disable_calculate_lag_rate = false\n```\n\n## 告警规则\n\n夜莺提供了内置的 Kafka 告警规则，克隆到自己的业务组下即可使用。\n\n![20230801162030](https://download.flashcat.cloud/ulric/20230801162030.png)\n\n## 仪表盘：\n\n夜莺提供了内置的 Kafka 仪表盘，克隆到自己的业务组下即可使用。\n\n![20230801162017](https://download.flashcat.cloud/ulric/20230801162017.png)', 'kafka消费数据采集', '1732764105', '王杨(822032277)', '1732764105', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('90', 'Filecount', '# # collect interval\n# interval = 15\n\n[[instances]]\n# # append some labels for series\n# labels = { region=\"cloud\", product=\"n9e\" }\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n## Directories to gather stats about.\n## This accept standard unit glob matching rules, but with the addition of\n## ** as a \"super asterisk\". ie:\n##   /var/log/**    -> recursively find all directories in /var/log and count files in each directories\n##   /var/log/*/*   -> find all directories with a parent dir in /var/log and count files in each directories\n##   /var/log       -> count all files in /var/log and all of its subdirectories\n## directories = [\"/var/cache/apt\", \"/tmp\"]\ndirectories = [\"/tmp\"]\n\n## Only count files that match the name pattern. Defaults to \"*\".\nfile_name = \"*\"\n\n## Count files in subdirectories. Defaults to true.\nrecursive = true\n\n## Only count regular files. Defaults to true.\nregular_only = true\n\n## Follow all symlinks while walking the directory tree. Defaults to false.\nfollow_symlinks = false\n\n## Only count files that are at least this size. If size is\n## a negative number, only count files that are smaller than the\n## absolute value of size. Acceptable units are B, KiB, MiB, KB, ...\n## Without quotes and units, interpreted as size in bytes.\nsize = \"0B\"\n\n## Only count files that have not been touched for at least this\n## duration. If mtime is negative, only count files that have been\n## touched in this duration. Defaults to \"0s\".\nmtime = \"0s\"\n', '# Filecount Input Plugin\n\nforked from telegraf/inputs.filecount\n\nReports the number and total size of files in specified directories.\n\n\n## Configuration\n\n```toml filecount.toml\n# # collect interval\n# interval = 15\n\n[[instances]]\n# # append some labels for series\n# labels = { region=\"cloud\", product=\"n9e\" }\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n## Directories to gather stats about.\n## This accept standard unit glob matching rules, but with the addition of\n## ** as a \"super asterisk\". ie:\n##   /var/log/**    -> recursively find all directories in /var/log and count files in each directories\n##   /var/log/*/*   -> find all directories with a parent dir in /var/log and count files in each directories\n##   /var/log       -> count all files in /var/log and all of its subdirectories\n## directories = [\"/var/cache/apt\", \"/tmp\"]\ndirectories = [\"/tmp\", \"/root\"]\n\n## Only count files that match the name pattern. Defaults to \"*\".\nfile_name = \"*\"\n\n## Count files in subdirectories. Defaults to true.\nrecursive = true\n\n## Only count regular files. Defaults to true.\nregular_only = true\n\n## Follow all symlinks while walking the directory tree. Defaults to false.\nfollow_symlinks = false\n\n## Only count files that are at least this size. If size is\n## a negative number, only count files that are smaller than the\n## absolute value of size. Acceptable units are B, KiB, MiB, KB, ...\n## Without quotes and units, interpreted as size in bytes.\nsize = \"0B\"\n\n## Only count files that have not been touched for at least this\n## duration. If mtime is negative, only count files that have been\n## touched in this duration. Defaults to \"0s\".\nmtime = \"0s\"\n\n```\n\n## Metrics\n\n- filecount\n    - tags:\n        - directory (the directory path)\n    - fields:\n        - count (integer)\n        - size_bytes (integer)\n        - oldest_file_timestamp (int, unix time nanoseconds)\n        - newest_file_timestamp (int, unix time nanoseconds)\n\n## Example Output\n\n```text\n13:25:07 filecount_count agent_hostname=host1 directory=/tmp 319\n13:25:07 filecount_size_bytes agent_hostname=host1 directory=/tmp 83196547\n13:25:07 filecount_oldest_file_timestamp agent_hostname=host1 directory=/tmp 0\n13:25:07 filecount_newest_file_timestamp agent_hostname=host1 directory=/tmp 1692336254306413522\n```\n', '监控目录文件数量及大小', '1732764200', '王杨(822032277)', '1732764200', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('91', 'MongoDB', '[[instances]]\n# log level, enum: panic, fatal, error, warn, warning, info, debug, trace, defaults to info.\nlog_level = \"info\"\n# append some const labels to metrics\n# NOTICE! the instance label is required for dashboards\nlabels = { instance=\"mongo-cluster-01\" }\n\n# mongodb dsn, see https://www.mongodb.com/docs/manual/reference/connection-string/\n# mongodb_uri = \"mongodb://127.0.0.1:27017\"\nmongodb_uri = \"\"\n# if you don\'t specify the username or password in the mongodb_uri, you can set here. \n# This will overwrite the dsn, it would be helpful when special characters existing in the username or password and you don\'t want to encode them.\n# NOTICE! this user must be granted enough rights to query needed stats, see ../inputs/mongodb/README.md\nusername = \"username@Bj\"\npassword = \"password@Bj\"\n# if set to true, use the direct connection way\n# direct_connect = true\n\n# collect all means you collect all the metrics, if set, all below enable_xxx flags in this section will be ignored\ncollect_all = true\n# if set to true, collect databases metrics\n# enable_db_stats = true\n# if set to true, collect getDiagnosticData metrics\n# enable_diagnostic_data = true\n# if set to true, collect replSetGetStatus metrics\n# enable_replicaset_status = true\n# if set to true, collect top metrics by admin command\n# enable_top_metrics = true\n# if set to true, collect index metrics. You should specify one of the coll_stats_namespaces and the discovering_mode flags.\n# enable_index_stats = true\n# if set to true, collect collections metrics. You should specify one of the coll_stats_namespaces and the discovering_mode flags.\n# enable_coll_stats = true\n\n# Only get stats for the collections matching this list of namespaces. if none set, discovering_mode will be enabled.\n# Example: db1.col1,db.col1\n# coll_stats_namespaces = []\n# Only get stats for index with the collections matching this list of namespaces.\n# Example: db1.col1,db.col1\n# index_stats_collections = []\n# if set to true, replace -1 to DESC for label key_name of the descending_index metrics\n# enable_override_descending_index = true\n\n# which exposes metrics with 0.1x compatible metric names has been implemented which simplifies migration from the old version to the current version.\n# compatible_mode = true\n\n\n# [[instances]]\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# log_level = \"error\"\n\n# append some labels to metrics\n# labels = { instance=\"mongo-cluster-02\" }\n# mongodb_uri = \"mongodb://username:password@127.0.0.1:27017\"\n# collect_all = true\n# compatible_mode = true', '# mongodb\n\nmongodb 监控采集插件，由 [mongodb-exporter](https://github.com/percona/mongodb_exporter)封装而来。\n\n## Configuration\n\n配置文件示例：\n\n```toml\n[[instances]]\n# log level, enum: panic, fatal, error, warn, warning, info, debug, trace, defaults to info.\nlog_level = \"info\"\n# append some const labels to metrics\n# NOTICE! the instance label is required for dashboards\nlabels = { instance=\"mongo-cluster-01\" }\n\n# mongodb dsn, see https://www.mongodb.com/docs/manual/reference/connection-string/\n# mongodb_uri = \"mongodb://127.0.0.1:27017\"\nmongodb_uri = \"\"\n# if you don\'t specify the username or password in the mongodb_uri, you can set here. \n# This will overwrite the dsn, it would be helpful when special characters existing in the username or password and you don\'t want to encode them.\n# NOTICE! this user must be granted enough rights to query needed stats, see ../inputs/mongodb/README.md\nusername = \"username@Bj\"\npassword = \"password@Bj\"\n# if set to true, use the direct connection way\n# direct_connect = true\n\n# collect all means you collect all the metrics, if set, all below enable_xxx flags in this section will be ignored\ncollect_all = true\n# if set to true, collect databases metrics\n# enable_db_stats = true\n# if set to true, collect getDiagnosticData metrics\n# enable_diagnostic_data = true\n# if set to true, collect replSetGetStatus metrics\n# enable_replicaset_status = true\n# if set to true, collect top metrics by admin command\n# enable_top_metrics = true\n# if set to true, collect index metrics. You should specify one of the coll_stats_namespaces and the discovering_mode flags.\n# enable_index_stats = true\n# if set to true, collect collections metrics. You should specify one of the coll_stats_namespaces and the discovering_mode flags.\n# enable_coll_stats = true\n\n# Only get stats for the collections matching this list of namespaces. if none set, discovering_mode will be enabled.\n# Example: db1.col1,db.col1\n# coll_stats_namespaces = []\n# Only get stats for index with the collections matching this list of namespaces.\n# Example: db1.col1,db.col1\n# index_stats_collections = []\n# if set to true, replace -1 to DESC for label key_name of the descending_index metrics\n# enable_override_descending_index = true\n\n# which exposes metrics with 0.1x compatible metric names has been implemented which simplifies migration from the old version to the current version.\n# compatible_mode = true\n\n\n# [[instances]]\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# log_level = \"error\"\n\n# append some labels to metrics\n# labels = { instance=\"mongo-cluster-02\" }\n# mongodb_uri = \"mongodb://username:password@127.0.0.1:27017\"\n# collect_all = true\n# compatible_mode = true\n```\n\ncategraf 作为一个 client 连接 MongoDB，需要有足够的权限来收集指标，具体的权限配置请参考[官方文档](https://www.mongodb.com/docs/manual/reference/built-in-roles/#mongodb-authrole-clusterMonitor)。至少具有以下权限才可以：\n\n```json\n{\n    \"role\":\"clusterMonitor\",\n    \"db\":\"admin\"\n},\n{\n    \"role\":\"read\",\n    \"db\":\"local\"\n}\n```\n\n授权操作样例：\n\n```shell\nmongo -h xxx -u xxx -p xxx --authenticationDatabase admin\n> use admin\n> db.createUser({user:\"categraf\",pwd:\"categraf\",roles: [{role:\"read\",db:\"local\"},{\"role\":\"clusterMonitor\",\"db\":\"admin\"}]})\n```\n\n## 监控大盘和告警规则\n\n夜莺内置了 MongoDB 的告警规则和监控大盘，克隆到自己的业务组使用即可。虽然文件后缀是 `_exporter` 也可以使用，因为 categraf 这个插件是基于 mongodb-exporter 封装的。', 'mongodb监控指标采集', '1732764267', '王杨(822032277)', '1732764267', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('92', 'Prometheus', '# prometheus 各种exporter的指标采集,主要采集/metrics暴露出来的指标\n# # collect interval\n# interval = 15\n\n[[instances]]\nurls = [\n#     \"http://localhost:19000/metrics\"\n]\n\nurl_label_key = \"instance\"\nurl_label_value = \"{{.Host}}\"\n\n## Scrape Services available in Consul Catalog\n# [instances.consul]\n#   enabled = false\n#   agent = \"http://localhost:8500\"\n#   query_interval = \"5m\"\n\n#   [[instances.consul.query]]\n#     name = \"a service name\"\n#     tag = \"a service tag\"\n#     url = \'http://{{if ne .ServiceAddress \"\"}}{{.ServiceAddress}}{{else}}{{.Address}}{{end}}:{{.ServicePort}}/{{with .ServiceMeta.metrics_path}}{{.}}{{else}}metrics{{end}}\'\n#     [instances.consul.query.tags]\n#       host = \"{{.Node}}\"\n\n# bearer_token_string = \"\"\n\n# e.g. /run/secrets/kubernetes.io/serviceaccount/token\n# bearer_token_file = \"\"\n\n# # basic auth\n# username = \"\"\n# password = \"\"\n\n# headers = [\"X-From\", \"categraf\"]\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# labels = {}\n\n# support glob\n# ignore_metrics = [ \"go_*\" ]\n\n# support glob\n# ignore_label_keys = []\n\n# timeout for every url\n# timeout = \"3s\"\n\n## Optional TLS Config\n# use_tls = false\n# tls_min_version = \"1.2\"\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = true', '# prometheus\n\nprometheus 插件的作用，就是抓取 `/metrics` 接口的数据，上报给服务端。通过，各类 exporter 会暴露 `/metrics` 接口数据，越来越多的开源组件也会内置 prometheus SDK，吐出 prometheus 格式的监控数据，比如 rabbitmq 插件，其 README 中就有介绍。\n\n增加了两个配置：url_label_key 和 url_label_value。为了标识监控数据是从哪个 scrape url 拉取的，会为监控数据附一个标签来标识这个 url，默认的标签 KEY 是用 instance，当然，也可以改成别的，不过不建议。url_label_value 是标签值，支持 go template 语法，如果为空，就是整个 url 的内容，也可以通过模板变量只取一部分，比如 `http://localhost:9104/metrics`，只想取 IP 和端口部分，就可以写成：\n\n```ini\nurl_label_value = \"{{.Host}}\"\n```\n\n如果 HTTP scheme 部分和 `/metrics` Path 部分都想取，可以这么写：\n\n```ini\nurl_label_value = \"{{.Scheme}}://{{.Host}}{{.Path}}\"\n```\n\n相关变量是用这个方法生成的，供大家参考：\n\n```go\nfunc (ul *UrlLabel) GenerateLabel(u *url.URL) (string, string, error) {\n	if ul.LabelValue == \"\" {\n		return ul.LabelKey, u.String(), nil\n	}\n\n	dict := map[string]string{\n		\"Scheme\":   u.Scheme,\n		\"Host\":     u.Host,\n		\"Hostname\": u.Hostname(),\n		\"Port\":     u.Port(),\n		\"Path\":     u.Path,\n		\"Query\":    u.RawQuery,\n		\"Fragment\": u.Fragment,\n	}\n\n	var buffer bytes.Buffer\n	err := ul.LabelValueTpl.Execute(&buffer, dict)\n	if err != nil {\n		return \"\", \"\", err\n	}\n\n	return ul.LabelKey, buffer.String(), nil\n}\n```', 'prometheus exporter指标采集', '1732764577', '王杨(822032277)', '1732764577', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('93', 'Netstat_Filter', '# # collect interval\n# interval = 15\n[[instances]]\n# laddr_ip = \"\"\n# laddr_port = 0\n# raddr_ip = \"\"\n# raddr_port = 0', '# netstat_filter\n\n该插件采集网络连接情况，并根据用户条件进行过滤统计，以达到监控用户关心链接情况\n## 指标列表\ntcp_established  \ntcp_syn_sent\ntcp_syn_recv\ntcp_fin_wait1\ntcp_fin_wait2\ntcp_time_wait\ntcp_close\ntcp_close_wait\ntcp_last_ack\ntcp_listen\ntcp_closing\ntcp_none\ntcp_send_queue\ntcp_recv_queue\n\n## 功能说明\n对源IP、源端口、目标IP和目标端口过滤后进行网卡recv-Q、send-Q进行采集，该指标可以很好反应出指定连接的质量，例如rtt时间过长，导致收到服务端ack确认很慢就会使send-Q长期大于0，可以及时通过监控发现，从而提前优化网络或程序\n\n当过滤结果为多个连接时会将send和recv值进行加和\n例如：\n配置文件``raddr_port = 11883``\n当本地和不同IP的11883都有连接建立的情况下，会将多条连接的结果进行加和。或在并发多连接的情况下，会合并加合，总之过滤的越粗略被加合数就会越多。\n\n多条规则请复制``[[instances]]``进行配置\n\n## 注意事项\nnetstat_filter_tcp_send_queue和netstat_filter_tcp_recv_queue指标目前只支持linux。windows用户默认为0。', '网络连接情况', '1732764646', '王杨(822032277)', '1732764646', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('94', 'Oracle', '# # collect interval\n# interval = 15\n\n#[[instances]]\n# address = \"10.1.2.3:1521/orcl\"\n# username = \"monitor\"\n# password = \"123456\"\n# is_sys_dba = false\n# is_sys_oper = false\n# disable_connection_pool = false\n# max_open_connections = 5\n# # interval = global.interval * interval_times\n# interval_times = 1\n# labels = { region=\"cloud\" }\n\n# [[instances.metrics]]\n# mesurement = \"sessions\"\n# label_fields = [ \"status\", \"type\" ]\n# metric_fields = [ \"value\" ]\n# timeout = \"3s\"\n# request = \'\'\'\n# SELECT status, type, COUNT(*) as value FROM v$session GROUP BY status, type\n# \'\'\'\n\n# [[instances]]\n# address = \"192.168.10.10:1521/orcl\"\n# username = \"monitor\"\n# password = \"123456\"\n# is_sys_dba = false\n# is_sys_oper = false\n# disable_connection_pool = false\n# max_open_connections = 5\n# # labels = { region=\"local\" }\n\n\n[[metrics]]\nmesurement = \"sessions\"\nlabel_fields = [ \"status\", \"type\" ]\nmetric_fields = [ \"value\" ]\ntimeout = \"3s\"\nrequest = \'\'\'\nSELECT status, type, COUNT(*) as value FROM v$session GROUP BY status, type\n\'\'\'\n\n[[metrics]]\nmesurement = \"lock\"\nmetric_fields = [ \"cnt\" ]\ntimeout = \"3s\"\nrequest = \'\'\'\nSELECT COUNT(*) AS cnt\n  FROM ALL_OBJECTS A, V$LOCKED_OBJECT B, SYS.GV_$SESSION C\n WHERE A.OBJECT_ID = B.OBJECT_ID\n   AND B.PROCESS = C.PROCESS\n\'\'\'\n\n[[metrics]]\nmesurement = \"slow_queries\"\nmetric_fields = [ \"p95_time_usecs\" , \"p99_time_usecs\"]\ntimeout = \"3s\"\nrequest = \'\'\'\nselect  percentile_disc(0.95)  within group (order by elapsed_time) as p95_time_usecs,\n  percentile_disc(0.99)  within group (order by elapsed_time) as p99_time_usecs\nfrom v$sql where last_active_time >= sysdate - 5/(24*60)\n\'\'\'\n\n[[metrics]]\nmesurement = \"resource\"\nlabel_fields = [ \"resource_name\" ]\nmetric_fields = [ \"current_utilization\", \"limit_value\" ]\ntimeout = \"3s\"\nrequest = \'\'\'\nSELECT resource_name,current_utilization,CASE WHEN TRIM(limit_value) LIKE \'UNLIMITED\' THEN \'-1\' ELSE TRIM(limit_value) END as limit_value FROM v$resource_limit\n\'\'\'\n\n[[metrics]]\nmesurement = \"asm_diskgroup\"\nlabel_fields = [ \"name\" ]\nmetric_fields = [ \"total\", \"free\" ]\ntimeout = \"3s\"\nrequest = \'\'\'\nSELECT name,total_mb*1024*1024 as total,free_mb*1024*1024 as free FROM v$asm_diskgroup_stat where exists (select 1 from v$datafile where name like \'+%\')\n\'\'\'\nIgnoreZeroResult = true\n\n[[metrics]]\nmesurement = \"activity\"\nmetric_fields = [ \"value\" ]\nfield_to_append = \"name\"\ntimeout = \"3s\"\nrequest = \'\'\'\nSELECT name, value FROM v$sysstat WHERE name IN (\'parse count (total)\', \'execute count\', \'user commits\', \'user rollbacks\')\n\'\'\'\n\n[[metrics]]\nmesurement = \"process\"\nmetric_fields = [ \"count\" ]\ntimeout = \"3s\"\nrequest = \'\'\'\nSELECT COUNT(*) as count FROM v$process\n\'\'\'\n\n[[metrics]]\nmesurement = \"wait_time\"\nmetric_fields = [ \"value\" ]\nlabel_fields = [\"wait_class\"]\ntimeout = \"3s\"\nrequest = \'\'\'\nSELECT\n  n.wait_class as WAIT_CLASS,\n  round(m.time_waited/m.INTSIZE_CSEC,3) as VALUE\nFROM\n  v$waitclassmetric  m, v$system_wait_class n\nWHERE\n  m.wait_class_id=n.wait_class_id AND n.wait_class != \'Idle\'\n\'\'\'\n\n[[metrics]]\nmesurement = \"tablespace\"\nlabel_fields = [ \"tablespace\", \"type\" ]\nmetric_fields = [ \"bytes\", \"max_bytes\", \"free\" ]\ntimeout = \"3s\"\nrequest = \'\'\'\nSELECT\n    dt.tablespace_name as tablespace,\n    dt.contents as type,\n    dt.block_size * dtum.used_space as bytes,\n    dt.block_size * dtum.tablespace_size as max_bytes,\n    dt.block_size * (dtum.tablespace_size - dtum.used_space) as free\nFROM  dba_tablespace_usage_metrics dtum, dba_tablespaces dt\nWHERE dtum.tablespace_name = dt.tablespace_name\nORDER by tablespace\n\'\'\'\n\n[[metrics]]\nmesurement = \"sysmetric\"\nmetric_fields = [ \"value\" ]\nfield_to_append = \"metric_name\"\ntimeout = \"3s\"\nrequest = \'\'\'\nselect METRIC_NAME,VALUE from v$sysmetric where group_id=2\n\'\'\'', '# Oracle plugin\n\nOracle 插件，用于监控 Oracle 数据库。默认无法跑在 Windows 上。如果你的 Oracle 部署在 Windows 上，也没问题，使用部署在 Linux 上的 Categraf 远程监控 Windows 上的 Oracle，也行得通。\n\nOracle 插件的核心监控原理，就是执行下面 [这些 SQL 语句](https://github.com/flashcatcloud/categraf/blob/main/conf/input.oracle/metric.toml)，然后解析出结果，上报到监控服务端。\n\n以其中一个为例：\n\n```toml\n[[metrics]]\nmesurement = \"activity\"\nmetric_fields = [ \"value\" ]\nfield_to_append = \"name\"\ntimeout = \"3s\"\nrequest = \'\'\'\nSELECT name, value FROM v$sysstat WHERE name IN (\'parse count (total)\', \'execute count\', \'user commits\', \'user rollbacks\')\n\'\'\'\n```\n\n- mesurement：指标类别\n- label_fields：作为 label 的字段\n- metric_fields：作为 metric 的字段，因为是作为 metric 的字段，所以这个字段的值必须是数字\n- field_to_append：表示这个字段附加到 metric_name 后面，作为 metric_name 的一部分\n- timeout：超时时间\n- request：具体查询的 SQL 语句\n\n如果你想监控的指标，默认没有采集，只需要增加自定义的 `[[metrics]]` 配置即可。', 'Oracle数据库监控', '1732764712', '王杨(822032277)', '1732764712', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('95', 'Tengine', '## collect interval\n# interval = 15\n\n## Set the mapping of extra tags in batches\n[mappings]\n# \"http://127.0.0.1/us\" = { \"job\" = \"local\" }\n# \"https://www.baidu.com/us\" = { \"job\" = \"baidu\" }\n\n[[instances]]\n## An array of Tengine reqstat module URI to gather stats.\nurls = [\n#    \"http://127.0.0.1/us\",\n#    \"https://www.baidu.com/us\"\n]\n\n## append some labels for series\n# labels = { region=\"cloud\", product=\"n9e\" }\n\n## interval = global.interval * interval_times\n# interval_times = 1\n\n## HTTP response timeout (default: 5s)\n# response_timeout = \"5s\"\n\n## Whether to follow redirects from the server (defaults to false)\n# follow_redirects = false\n\n## Optional HTTP Basic Auth Credentials\n#username = \"admin\"\n#password = \"admin\"\n\n## Optional headers\n# headers = [\"X-From\", \"categraf\", \"X-Xyz\", \"abc\"]\n\n## Optional TLS Config\n# use_tls = false\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = false', '# Tengine Input Plugin\n\nThe tengine plugin gathers metrics from the\n[Tengine Web Server](http://tengine.taobao.org/) via the\n[reqstat](http://tengine.taobao.org/document/http_reqstat.html) module.\n\n## Tengine Configuration Example\n\n```\nhttp {\n\n    req_status_zone server \"$host,$server_addr:$server_port\" 10M;\n    #req_status_zone_add_indicator server $limit;\n    req_status server;\n    \n    server {\n        location /us {\n            req_status_show;\n            #req_status_show_field req_total $limit;\n            #allow 127.0.0.1/32;\n            #deny all;\n        }\n        \n        #set $limit 0;\n        #if ($arg_limit = \'1\') {\n        #    set $limit 1;\n        #}\n    }\n}\n```\n\n## Metrics\n\n- Measurement\n    - tags:\n        - target\n        - target_port\n        - server_name\n        - server_schema\n    - fields:\n        - bytes_in (integer, total number of bytes received from client)\n        - bytes_out (integer, total number of bytes sent to client)\n        - conn_total (integer, total number of accepted connections)\n        - req_total (integer, total number of processed requests)\n        - http_2xx (integer, total number of 2xx requests)\n        - http_3xx (integer, total number of 3xx requests)\n        - http_4xx (integer, total number of 4xx requests)\n        - http_5xx (integer, total number of 5xx requests)\n        - http_other_status (integer, total number of other requests)\n        - rt (integer, accumulation or rt)\n        - ups_req (integer, total number of requests calling for upstream)\n        - ups_rt (integer, accumulation or upstream rt)\n        - ups_tries (integer, total number of times calling for upstream)\n        - http_200 (integer, total number of 200 requests)\n        - http_206 (integer, total number of 206 requests)\n        - http_302 (integer, total number of 302 requests)\n        - http_304 (integer, total number of 304 requests)\n        - http_403 (integer, total number of 403 requests)\n        - http_404 (integer, total number of 404 requests)\n        - http_416 (integer, total number of 416 requests)\n        - http_499 (integer, total number of 499 requests)\n        - http_500 (integer, total number of 500 requests)\n        - http_502 (integer, total number of 502 requests)\n        - http_503 (integer, total number of 503 requests)\n        - http_504 (integer, total number of 504 requests)\n        - http_508 (integer, total number of 508 requests)\n        - http_other_detail_status (integer, total number of requests of other status codes*http_ups_4xx total number of requests of upstream 4xx)\n        - http_ups_5xx (integer, total number of requests of upstream 5xx)\n\n## Example Output\n\n```text\ntengine_rt agent_hostname=zy-fat project=matrix server_name=www.baidu.com server_schema=http target=127.0.0.1 target_port=80 37634\ntengine_ups_rt agent_hostname=zy-fat project=matrix server_name=www.baidu.com server_schema=http target=127.0.0.1 target_port=80 37394\ntengine_http_499 agent_hostname=zy-fat project=matrix server_name=www.baidu.com server_schema=http target=127.0.0.1 target_port=80 0\ntengine_http_504 agent_hostname=zy-fat project=matrix server_name=www.baidu.com server_schema=http target=127.0.0.1 target_port=80 0\ntengine_bytes_in agent_hostname=zy-fat project=matrix server_name=www.baidu.com server_schema=http target=127.0.0.1 target_port=80 129592\ntengine_http_4xx agent_hostname=zy-fat project=matrix server_name=www.baidu.com server_schema=http target=127.0.0.1 target_port=80 535\ntengine_http_other_status agent_hostname=zy-fat project=matrix server_name=www.baidu.com server_schema=https target=127.0.0.1 target_port=80 0\ntengine_http_200 agent_hostname=zy-fat project=matrix server_name=www.baidu.com server_schema=https target=127.0.0.1 target_port=80 14452\ntengine_http_499 agent_hostname=zy-fat project=matrix server_name=www.baidu.com server_schema=https target=127.0.0.1 target_port=80 0\ntengine_http_503 agent_hostname=zy-fat project=matrix server_name=www.baidu.com server_schema=https target=127.0.0.1 target_port=80 0\ntengine_http_504 agent_hostname=zy-fat project=matrix server_name=www.baidu.com server_schema=https target=127.0.0.1 target_port=80 0\ntengine_http_500 agent_hostname=zy-fat project=matrix server_name=www.baidu.com server_schema=https target=127.0.0.1 target_port=80 0\ntengine_http_ups_4xx agent_hostname=zy-fat project=matrix server_name=www.baidu.com server_schema=https target=127.0.0.1 target_port=80 13\ntengine_http_ups_5xx agent_hostname=zy-fat project=matrix server_name=www.baidu.com server_schema=https target=127.0.0.1 target_port=80 1\n```', '通过reqstat模块监控', '1732765027', '王杨(822032277)', '1732765027', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('96', 'Supervisor', '# Gathers information about processes that running under supervisor using XML-RPC API\n[[instances]]\n## Url of supervisor\'s XML-RPC endpoint if basic auth enabled in supervisor http server,\n## than you have to add credentials to url (ex. http://login:pass@localhost:9001/RPC2)\n# url = \"http://login:pass@localhost:9001/RPC2\", eg: url = \"http://localhost:9001/RPC2\"\nurl =\"\"\n## With settings below you can manage gathering additional information about processes\n## If both of them empty, then all additional information will be collected.\n## Currently supported supported additional metrics are: pid, rc\n# metrics_include = []\n# metrics_exclude = [\"pid\", \"rc\"]', '# Supervisor\n\n此插件通过使用XML-RPC API收集在supervisor下运行的进程信息。\n\nsupervisor的最低测试版本为3.3.2。\n\n## Supervisor 配置\n\n这个插件需要在supervisor中启用HTTP服务器，同时建议在HTTP服务器上启用基本身份验证。使用基本认证时，请确保在插件的url设置中包含用户名和密码。下面是一个`inet_http_server`部分的supervisor配置示例，该配置可以与默认插件配置一起工作：\n\n```ini\n[inet_http_server]\nport = 127.0.0.1:9001\nusername = user\npassword = pass\n```\n\n## 全局配置选项\n\n除了特定于插件的配置设置外，插件还支持额外的全局和插件配置设置。这些设置用于修改指标、标签和字段或创建别名和配置排序等。\n\n## 配置\n\n```toml\n# Gathers information about processes that running under supervisor using XML-RPC API\n[[instances]]\n  ## Url of supervisor\'s XML-RPC endpoint if basic auth enabled in supervisor http server,\n  ## than you have to add credentials to url (ex. http://login:pass@localhost:9001/RPC2)\n  # url = \"http://login:pass@localhost:9001/RPC2\"\n  ## With settings below you can manage gathering additional information about processes\n  ## If both of them empty, then all additional information will be collected.\n  ## Currently supported supported additional metrics are: pid, rc\n  # metrics_include = []\n  # metrics_exclude = [\"pid\", \"rc\"]\n```\n\n注意，`url = \"http://login:pass@localhost:9001/RPC2\"`中的`login:pass`是用户名和密码。相关信息可以参见您的supervisor配置文件。\n\n### 可选指标\n\n通过在配置文件中设置`metrics_include`和`metrics_exclude`参数，用于控制哪些指标(metrics)应该被包括(`include`)或排除(`exclude`)在监控数据中。这两个配置选项为用户提供了细粒度控制，以便根据特定需要定制收集的数据。这在处理大量指标或只关心某些特定指标的情况下尤其有用。\n\n#### metrics_include\n\n- `metrics_include` 选项允许你指定一个指标名称列表，仅这些指标会被收集和发送。如果设置了这个选项，那么只有列表中的指标会被包含，其他所有指标都会被忽略。\n- 这个选项通常用于限制数据的收集范围，以减少网络流量、存储需求或者仅仅关注一小部分重要指标。\n- 格式通常是一个指标名称的数组，例如：`metrics_include = [\"cpu_usage_idle\", \"cpu_usage_user\"]`。\n\n#### metrics_exclude\n\n- 相反，`metrics_exclude` 选项允许你指定一个指标名称列表，这些指标将不会被收集和发送。如果设置了这个选项，那么列表中的指标会被排除，其他所有指标都会被包含。\n- 这个选项用于从收集的数据中排除不感兴趣或不相关的指标，有助于减少处理和存储无用数据的负担。\n- 格式同样是一个指标名称的数组，例如：`metrics_exclude = [\"memory_free\", \"memory_cached\"]`。\n\n#### 使用注意事项\n\n- 如果同时使用`metrics_include`和`metrics_exclude`，首先应用`metrics_include`过滤规则，然后应用`metrics_exclude`。这意味着如果一个指标在`metrics_include`中被明确包含，在`metrics_exclude`中也被明确排除，那么这个指标最终将被排除。\n- 这两个配置选项的工作原理和具体可用值可能依赖于具体的插件。有的插件可能允许根据指标的某些属性或标签来进行包含或排除。\n- 正确使用这两个配置选项可以显著改善Telegraf的性能和效率，特别是在资源受限的环境中或当监控系统规模较大时。\n\n#### 示例\n\n假设你使用Categraf监控系统性能，并使用`cpu`插件收集CPU使用情况的指标。如果你只对CPU的闲置时间和用户时间感兴趣，可以使用以下配置：\n\n```toml\n[[instances]]\n  ## 仅收集CPU的闲置时间和用户使用时间的指标\n  metrics_include = [\"cpu_usage_idle\", \"cpu_usage_user\"]\n```\n\n或者，如果你想收集所有CPU相关指标，但排除闲置时间和用户时间，可以使用：\n\n```toml\n[[instances]]\n  ## 排除CPU的闲置时间和用户使用时间的指标\n  metrics_exclude = [\"cpu_usage_idle\", \"cpu_usage_user\"]\n```\n\n通过精细控制指标的收集，你可以优化监控设置，确保只处理对你最重要的信息。\n\n### 服务器标签\n\n服务器标签用于标识指标源服务器。你可以选择默认使用supervisor的http端点的`host:port`，或者你可以使用在supervisor配置文件中设置的supervisor的标识字符串。\n\n## 指标\n\n- supervisor_processes\n    - tags：\n        - source（supervisor实例的主机名或IP地址）\n        - port（supervisor的HTTP服务器端口号）\n        - id（supervisor的标识字符串）\n        - name（进程名）\n        - group（进程组）\n    - fields：\n        - state（int，参见参考表）\n        - uptime（int，秒）\n        - pid（int，可选）\n        - exitCode（int，可选）\n\n- supervisor_instance\n    - tags：\n        - source（supervisor实例的主机名或IP地址）\n        - port（supervisor的HTTP服务器端口号）\n        - id（supervisor的标识字符串）\n    - fields：\n        - state（int，参见参考表）\n\n### Supervisor进程状态字段参考表\n\n| 状态码  | 状态名      | 描述                                    |\n|------|----------|---------------------------------------|\n| 0    | STOPPED  | 进程因停止请求停止了，或者从未启动。                    |\n| 10   | STARTING | 进程因启动请求正在启动。                          |\n| 20   | RUNNING  | 进程正在运行。                               |\n| 30   | BACKOFF  | 进程进入STARTING状态但随后过快退出，未能移动到RUNNING状态。 |\n| 40   | STOPPING | 进程因停止请求正在停止。                          |\n| 100  | EXITED   | 进程已从RUNNING状态退出（预期地或意外地）。             |\n| 200  | FATAL    | 无法成功启动进程。                             |\n| 1000 | UNKNOWN  | 进程处于未知状态（supervisord编程错误）。            |\n\n### Supervisor实例状态字段参考\n\n| 状态码 | 状态名     | 描述                 |\n|-----|---------|--------------------|\n| 2   | FATAL   | Supervisor遇到了严重错误。 |\n| 1   | RUNNING | Supervisor正在正常工作。  |\n| 0   |         |                    |', '通过XML-RPC API采集supervisor下运行的进程', '1732765165', '王杨(822032277)', '1732765165', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('97', 'Processes', '# # collect interval\n# interval = 15\n\n# # force use ps command to gather\n# force_ps = false\n\n# # force use /proc to gather\n# force_proc = false', '# processes\n\n统计进程数量，比如 running 的有多少，sleeping 的有多少，total 有多少\n\n## 监控大盘\n\n该插件没有单独的监控大盘，OS 的监控大盘统一放到 system 下面了', '监控进程数量', '1732765569', '王杨(822032277)', '1732765569', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('98', 'ClickHouse', '# # collect interval\n# interval = 15\n\n# Read metrics from one or many ClickHouse servers\n[[instances]]\n  ## Username for authorization on ClickHouse server\n  username = \"default\"\n\n  ## Password for authorization on ClickHouse server\n  # password = \"\"\n\n  ## HTTP(s) timeout while getting metrics values\n  ## The timeout includes connection time, any redirects, and reading the\n  ## response body.\n  # timeout = 5\n\n  ## List of servers for metrics scraping\n  ## metrics scrape via HTTP(s) clickhouse interface\n  ## https://clickhouse.tech/docs/en/interfaces/http/\n  # servers = [\"http://127.0.0.1:8123\"]\n\n  ## If \"auto_discovery\"\" is \"true\" plugin tries to connect to all servers\n  ## available in the cluster with using same \"user:password\" described in\n  ## \"user\" and \"password\" parameters and get this server hostname list from\n  ## \"system.clusters\" table. See\n  ## - https://clickhouse.tech/docs/en/operations/system_tables/#system-clusters\n  ## - https://clickhouse.tech/docs/en/operations/server_settings/settings/#server_settings_remote_servers\n  ## - https://clickhouse.tech/docs/en/operations/table_engines/distributed/\n  ## - https://clickhouse.tech/docs/en/operations/table_engines/replication/#creating-replicated-tables\n  # auto_discovery = true\n\n  ## Filter cluster names in \"system.clusters\" when \"auto_discovery\" is \"true\"\n  ## when this filter present then \"WHERE cluster IN (...)\" filter will apply\n  ## please use only full cluster names here, regexp and glob filters is not\n  ## allowed for \"/etc/clickhouse-server/config.d/remote.xml\"\n  ## <yandex>\n  ##  <remote_servers>\n  ##    <my-own-cluster>\n  ##        <shard>\n  ##          <replica><host>clickhouse-ru-1.local</host><port>9000</port></replica>\n  ##          <replica><host>clickhouse-ru-2.local</host><port>9000</port></replica>\n  ##        </shard>\n  ##        <shard>\n  ##          <replica><host>clickhouse-eu-1.local</host><port>9000</port></replica>\n  ##          <replica><host>clickhouse-eu-2.local</host><port>9000</port></replica>\n  ##        </shard>\n  ##    </my-onw-cluster>\n  ##  </remote_servers>\n  ##\n  ## </yandex>\n  ##\n  ## example: cluster_include = [\"my-own-cluster\"]\n  # cluster_include = []\n\n  ## Filter cluster names in \"system.clusters\" when \"auto_discovery\" is\n  ## \"true\" when this filter present then \"WHERE cluster NOT IN (...)\"\n  ## filter will apply\n  ##    example: cluster_exclude = [\"my-internal-not-discovered-cluster\"]\n  # cluster_exclude = []\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n    # [[instances.metrics]]\n    # mesurement = \"sessions\"\n    # label_fields = [ \"status\", \"type\" ]\n    # metric_fields = [ \"value\" ]\n    # timeout = \"3s\"\n    # request = \'\'\'\n    # SELECT status, type, COUNT(*) as value FROM v$session GROUP BY status, type\n    # \'\'\'', '# ClickHouse Input Plugin\n\nThis plugin gathers the statistic data from\n[ClickHouse](https://github.com/ClickHouse/ClickHouse) server.\n\n## Global configuration options\n\nIn addition to the plugin-specific configuration settings, plugins support\nadditional global and plugin configuration settings. These settings are used to\nmodify metrics, tags, and field or create aliases and configure ordering, etc.\nSee the [CONFIGURATION.md][CONFIGURATION.md] for more details.\n\n## Configuration\n\n```toml\n# # collect interval\n# interval = 15\n\n# Read metrics from one or many ClickHouse servers\n[[instances]]\n  ## Username for authorization on ClickHouse server\n  username = \"default\"\n\n  ## Password for authorization on ClickHouse server\n  # password = \"\"\n\n  ## HTTP(s) timeout while getting metrics values\n  ## The timeout includes connection time, any redirects, and reading the\n  ## response body.\n  # timeout = 5\n\n  ## List of servers for metrics scraping\n  ## metrics scrape via HTTP(s) clickhouse interface\n  ## https://clickhouse.tech/docs/en/interfaces/http/\n  servers = [\"http://127.0.0.1:8123\"]\n\n  ## If \"auto_discovery\"\" is \"true\" plugin tries to connect to all servers\n  ## available in the cluster with using same \"user:password\" described in\n  ## \"user\" and \"password\" parameters and get this server hostname list from\n  ## \"system.clusters\" table. See\n  ## - https://clickhouse.tech/docs/en/operations/system_tables/#system-clusters\n  ## - https://clickhouse.tech/docs/en/operations/server_settings/settings/#server_settings_remote_servers\n  ## - https://clickhouse.tech/docs/en/operations/table_engines/distributed/\n  ## - https://clickhouse.tech/docs/en/operations/table_engines/replication/#creating-replicated-tables\n  # auto_discovery = true\n\n  ## Filter cluster names in \"system.clusters\" when \"auto_discovery\" is \"true\"\n  ## when this filter present then \"WHERE cluster IN (...)\" filter will apply\n  ## please use only full cluster names here, regexp and glob filters is not\n  ## allowed for \"/etc/clickhouse-server/config.d/remote.xml\"\n  ## <yandex>\n  ##  <remote_servers>\n  ##    <my-own-cluster>\n  ##        <shard>\n  ##          <replica><host>clickhouse-ru-1.local</host><port>9000</port></replica>\n  ##          <replica><host>clickhouse-ru-2.local</host><port>9000</port></replica>\n  ##        </shard>\n  ##        <shard>\n  ##          <replica><host>clickhouse-eu-1.local</host><port>9000</port></replica>\n  ##          <replica><host>clickhouse-eu-2.local</host><port>9000</port></replica>\n  ##        </shard>\n  ##    </my-onw-cluster>\n  ##  </remote_servers>\n  ##\n  ## </yandex>\n  ##\n  ## example: cluster_include = [\"my-own-cluster\"]\n  # cluster_include = []\n\n  ## Filter cluster names in \"system.clusters\" when \"auto_discovery\" is\n  ## \"true\" when this filter present then \"WHERE cluster NOT IN (...)\"\n  ## filter will apply\n  ##    example: cluster_exclude = [\"my-internal-not-discovered-cluster\"]\n  # cluster_exclude = []\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n```\n\n## Metrics\n\n- clickhouse_events (see [system.events][system.events] for details)\n\n    - tags:\n        - source (ClickHouse server hostname)\n        - cluster (Name of the cluster [optional])\n        - shard_num (Shard number in the cluster [optional])\n    - fields:\n        - all rows from [system.events][system.events]\n- clickhouse_metrics (see [system.metrics][system.metrics] for details)\n\n    - tags:\n        - source (ClickHouse server hostname)\n        - cluster (Name of the cluster [optional])\n        - shard_num (Shard number in the cluster [optional])\n    - fields:\n        - all rows from [system.metrics][system.metrics]\n- clickhouse_asynchronous_metrics (see [system.asynchronous_metrics][system.asynchronous_metrics]\n  for details)\n\n    - tags:\n        - source (ClickHouse server hostname)\n        - cluster (Name of the cluster [optional])\n        - shard_num (Shard number in the cluster [optional])\n    - fields:\n        - all rows from [system.asynchronous_metrics][system.asynchronous_metrics]\n- clickhouse_tables\n\n    - tags:\n        - source (ClickHouse server hostname)\n        - table\n        - database\n        - cluster (Name of the cluster [optional])\n        - shard_num (Shard number in the cluster [optional])\n    - fields:\n        - bytes\n        - parts\n        - rows\n- clickhouse_zookeeper (see [system.zookeeper][system.zookeeper] for details)\n\n    - tags:\n        - source (ClickHouse server hostname)\n        - cluster (Name of the cluster [optional])\n        - shard_num (Shard number in the cluster [optional])\n    - fields:\n        - root_nodes (count of node where path=/)\n- clickhouse_replication_queue (see [system.replication_queue][system.replication_queue] for details)\n\n    - tags:\n        - source (ClickHouse server hostname)\n        - cluster (Name of the cluster [optional])\n        - shard_num (Shard number in the cluster [optional])\n    - fields:\n        - too_many_tries_replicas (count of replicas which have `num_tries > 1`)\n- clickhouse_detached_parts (see [system.detached_parts][system.detached_parts] for details)\n\n    - tags:\n        - source (ClickHouse server hostname)\n        - cluster (Name of the cluster [optional])\n        - shard_num (Shard number in the cluster [optional])\n    - fields:\n        - detached_parts (total detached parts for all tables and databases\n          from [system.detached_parts][system.detached_parts])\n- clickhouse_dictionaries (see [system.dictionaries][system.dictionaries] for details)\n\n    - tags:\n        - source (ClickHouse server hostname)\n        - cluster (Name of the cluster [optional])\n        - shard_num (Shard number in the cluster [optional])\n        - dict_origin (xml Filename when dictionary created from *_dictionary.xml,\n          database.table when dictionary created from DDL)\n    - fields:\n        - is_loaded (0 - when dictionary data not successful load, 1 - when\n          dictionary data loading fail\n        - bytes_allocated (bytes allocated in RAM after a dictionary loaded)\n- clickhouse_mutations (see [system.mutations][system.mutations] for details)\n\n    - tags:\n        - source (ClickHouse server hostname)\n        - cluster (Name of the cluster [optional])\n        - shard_num (Shard number in the cluster [optional])\n    - fields:\n        - running - gauge which show how much mutation doesn\'t complete now\n        - failed - counter which show total failed mutations from first\n          clickhouse-server run\n        - completed - counter which show total successful finished mutations\n          from first clickhouse-server run\n- clickhouse_disks (see [system.disks][system.disks] for details)\n\n    - tags:\n        - source (ClickHouse server hostname)\n        - cluster (Name of the cluster [optional])\n        - shard_num (Shard number in the cluster [optional])\n        - name (disk name in storage configuration)\n        - path (path to disk)\n    - fields:\n        - free_space_percent - 0-100, gauge which show current percent of\n          free disk space bytes relative to total disk space bytes\n        - keep_free_space_percent - 0-100, gauge which show current percent\n          of required keep free disk bytes relative to total disk space bytes\n- clickhouse_processes (see [system.processes][system.processes] for details)\n\n    - tags:\n        - source (ClickHouse server hostname)\n        - cluster (Name of the cluster [optional])\n        - shard_num (Shard number in the cluster [optional])\n    - fields:\n        - percentile_50 - float gauge which show 50% percentile (quantile 0.5) for\n          `elapsed` field of running processes\n        - percentile_90 - float gauge which show 90% percentile (quantile 0.9) for\n          `elapsed` field of running processes\n        - longest_running - float gauge which show maximum value for `elapsed`\n          field of running processes\n- clickhouse_text_log (see [system.text_log][system.text_log] for details)\n\n    - tags:\n        - source (ClickHouse server hostname)\n        - cluster (Name of the cluster [optional])\n        - shard_num (Shard number in the cluster [optional])\n        - level (message level, only messages with level less or equal Notice are\n          collected)\n    - fields:\n        - messages_last_10_min - gauge which show how many messages collected\n\n## Example Output\n\n```text\nclickhouse_events,cluster=test_cluster_two_shards_localhost,host=kshvakov,source=localhost,shard_num=1 read_compressed_bytes=212i,arena_alloc_chunks=35i,function_execute=85i,merge_tree_data_writer_rows=3i,rw_lock_acquired_read_locks=421i,file_open=46i,io_buffer_alloc_bytes=86451985i,inserted_bytes=196i,regexp_created=3i,real_time_microseconds=116832i,query=23i,network_receive_elapsed_microseconds=268i,merge_tree_data_writer_compressed_bytes=1080i,arena_alloc_bytes=212992i,disk_write_elapsed_microseconds=556i,inserted_rows=3i,compressed_read_buffer_bytes=81i,read_buffer_from_file_descriptor_read_bytes=148i,write_buffer_from_file_descriptor_write=47i,merge_tree_data_writer_blocks=3i,soft_page_faults=896i,hard_page_faults=7i,select_query=21i,merge_tree_data_writer_uncompressed_bytes=196i,merge_tree_data_writer_blocks_already_sorted=3i,user_time_microseconds=40196i,compressed_read_buffer_blocks=5i,write_buffer_from_file_descriptor_write_bytes=3246i,io_buffer_allocs=296i,created_write_buffer_ordinary=12i,disk_read_elapsed_microseconds=59347044i,network_send_elapsed_microseconds=1538i,context_lock=1040i,insert_query=1i,system_time_microseconds=14582i,read_buffer_from_file_descriptor_read=3i 1569421000000000000\nclickhouse_asynchronous_metrics,cluster=test_cluster_two_shards_localhost,host=kshvakov,source=localhost,shard_num=1 jemalloc.metadata_thp=0i,replicas_max_relative_delay=0i,jemalloc.mapped=1803177984i,jemalloc.allocated=1724839256i,jemalloc.background_thread.run_interval=0i,jemalloc.background_thread.num_threads=0i,uncompressed_cache_cells=0i,replicas_max_absolute_delay=0i,mark_cache_bytes=0i,compiled_expression_cache_count=0i,replicas_sum_queue_size=0i,number_of_tables=35i,replicas_max_merges_in_queue=0i,replicas_max_inserts_in_queue=0i,replicas_sum_merges_in_queue=0i,replicas_max_queue_size=0i,mark_cache_files=0i,jemalloc.background_thread.num_runs=0i,jemalloc.active=1726210048i,uptime=158i,jemalloc.retained=380481536i,replicas_sum_inserts_in_queue=0i,uncompressed_cache_bytes=0i,number_of_databases=2i,jemalloc.metadata=9207704i,max_part_count_for_partition=1i,jemalloc.resident=1742442496i 1569421000000000000\nclickhouse_metrics,cluster=test_cluster_two_shards_localhost,host=kshvakov,source=localhost,shard_num=1 replicated_send=0i,write=0i,ephemeral_node=0i,zoo_keeper_request=0i,distributed_files_to_insert=0i,replicated_fetch=0i,background_schedule_pool_task=0i,interserver_connection=0i,leader_replica=0i,delayed_inserts=0i,global_thread_active=41i,merge=0i,readonly_replica=0i,memory_tracking_in_background_schedule_pool=0i,memory_tracking_for_merges=0i,zoo_keeper_session=0i,context_lock_wait=0i,storage_buffer_bytes=0i,background_pool_task=0i,send_external_tables=0i,zoo_keeper_watch=0i,part_mutation=0i,disk_space_reserved_for_merge=0i,distributed_send=0i,version_integer=19014003i,local_thread=0i,replicated_checks=0i,memory_tracking=0i,memory_tracking_in_background_processing_pool=0i,leader_election=0i,revision=54425i,open_file_for_read=0i,open_file_for_write=0i,storage_buffer_rows=0i,rw_lock_waiting_readers=0i,rw_lock_waiting_writers=0i,rw_lock_active_writers=0i,local_thread_active=0i,query_preempted=0i,tcp_connection=1i,http_connection=1i,read=2i,query_thread=0i,dict_cache_requests=0i,rw_lock_active_readers=1i,global_thread=43i,query=1i 1569421000000000000\nclickhouse_tables,cluster=test_cluster_two_shards_localhost,database=system,host=kshvakov,source=localhost,shard_num=1,table=trace_log bytes=754i,parts=1i,rows=1i 1569421000000000000\nclickhouse_tables,cluster=test_cluster_two_shards_localhost,database=default,host=kshvakov,source=localhost,shard_num=1,table=example bytes=326i,parts=2i,rows=2i 1569421000000000000\n```\n\n[CONFIGURATION.md]: ../../../docs/CONFIGURATION.md#plugins\n[system.asynchronous_metrics]: https://clickhouse.tech/docs/en/operations/system-tables/asynchronous_metrics/\n[system.detached_parts]: https://clickhouse.tech/docs/en/operations/system-tables/detached_parts/\n[system.dictionaries]: https://clickhouse.tech/docs/en/operations/system-tables/dictionaries/\n[system.disks]: https://clickhouse.tech/docs/en/operations/system-tables/disks/\n[system.events]: https://clickhouse.tech/docs/en/operations/system-tables/events/\n[system.metrics]: https://clickhouse.tech/docs/en/operations/system-tables/metrics/\n[system.mutations]: https://clickhouse.tech/docs/en/operations/system-tables/mutations/\n[system.processes]: https://clickhouse.tech/docs/en/operations/system-tables/processes/\n[system.replication_queue]: https://clickhouse.com/docs/en/operations/system-tables/replication_queue/\n[system.text_log]: https://clickhouse.tech/docs/en/operations/system-tables/text_log/\n[system.zookeeper]: https://clickhouse.tech/docs/en/operations/system-tables/zookeeper/', 'ClickHouse监控指标采集', '1732765730', '王杨(822032277)', '1732765730', '王杨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('99', 'Redis_Sentinel', '# # collect interval\n# interval = 15\n\n[[instances]]\n# [protocol://][:password]@address[:port]\n# e.g. servers = [\"tcp://localhost:26379\"]\nservers = []\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n# add some dimension data by labels\n# labels = {}\n\n## Optional TLS Config\n# use_tls = false\n# tls_min_version = \"1.2\"\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = true', '# Redis Sentinel Input Plugin\n\nA plugin for Redis Sentinel to monitor multiple Sentinel instances that are\nmonitoring multiple Redis servers and replicas.\n\n## Configuration\n\n```toml\n[[instances]]\n# [protocol://][:password]@address[:port]\n# e.g. servers = [\"tcp://localhost:26379\"]\nservers = []\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n# add some dimension data by labels\n# labels = {}\n\n## Optional TLS Config\n# use_tls = false\n# tls_min_version = \"1.2\"\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = true\n```\n\n## Measurements & Fields\n\nThe plugin gathers the results of these commands and measurements:\n\n* `sentinel masters` - `redis_sentinel_masters`\n* `sentinel sentinels` - `redis_sentinels`\n* `sentinel replicas` - `redis_replicas`\n* `info all` - `redis_sentinel`\n\nThe `has_quorum` field in `redis_sentinel_masters` is from calling the command\n`sentinels ckquorum`.\n\nThere are 5 remote network requests made for each server listed in the config.\n\n## Metrics\n\n* redis_sentinel_masters\n  * tags:\n    * host\n    * master\n    * port\n    * source\n\n  * fields:\n    * config_epoch (int)\n    * down_after_milliseconds (int)\n    * failover_timeout (int)\n    * flags (string)\n    * has_quorum (bool)\n    * info_refresh (int)\n    * ip (string)\n    * last_ok_ping_reply (int)\n    * last_ping_reply (int)\n    * last_ping_sent (int)\n    * link_pending_commands (int)\n    * link_refcount (int)\n    * num_other_sentinels (int)\n    * num_slaves (int)\n    * parallel_syncs (int)\n    * port (int)\n    * quorum (int)\n    * role_reported (string)\n    * role_reported_time (int)\n\n* redis_sentinel_sentinels\n  * tags:\n    * host\n    * master\n    * port\n    * sentinel_ip\n    * sentinel_port\n    * source\n\n  * fields:\n    * down_after_milliseconds (int)\n    * flags (string)\n    * last_hello_message (int)\n    * last_ok_ping_reply (int)\n    * last_ping_reply (int)\n    * last_ping_sent (int)\n    * link_pending_commands (int)\n    * link_refcount (int)\n    * name (string)\n    * voted_leader (string)\n    * voted_leader_epoch (int)\n\n* redis_sentinel_replicas\n  * tags:\n    * host\n    * master\n    * port\n    * replica_ip\n    * replica_port\n    * source\n\n  * fields:\n    * down_after_milliseconds (int)\n    * flags (string)\n    * info_refresh (int)\n    * last_ok_ping_reply (int)\n    * last_ping_reply (int)\n    * last_ping_sent (int)\n    * link_pending_commands (int)\n    * link_refcount (int)\n    * master_host (string)\n    * master_link_down_time (int)\n    * master_link_status (string)\n    * master_port (int)\n    * name (string)\n    * role_reported (string)\n    * role_reported_time (int)\n    * slave_priority (int)\n    * slave_repl_offset (int)\n\n* redis_sentinel\n  * tags:\n    * host\n    * port\n    * source\n\n  * fields:\n    * active_defrag_hits (int)\n    * active_defrag_key_hits (int)\n    * active_defrag_key_misses (int)\n    * active_defrag_misses (int)\n    * blocked_clients (int)\n    * client_recent_max_input_buffer (int)\n    * client_recent_max_output_buffer (int)\n    * clients (int)\n    * evicted_keys (int)\n    * expired_keys (int)\n    * expired_stale_perc (float)\n    * expired_time_cap_reached_count (int)\n    * instantaneous_input_kbps (float)\n    * instantaneous_ops_per_sec (int)\n    * instantaneous_output_kbps (float)\n    * keyspace_hits (int)\n    * keyspace_misses (int)\n    * latest_fork_usec (int)\n    * lru_clock (int)\n    * migrate_cached_sockets (int)\n    * pubsub_channels (int)\n    * pubsub_patterns (int)\n    * redis_version (string)\n    * rejected_connections (int)\n    * sentinel_masters (int)\n    * sentinel_running_scripts (int)\n    * sentinel_scripts_queue_length (int)\n    * sentinel_simulate_failure_flags (int)\n    * sentinel_tilt (int)\n    * slave_expires_tracked_keys (int)\n    * sync_full (int)\n    * sync_partial_err (int)\n    * sync_partial_ok (int)\n    * total_commands_processed (int)\n    * total_connections_received (int)\n    * total_net_input_bytes (int)\n    * total_net_output_bytes (int)\n    * uptime_ns (int, nanoseconds)\n    * used_cpu_sys (float)\n    * used_cpu_sys_children (float)\n    * used_cpu_user (float)\n    * used_cpu_user_children (float)\n\n', 'Redis_Sentinel监控指标采集', '1733126558', '王杨(822032277)', '1733126558', '王杨(822032277)');
