/*
Navicat MySQL Data Transfer

Source Server         : n9e
Source Server Version : 50738
Source Host           : 116.211.139.40:3306
Source Database       : n9e_v5

Target Server Type    : MYSQL
Target Server Version : 50738
File Encoding         : 65001

Date: 2024-12-12 17:04:09
*/

SET FOREIGN_KEY_CHECKS=0;

-- ----------------------------
-- Table structure for plugin_tpl
-- ----------------------------
DROP TABLE IF EXISTS `plugin_tpl`;
CREATE TABLE `plugin_tpl` (
  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT '''unique identifier''',
  `ident` varchar(191) NOT NULL COMMENT '''identifier of plugin_tpl''',
  `toml` text NOT NULL COMMENT '''toml of plugin_tpl''',
  `readme` text NOT NULL COMMENT '''readme of plugin_tpl''',
  `note` varchar(4096) NOT NULL COMMENT '''description of plugin_tpl''',
  `created_at` bigint(20) NOT NULL DEFAULT '0' COMMENT '''create time''',
  `created_by` varchar(191) NOT NULL DEFAULT '' COMMENT '''creator''',
  `updated_at` bigint(20) NOT NULL DEFAULT '0' COMMENT '''update time''',
  `updated_by` varchar(191) NOT NULL DEFAULT '' COMMENT '''updater''',
  PRIMARY KEY (`id`),
  KEY `idx_ident` (`ident`)
) ENGINE=InnoDB AUTO_INCREMENT=100 DEFAULT CHARSET=utf8mb4;

-- ----------------------------
-- Records of plugin_tpl
-- ----------------------------
INSERT INTO `plugin_tpl` VALUES ('67', 'MySQL', '# MySqlç›‘æ§,ç¡®ä¿è¿æ¥è´¦æˆ·æœ‰åªè¯»ç›¸å…³æƒé™ï¼š\n#GRANT PROCESS, REPLICATION CLIENT, SELECT ON *.* TO \'monitor\'@\'127.0.0.1\';\n\n#####å¦‚ä¸å…³æ³¨ä¸‹æ–‡çš„è¯¦ç»†è¯´æ˜,å¯å‚è€ƒä½¿ç”¨ä¸‹é¢çš„é…ç½®;åˆ é™¤#æ³¨é‡Š,ä¿®æ”¹è¿æ¥å’Œlablesä¿¡æ¯\n\n#[[instances]]\n#address = \"127.0.0.1:3306\"\n#username = \"monitor\"\n#password = \"xxxxx\"\n#extra_status_metrics = true\n#extra_innodb_metrics = true\n#gather_processlist_processes_by_state = true\n#gather_processlist_processes_by_user = true\n#gather_schema_size = true\n#gather_table_size = false\n#gather_system_table_size = true\n#gather_slave_status = true\n#labels = { instance=\"yiletoo-172.20.26.88:33066\" }\n\n###å¦‚ä¸å…³æ³¨ä¸‹æ–‡çš„è¯¦ç»†è¯´æ˜,å¯å‚è€ƒä½¿ç”¨ä¸Šé¢çš„é…ç½®;åˆ é™¤#æ³¨é‡Š,ä¿®æ”¹è¿æ¥å’Œlablesä¿¡æ¯\n#########################################################################\n\n####å¦‚æœä½¿ç”¨äº†ä¸Šé¢çš„ç¤ºä¾‹é…ç½®,ä»¥ä¸‹æ‰€æœ‰çš„å†…å®¹å¯ä»¥å…¨éƒ¨åˆ é™¤\n######################################################\n\n# å®šä¹‰mysqlé‡‡é›†å‘¨æœŸï¼Œå•ä½æ˜¯ç§’\ninterval = 15\n\n# å®šä¹‰å…¨å±€è¦æ‰§è¡Œçš„è‡ªå®šä¹‰SQLï¼Œæ¯ä¸ªinstanceéƒ½ä¼šæ‰§è¡Œ\n# æ³¨æ„è¿™é‡Œæ˜¯ [[queries]]ï¼Œè€Œä¸æ˜¯ [[instances.queries]]ï¼Œ[[queries]]æ˜¯å…¨å±€çš„ï¼Œ[[instances.queries]]æ˜¯é’ˆå¯¹æŸä¸ªinstanceçš„\n#[[queries]]\n#mesurement = \"lock_wait\"\n#metric_fields = [ \"total\" ]\n#timeout = \"3s\"\n#request = \'\'\'\n#SELECT count(*) as total FROM information_schema.innodb_trx WHERE trx_state=\'LOCK WAIT\'\n#\'\'\'\n\n# å®šä¹‰instanceï¼Œ ä¸€ä¸ªinstanceå¯¹åº”ä¸€ä¸ªmysqlå®ä¾‹\n# æŒ‡å®šè¿æ¥ä¿¡æ¯,ç¡®ä¿è¿æ¥è´¦æˆ·æœ‰åªè¯»ç›¸å…³æƒé™ï¼›\n#GRANT PROCESS, REPLICATION CLIENT, SELECT ON *.* TO \'monitor\'@\'127.0.0.1\';\n\n[[instances]]\n#å¦‚æœipåœ°å€é‡‡é›†ä¸é€šè¯·ä½¿ç”¨sockæ–‡ä»¶è¿æ¥\n#address = \"/webser/mysql57_33066/var/mysql57_33066.sock\"\n#address = \"127.0.0.1:3306\"\n#username = \"monitor\"\n#password = \"xxxxxxxx\"\n\n# ä¸º mysql å®ä¾‹é™„åŠ ä¸€äº›æ ‡ç­¾ï¼Œæ¯”å¦‚ä¸Šé¢çš„ä¾‹å­ä¸­æåˆ°çš„ instance æ ‡ç­¾\nlabels = { instance=\"matrix-pro:3306\" }\n\n# æ˜¯å¦ä½¿ç”¨ tls ç­‰å®šåˆ¶å‚æ•°\nparameters = \"tls=false\"\n\n# é€šè¿‡ show global status ç›‘æ§ mysqlï¼Œé»˜è®¤æŠ“å–ä¸€äº›åŸºç¡€æŒ‡æ ‡ï¼Œ\n# å¦‚æœæƒ³æŠ“å–æ›´å¤š global status çš„æŒ‡æ ‡ï¼ŒæŠŠä¸‹é¢çš„é…ç½®è®¾ç½®ä¸º true\n#extra_status_metrics = true\n\n# é€šè¿‡show global variables ç›‘æ§ mysql çš„å…¨å±€å˜é‡ï¼Œé»˜è®¤æŠ“å–ä¸€äº›å¸¸è§„çš„\n# å¸¸è§„çš„åŸºæœ¬å¤Ÿç”¨äº†ï¼Œæ‰©å±•çš„éƒ¨åˆ†å¦‚æœä¹Ÿæƒ³é‡‡é›†ï¼Œä¸‹é¢çš„é…ç½®è®¾ç½®ä¸º true\nextra_innodb_metrics = true\n\n# ç›‘æ§ processlistï¼Œå…³æ³¨è¾ƒå°‘ï¼Œé»˜è®¤ä¸é‡‡é›†\n#gather_processlist_processes_by_state = false\n#gather_processlist_processes_by_user = false\n\n# ç›‘æ§å„ä¸ªæ•°æ®åº“çš„ç£ç›˜å ç”¨å¤§å°ï¼Œå¦‚æœä½ çš„ DB å¾ˆå¤§ï¼Œå¯èƒ½ä¼šå¾ˆè€—æ—¶ï¼Œä¸å»ºè®®é‡‡é›†ï¼Œç”¨å¤„ä¸å¤§\n#gather_schema_size = false\n\n# ç›‘æ§æ‰€æœ‰çš„ table çš„ç£ç›˜å ç”¨å¤§å°ï¼Œå¦‚æœä½ çš„ DB å¾ˆå¤§ï¼Œå¯èƒ½ä¼šå¾ˆè€—æ—¶ï¼Œä¸å»ºè®®é‡‡é›†ï¼Œç”¨å¤„ä¸å¤§\n#gather_table_size = false\n\n# æ˜¯å¦é‡‡é›†ç³»ç»Ÿè¡¨çš„å¤§å°ï¼Œé€šå¸¸ä¸ç”¨ï¼Œæ‰€ä»¥é»˜è®¤è®¾ç½®ä¸ºfalse\n#gather_system_table_size = false\n\n# é€šè¿‡ show slave status ç›‘æ§ slave çš„æƒ…å†µï¼Œæ¯”è¾ƒå…³é”®ï¼Œæ‰€ä»¥é»˜è®¤é‡‡é›†\n#gather_slave_status = true\n\n# è¶…æ—¶æ—¶é—´\n#timeout_seconds = 3\n\n# é‡‡é›†å‘¨æœŸçš„å€æ•°ï¼Œæ¯”å¦‚è®¾ç½®ä¸º2ï¼Œé‚£ä¹ˆé‡‡é›†å‘¨æœŸå°±æ˜¯ interval * 2\n#interval_times = 1\n\n# tls ç›¸å…³é…ç½®ï¼Œå¯é€‰é…ç½®\n#use_tls = false\n#tls_min_version = \"1.2\"\n#tls_ca = \"/etc/categraf/ca.pem\"\n#tls_cert = \"/etc/categraf/cert.pem\"\n#tls_key = \"/etc/categraf/key.pem\"\n# Use TLS but skip chain & host verification\n#insecure_skip_verify = true\n\n# å®šä¹‰åªé’ˆå¯¹å½“å‰ mysql å®ä¾‹çš„è‡ªå®šä¹‰ sql\n#[[instances.queries]]\n#mesurement = \"lock_wait\"\n#metric_fields = [ \"total\" ]\n#timeout = \"3s\"\n#request = \'\'\'\n#SELECT count(*) as total FROM information_schema.innodb_trx WHERE trx_state=\'LOCK WAIT\'\n#\'\'\'', '# mysql\n\nmysql ç›‘æ§é‡‡é›†æ’ä»¶ï¼Œæ ¸å¿ƒåŸç†å°±æ˜¯è¿åˆ° mysql å®ä¾‹ï¼Œæ‰§è¡Œä¸€äº› sqlï¼Œè§£æè¾“å‡ºå†…å®¹ï¼Œæ•´ç†ä¸ºç›‘æ§æ•°æ®ä¸ŠæŠ¥ã€‚\n\n## Configuration\n\ncategraf çš„ `conf/input.mysql/mysql.toml`\n\n```toml\n[[instances]]\n# è¦ç›‘æ§ MySQLï¼Œé¦–å…ˆè¦ç»™å‡ºè¦ç›‘æ§çš„MySQLçš„è¿æ¥åœ°å€ã€ç”¨æˆ·åã€å¯†ç \naddress = \"127.0.0.1:3306\"\nusername = \"root\"\npassword = \"1234\"\n\n# # set tls=custom to enable tls\n# parameters = \"tls=false\"\n\n# é€šè¿‡ show global statusç›‘æ§mysqlï¼Œé»˜è®¤æŠ“å–ä¸€äº›åŸºç¡€æŒ‡æ ‡ï¼Œ\n# å¦‚æœæƒ³æŠ“å–æ›´å¤šglobal statusçš„æŒ‡æ ‡ï¼ŒæŠŠä¸‹é¢çš„é…ç½®è®¾ç½®ä¸ºtrue\nextra_status_metrics = true\n\n# é€šè¿‡show global variablesç›‘æ§mysqlçš„å…¨å±€å˜é‡ï¼Œé»˜è®¤æŠ“å–ä¸€äº›å¸¸è§„çš„\n# å¸¸è§„çš„åŸºæœ¬å¤Ÿç”¨äº†ï¼Œæ‰©å±•çš„éƒ¨åˆ†ï¼Œé»˜è®¤ä¸é‡‡é›†ï¼Œä¸‹é¢çš„é…ç½®è®¾ç½®ä¸ºfalse\nextra_innodb_metrics = false\n\n# ç›‘æ§processlistï¼Œå…³æ³¨è¾ƒå°‘ï¼Œé»˜è®¤ä¸é‡‡é›†\ngather_processlist_processes_by_state = false\ngather_processlist_processes_by_user = false\n\n# ç›‘æ§å„ä¸ªæ•°æ®åº“çš„ç£ç›˜å ç”¨å¤§å°\ngather_schema_size = false\n\n# ç›‘æ§æ‰€æœ‰çš„tableçš„ç£ç›˜å ç”¨å¤§å°\ngather_table_size = false\n\n# æ˜¯å¦é‡‡é›†ç³»ç»Ÿè¡¨çš„å¤§å°ï¼Œé€šè¿‡ä¸ç”¨ï¼Œæ‰€ä»¥é»˜è®¤è®¾ç½®ä¸ºfalse\ngather_system_table_size = false\n\n# é€šè¿‡ show slave statusç›‘æ§slaveçš„æƒ…å†µï¼Œæ¯”è¾ƒå…³é”®ï¼Œæ‰€ä»¥é»˜è®¤é‡‡é›†\ngather_slave_status = true\n\n# # timeout\n# timeout_seconds = 3\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# ä¸ºmysqlå®ä¾‹é™„ä¸€ä¸ªinstanceçš„æ ‡ç­¾ï¼Œå› ä¸ºé€šè¿‡address=127.0.0.1:3306ä¸å¥½åŒºåˆ†\n# important! use global unique string to specify instance\n# labels = { instance=\"n9e-10.2.3.4:3306\" }\n\n## Optional TLS Config\n# use_tls = false\n# tls_min_version = \"1.2\"\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = true\n\n# è‡ªå®šä¹‰SQLï¼ŒæŒ‡å®šSQLã€è¿”å›çš„å„ä¸ªåˆ—é‚£äº›æ˜¯ä½œä¸ºmetricï¼Œå“ªäº›æ˜¯ä½œä¸ºlabel\n# [[instances.queries]]\n# mesurement = \"users\"\n# metric_fields = [ \"total\" ]\n# label_fields = [ \"service\" ]\n# # field_to_append = \"\"\n# timeout = \"3s\"\n# request = \'\'\'\n# select \'n9e\' as service, count(*) as total from n9e_v5.users\n# \'\'\'\n```\n\n## ç›‘æ§å¤šä¸ªå®ä¾‹\n\nå¤§å®¶æœ€å¸¸é—®çš„é—®é¢˜æ˜¯å¦‚ä½•ç›‘æ§å¤šä¸ªmysqlå®ä¾‹ï¼Œå®é™…å¤§å®¶å¯¹tomlé…ç½®å­¦ä¹ ä¸€ä¸‹å°±äº†è§£äº†ï¼Œ`[[instances]]` éƒ¨åˆ†è¡¨ç¤ºæ•°ç»„ï¼Œæ˜¯å¯ä»¥å‡ºç°å¤šä¸ªçš„ï¼Œä¸¾ä¾‹ï¼š\n\n```toml\n[[instances]]\naddress = \"10.2.3.6:3306\"\nusername = \"root\"\npassword = \"1234\"\nlabels = { instance=\"n9e-10.2.3.6:3306\" }\n\n[[instances]]\naddress = \"10.2.6.9:3306\"\nusername = \"root\"\npassword = \"1234\"\nlabels = { instance=\"zbx-10.2.6.9:3306\" }\n\n[[instances]]\naddress = \"/tmp/mysql.sock\"\nusername = \"root\"\npassword = \"1234\"\nlabels = { instance=\"zbx-localhost:3306\" }\n```\n\n## ç›‘æ§å¤§ç›˜\n\nå¤œèºå†…ç½®äº† mysql ç›¸å…³çš„ç›‘æ§å¤§ç›˜ï¼Œå†…ç½®äº†è‡³å°‘ 4 ä¸ªä»ªè¡¨ç›˜ï¼š\n\n### mysql_by_categraf_instance\n\nè¿™ä¸ªå¤§ç›˜æ˜¯ä½¿ç”¨ categraf ä½œä¸ºé‡‡é›†å™¨ï¼Œä½¿ç”¨ instance ä½œä¸ºå¤§ç›˜å˜é‡ï¼Œæ‰€ä»¥ä¸Šä¾‹é‡‡é›†é…ç½®ä¸­éƒ½æœ‰ä¸€ä¸ª instance çš„æ ‡ç­¾ï¼Œå°±æ˜¯å’Œè¿™ä¸ªå¤§ç›˜é…åˆä½¿ç”¨çš„ã€‚\n\n### mysql_by_categraf_ident\n\nè¿™ä¸ªå¤§ç›˜æ˜¯ä½¿ç”¨ categraf ä½œä¸ºé‡‡é›†å™¨ï¼Œä½¿ç”¨ ident ä½œä¸ºå¤§ç›˜å˜é‡ï¼Œå³åœ¨æŸ¥çœ‹ mysql ç›‘æ§æŒ‡æ ‡çš„æ—¶å€™ï¼Œå…ˆé€šè¿‡å¤§ç›˜é€‰ä¸­å®¿ä¸»æœºå™¨ï¼Œå†é€šè¿‡æœºå™¨æ‰¾åˆ° mysql å®ä¾‹ã€‚\n\n### dashboard-by-aws-rds\n\nè¿™æ˜¯ç½‘å‹è´¡çŒ®çš„å¤§ç›˜ï¼Œé‡‡é›†çš„ aws çš„ rds ç›¸å…³çš„æ•°æ®åˆ¶ä½œçš„å¤§ç›˜ã€‚æ¬¢è¿å„ä½ç½‘å‹è´¡çŒ®å¤§ç›˜ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å…±å»ºç¤¾åŒºçš„æ–¹å¼ã€‚æŠŠæ‚¨åšå¥½çš„å¤§ç›˜å¯¼å‡ºä¸º JSONï¼Œæ PR åˆ° [è¿™ä¸ªç›®å½•](https://github.com/ccfos/nightingale/tree/main/integrations/MySQL/dashboards) ä¸‹å³å¯ã€‚\n\n### mysql_by_exporter\n\nè¿™æ˜¯ä½¿ç”¨ mysqld_exporter ä½œä¸ºé‡‡é›†å™¨åˆ¶ä½œçš„å¤§ç›˜ã€‚\n\n## å‘Šè­¦è§„åˆ™\n\nå¤œèºå†…ç½®äº† mysql ç›¸å…³çš„å‘Šè­¦è§„åˆ™ï¼Œå…‹éš†åˆ°è‡ªå·±çš„ä¸šåŠ¡ç»„å³å¯ä½¿ç”¨ã€‚ä¹Ÿæ¬¢è¿å¤§å®¶ä¸€èµ·æ¥é€šè¿‡ PR å®Œå–„ä¿®æ”¹è¿™ä¸ªå†…ç½®çš„ [å‘Šè­¦è§„åˆ™](https://github.com/ccfos/nightingale/tree/main/integrations/MySQL/alerts)ã€‚', 'mysqlç›‘æ§é‡‡é›†æ’ä»¶ï¼Œæ‰§è¡ŒSQLé‡‡é›†ç›‘æ§æ•°æ®', '1730797228', 'Ã§Å½â€¹Ã¦ÂÂ¨(822032277)', '1733909196', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('68', 'Redis', '# # collect interval\n# interval = 15\n\n[[instances]]\n# address = \"127.0.0.1:6379\"\n# username = \"\"\n# password = \"\"\n# pool_size = 2\n\n# # Optional. Specify redis commands to retrieve values\n# commands = [\n#     {command = [\"get\", \"sample-key1\"], metric = \"custom_metric_name1\"},\n#     {command = [\"get\", \"sample-key2\"], metric = \"custom_metric_name2\"}\n# ]\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# important! use global unique string to specify instance\n# labels = { instance=\"n9e-10.2.3.4:6379\" }\n\n## Optional TLS Config\n# use_tls = false\n# tls_min_version = \"1.2\"\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = true', '# redis\n\nredis çš„ç›‘æ§åŸç†ï¼Œå°±æ˜¯è¿ä¸Š redisï¼Œæ‰§è¡Œ info å‘½ä»¤ï¼Œè§£æç»“æœï¼Œæ•´ç†æˆç›‘æ§æ•°æ®ä¸ŠæŠ¥ã€‚\n\n## Configuration\n\nredis æ’ä»¶çš„é…ç½®åœ¨ `conf/input.redis/redis.toml` æœ€ç®€å•çš„é…ç½®å¦‚ä¸‹ï¼š\n\n```toml\n[[instances]]\naddress = \"127.0.0.1:6379\"\nusername = \"\"\npassword = \"\"\nlabels = { instance=\"n9e-10.23.25.2:6379\" }\n```\n\nå¦‚æœè¦ç›‘æ§å¤šä¸ª redis å®ä¾‹ï¼Œå°±å¢åŠ  instances å³å¯ï¼š\n\n```toml\n[[instances]]\naddress = \"10.23.25.2:6379\"\nusername = \"\"\npassword = \"\"\nlabels = { instance=\"n9e-10.23.25.2:6379\" }\n\n[[instances]]\naddress = \"10.23.25.3:6379\"\nusername = \"\"\npassword = \"\"\nlabels = { instance=\"n9e-10.23.25.3:6379\" }\n```\n\nå»ºè®®é€šè¿‡ labels é…ç½®é™„åŠ ä¸€ä¸ª instance æ ‡ç­¾ï¼Œä¾¿äºåé¢å¤ç”¨ç›‘æ§å¤§ç›˜ã€‚\n\n## ç›‘æ§å¤§ç›˜å’Œå‘Šè­¦è§„åˆ™\n\nå¤œèºå†…ç½®äº† redis çš„å‘Šè­¦è§„åˆ™å’Œç›‘æ§å¤§ç›˜ï¼Œå…‹éš†åˆ°è‡ªå·±çš„ä¸šåŠ¡ç»„ä¸‹å³å¯ä½¿ç”¨ã€‚\n\n## redis é›†ç¾¤å¦‚ä½•ç›‘æ§\n\nå…¶å®ï¼Œredis é›†ç¾¤çš„ç›‘æ§ï¼Œè¿˜æ˜¯å»ç›‘æ§æ¯ä¸ª redis å®ä¾‹ã€‚\n\nå¦‚æœä¸€ä¸ª redis é›†ç¾¤æœ‰ 3 ä¸ªå®ä¾‹ï¼Œå¯¹äºä¸šåŠ¡åº”ç”¨æ¥è®²ï¼Œå‘èµ·ä¸€ä¸ªè¯·æ±‚ï¼Œå¯èƒ½éšæœºè¯·æ±‚åˆ°æŸä¸€ä¸ªå®ä¾‹ä¸Šå»äº†ï¼Œè¿™ä¸ªæ˜¯æ²¡é—®é¢˜çš„ï¼Œä½†æ˜¯å¯¹äºç›‘æ§ client è€Œè¨€ï¼Œæ˜¾ç„¶æ˜¯å¸Œæœ›åˆ°æ‰€æœ‰å®ä¾‹ä¸Šè·å–æ•°æ®çš„ã€‚\n\nå½“ç„¶ï¼Œå¦‚æœå¤šä¸ª redis å®ä¾‹ç»„æˆäº†é›†ç¾¤ï¼Œæˆ‘ä»¬å¸Œæœ›æœ‰ä¸ªæ ‡è¯†æ¥æ ‡è¯†è¿™ä¸ªé›†ç¾¤ï¼Œè¿™ä¸ªæ—¶å€™ï¼Œå¯ä»¥é€šè¿‡ labels æ¥å®ç°ï¼Œæ¯”å¦‚ç»™æ¯ä¸ªå®ä¾‹å¢åŠ ä¸€ä¸ª redis_clus çš„æ ‡ç­¾ï¼Œå€¼ä¸ºé›†ç¾¤åå­—å³å¯ã€‚', 'redisç›‘æ§æŒ‡æ ‡é‡‡é›†', '1730799961', 'Ã§Å½â€¹Ã¦ÂÂ¨(822032277)', '1732762195', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('69', 'Ping', '# Pingç›‘æ§\n# # collect interval\n# interval = 15\n\n#####å¦‚ä¸å…³æ³¨ä¸‹æ–‡çš„è¯¦ç»†è¯´æ˜,å¯å‚è€ƒä½¿ç”¨ä¸‹é¢çš„é…ç½®;åˆ é™¤#æ³¨é‡Š,ä¿®æ”¹targets\n#æ³¨æ„åˆ—è¡¨æœ€åä¸€ä¸ªå…ƒç´ ç»“å°¾æ²¡æœ‰é€—å·\n[[instances]]\ntargets = [\n#     \"www.baidu.com\",\n#     \"127.0.0.1\",\n#     \"10.4.5.7\"\n]\n#é»˜è®¤ä½¿ç”¨åŸç”Ÿæ–¹å¼ï¼Œå¦‚éœ€ä½¿ç”¨ç³»ç»Ÿpingå‘½ä»¤è¯·è§£å¼€ä¸‹é¢çš„æ³¨é‡Š\n# method = \"exec\"\n# binary = \"ping\"\n# binary = \"ping6\"\n\n###å¦‚ä¸å…³æ³¨ä¸‹æ–‡çš„è¯¦ç»†è¯´æ˜,å¯å‚è€ƒä½¿ç”¨ä¸Šé¢çš„é…ç½®;åˆ é™¤#æ³¨é‡Š,ä¿®æ”¹targets\n###############################################################\n\n####å¦‚æœä½¿ç”¨äº†ä¸Šé¢çš„ç¤ºä¾‹é…ç½®,ä»¥ä¸‹æ‰€æœ‰å†…å®¹å¯ä»¥å…¨éƒ¨åˆ é™¤###\n######################################################\n\n## Method used for sending pings, can be either \"exec\" or \"native\".  When set\n## to \"exec\" the systems ping command will be executed.  When set to \"native\"\n## the plugin will send pings directly.\n##\n## While the default is \"native\" for backwards compatibility, new deployments\n## are encouraged to use the \"native\" method for improved compatibility and\n## performance.\n# method = \"exec\"\n\n## Specify the ping executable binary.\n# binary = \"ping\"\n\n# å¦‚éœ€æ·»åŠ æ ‡ç­¾,è¯·ä¿®æ”¹labels\n# labels = { region=\"cloud\", product=\"n9e\" }\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n## Number of ping packets to send per interval.  Corresponds to the \"-c\"\n## option of the ping command.\n# count = 1\n\n## Time to wait between sending ping packets in seconds.  Operates like the\n## \"-i\" option of the ping command.\n# ping_interval = 1.0\n\n## If set, the time to wait for a ping response in seconds.  Operates like\n## the \"-W\" option of the ping command.\n# timeout = 3.0\n\n## Interface or source address to send ping from.  Operates like the -I or -S\n## option of the ping command.\n# interface = \"\"\n\n## Use only IPv6 addresses when resolving a hostname.\n# ipv6 = false\n\n## Number of data bytes to be sent. Corresponds to the \"-s\"\n## option of the ping command.\n# size = 56\n\n# max concurrency coroutine\n# concurrency = 50', '# ping\n\nping ç›‘æ§æ’ä»¶ï¼Œæ¢æµ‹è¿œç«¯ç›®æ ‡åœ°å€èƒ½å¦ ping é€šï¼Œå¦‚æœæœºå™¨æ²¡æœ‰ç¦ pingï¼Œè¿™å°±æ˜¯ä¸€ä¸ªå¾ˆå¥½ç”¨çš„æ¢æµ‹æœºå™¨å­˜æ´»çš„æ‰‹æ®µ\n\n## Configuration\nè¿™ä¸ªæ’ä»¶æœ‰ä¸¤ç§ä¸»è¦çš„æ“ä½œæ–¹æ³•ï¼š`exec` å’Œ `native`.æ¨èä½¿ç”¨ `native` æ–¹æ³•ï¼Œå› ä¸ºå®ƒå…·æœ‰æ›´å¥½çš„ç³»ç»Ÿå…¼å®¹æ€§å’Œæ€§èƒ½.\nä¸ºäº†å‘åå…¼å®¹å’Œæ›´ç²¾å‡†çš„response_ms,`native` æ–¹æ³•æ˜¯é»˜è®¤çš„.\nä½¿ç”¨ `method = \"exec\"`,å°†ä¼šè°ƒç”¨ç³»ç»Ÿpingç¨‹åºæ¥å‘é€ping packets.\n\nè¦æ¢æµ‹çš„æœºå™¨é…ç½®åˆ° targets ä¸­ï¼Œtargets æ˜¯ä¸ªæ•°ç»„ï¼Œå¯ä»¥é…ç½®å¤šä¸ªï¼Œå½“ç„¶ä¹Ÿå¯ä»¥æ‹†æˆå¤šä¸ª `[[instances]]` é…ç½®æ®µï¼Œæ¯”å¦‚ï¼š\n\n```\n[[instances]]\ntargets = [ \"10.4.5.6\" ]\nlabels = { region=\"cloud\", product=\"n9e\" }\n\n[[instances]]\ntargets = [ \"10.4.5.7\" ]\nlabels = { region=\"cloud\", product=\"zbx\" }\n```\n\nä¸Šä¾‹ä¸­æ˜¯ ping ä¸¤ä¸ªåœ°å€ï¼Œä¸ºäº†ä¿¡æ¯æ›´ä¸°å¯Œï¼Œé™„åŠ äº† region å’Œ product æ ‡ç­¾\n\n## File Limit\n\n```sh\nsystemctl edit categraf\n```\n\nIncrease the number of open files:\n\n```ini\n[Service]\nLimitNOFILE=8192\n```\n\nRestart Categraf:\n\n```sh\nsystemctl restart categraf\n```\n\n### Linux Permissions\n\nOn most systems, ping requires `CAP_NET_RAW` capabilities or for Categraf to be run as root.\n\nWith systemd:\n\n```sh\nsystemctl edit categraf\n```\n\n```ini\n[Service]\nCapabilityBoundingSet=CAP_NET_RAW\nAmbientCapabilities=CAP_NET_RAW\n```\n\n```sh\nsystemctl restart categraf\n```\n\nWithout systemd:\n\n```sh\nsetcap cap_net_raw=eip /usr/bin/categraf\n```\n\nReference [`man 7 capabilities`][man 7 capabilities] for more information about\nsetting capabilities.\n\n[man 7 capabilities]: http://man7.org/linux/man-pages/man7/capabilities.7.html\n\n### Other OS Permissions\n\nWhen using `method = \"native\"`, you will need permissions similar to the executable ping program for your OS.\n\n## ç›‘æ§å¤§ç›˜å’Œå‘Šè­¦è§„åˆ™\n\nå¤œèºå†…ç½®äº†å‘Šè­¦è§„åˆ™å’Œç›‘æ§å¤§ç›˜ï¼Œå…‹éš†åˆ°è‡ªå·±çš„ä¸šåŠ¡ç»„ä¸‹å³å¯ä½¿ç”¨ã€‚\n\n## Example Output\n\n```text\nping_maximum_response_ms agent_hostname=zy-fat product=n9e region=cloud target=10.0.24.136 0.036\nping_packets_transmitted agent_hostname=zy-fat product=n9e region=cloud target=10.0.24.136 1\nping_packets_received agent_hostname=zy-fat product=n9e region=cloud target=10.0.24.136 1\nping_average_response_ms agent_hostname=zy-fat product=n9e region=cloud target=10.0.24.136 0.036\nping_minimum_response_ms agent_hostname=zy-fat product=n9e region=cloud target=10.0.24.136 0.036\nping_standard_deviation_ms agent_hostname=zy-fat product=n9e region=cloud target=10.0.24.136 0\nping_result_code agent_hostname=zy-fat product=n9e region=cloud target=10.0.24.136 0\nping_percent_packet_loss agent_hostname=zy-fat product=n9e region=cloud target=10.0.24.136 0\nping_ttl agent_hostname=zy-fat product=n9e region=cloud target=10.0.24.136 64\nping_minimum_response_ms agent_hostname=zy-fat product=n9e region=cloud target=www.baidu.com 20.935\nping_average_response_ms agent_hostname=zy-fat product=n9e region=cloud target=www.baidu.com 20.935\nping_standard_deviation_ms agent_hostname=zy-fat product=n9e region=cloud target=www.baidu.com 0\nping_result_code agent_hostname=zy-fat product=n9e region=cloud target=www.baidu.com 0\nping_packets_transmitted agent_hostname=zy-fat product=n9e region=cloud target=www.baidu.com 1\nping_packets_received agent_hostname=zy-fat product=n9e region=cloud target=www.baidu.com 1\nping_ttl agent_hostname=zy-fat product=n9e region=cloud target=www.baidu.com 50\nping_percent_packet_loss agent_hostname=zy-fat product=n9e region=cloud target=www.baidu.com 0\nping_maximum_response_ms agent_hostname=zy-fat product=n9e region=cloud target=www.baidu.com 20.935\n```', 'ping ç›‘æ§æ’ä»¶ï¼Œæ¢æµ‹è¿œç«¯ç›®æ ‡åœ°å€èƒ½å¦ ping é€š', '1730800231', 'Ã§Å½â€¹Ã¦ÂÂ¨(822032277)', '1733972372', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('70', 'Exec', '# # collect interval\n# interval = 15\n\n[[instances]]\n# # commands, support glob\ncommands = [\n#     \"/webser/app/categraf/scripts/*.sh\"\n]\n\n# # timeout for each command to complete\n# timeout = 5\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# # choices: influx prometheus falcon\n# # influx stdout example: mesurement,labelkey1=labelval1,labelkey2=labelval2 field1=1.2,field2=2.3\n# data_format = \"influx\"', '# åº”ç”¨åœºæ™¯\n```\nåº”ç”¨äºinputæ’ä»¶åº“execç›®å½•ä¹‹å¤–çš„ç‰¹æ®Šæˆ–è‡ªå®šä¹‰å®ç°æŒ‡å®šä¸šåŠ¡çš„ç›‘æ§ã€‚\nç›‘æ§è„šæœ¬é‡‡é›†åˆ°ç›‘æ§æ•°æ®ä¹‹åé€šè¿‡ç›¸åº”çš„æ ¼å¼è¾“å‡ºåˆ°stdoutï¼Œcategrafæˆªè·stdoutå†…å®¹ï¼Œè§£æä¹‹åä¼ ç»™æœåŠ¡ç«¯ï¼Œ\nè„šæœ¬çš„è¾“å‡ºæ ¼å¼æ”¯æŒ3ç§ï¼šinfluxã€falconã€prometheusï¼Œé€šè¿‡ exec.toml çš„ `data_format` é…ç½®å‘Šè¯‰ Categrafã€‚\ndata_formatæœ‰3ä¸ªå€¼ï¼Œå…¶ç”¨æ³•ä¸ºï¼š\n```\n\n## influx\n\ninflux æ ¼å¼çš„å†…å®¹è§„èŒƒï¼š\n```\nmesurement,labelkey1=labelval1,labelkey2=labelval2 field1=1.2,field2=2.3\n```\n- é¦–å…ˆmesurementï¼Œè¡¨ç¤ºä¸€ä¸ªç±»åˆ«çš„ç›‘æ§æŒ‡æ ‡ï¼Œæ¯”å¦‚ connectionsï¼›\n- mesurementåé¢æ˜¯é€—å·ï¼Œé€—å·åé¢æ˜¯æ ‡ç­¾ï¼Œå¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼Œåˆ™mesurementåé¢ä¸éœ€è¦é€—å·\n- æ ‡ç­¾æ˜¯k=vçš„æ ¼å¼ï¼Œå¤šä¸ªæ ‡ç­¾ç”¨é€—å·åˆ†éš”ï¼Œæ¯”å¦‚region=beijing,env=test\n- æ ‡ç­¾åé¢æ˜¯ç©ºæ ¼\n- ç©ºæ ¼åé¢æ˜¯å±æ€§å­—æ®µï¼Œå¤šä¸ªå±æ€§å­—æ®µç”¨é€—å·åˆ†éš”\n- å±æ€§å­—æ®µæ˜¯å­—æ®µå=å€¼çš„æ ¼å¼ï¼Œåœ¨categrafé‡Œå€¼åªèƒ½æ˜¯æ•°å­—\n  æœ€ç»ˆï¼Œmesurementå’Œå„ä¸ªå±æ€§å­—æ®µåç§°æ‹¼æ¥æˆmetricåå­—\n\n## falcon\nOpen-Falconçš„æ ¼å¼å¦‚ä¸‹ï¼Œä¸¾ä¾‹ï¼š\n\n```json\n[\n    {\n        \"endpoint\": \"test-endpoint\",\n        \"metric\": \"test-metric\",\n        \"timestamp\": 1658490609,\n        \"step\": 60,\n        \"value\": 1,\n        \"counterType\": \"GAUGE\",\n        \"tags\": \"idc=lg,loc=beijing\",\n    },\n    {\n        \"endpoint\": \"test-endpoint\",\n        \"metric\": \"test-metric2\",\n        \"timestamp\": 1658490609,\n        \"step\": 60,\n        \"value\": 2,\n        \"counterType\": \"GAUGE\",\n        \"tags\": \"idc=lg,loc=beijing\",\n    }\n]\n```\ntimestampã€stepã€counterTypeï¼Œè¿™ä¸‰ä¸ªå­—æ®µåœ¨categrafå¤„ç†çš„æ—¶å€™ä¼šç›´æ¥å¿½ç•¥æ‰ï¼Œendpointä¼šæ”¾åˆ°labelsé‡Œä¸ŠæŠ¥ã€‚\n\n## prometheus\nprometheus æ ¼å¼å¤§å®¶ä¸é™Œç”Ÿäº†ï¼Œæ¯”å¦‚æˆ‘è¿™é‡Œå‡†å¤‡ä¸€ä¸ªç›‘æ§è„šæœ¬ï¼Œè¾“å‡º prometheus çš„æ ¼å¼æ•°æ®ï¼š\n```shell\n#!/bin/sh\n\necho \'# HELP demo_http_requests_total Total number of http api requests\'\necho \'# TYPE demo_http_requests_total counter\'\necho \'demo_http_requests_total{api=\"add_product\"} 4633433\'\n```\nå…¶ä¸­ `#` æ³¨é‡Šçš„éƒ¨åˆ†ï¼Œå…¶å®ä¼šè¢« categraf å¿½ç•¥ï¼Œä¸è¦ä¹Ÿç½¢ï¼Œprometheus åè®®çš„æ•°æ®å…·ä½“çš„æ ¼å¼ï¼Œè¯·å¤§å®¶å‚è€ƒ prometheus å®˜æ–¹æ–‡æ¡£\n\n\n# éƒ¨ç½²åœºæ™¯\nä¸€èˆ¬åœ¨å¤åˆå‹ç”¨é€”æˆ–ç‹¬ç«‹çš„è™šæ‹Ÿæœºå¯ç”¨æ­¤æ’ä»¶ã€‚\n\n# å‰ç½®æ¡ä»¶\n```\n1.éœ€ä½¿ç”¨äººè§£è¯»æ¯ä¸ªè„šæœ¬æˆ–ç¨‹åºçš„é€»è¾‘ï¼Œå…¶è„šæœ¬æˆ–ç¨‹åºé¡¶éƒ¨æœ‰å¤§æ¦‚ä½œç”¨çš„æè¿°ã€‚\n```\n\n# é…ç½®åœºæ™¯\næœ¬é…ç½®å¯ç”¨æˆ–æ•°æ®å®šä¹‰å¦‚ä¸‹åŠŸèƒ½ï¼š\nå¢åŠ è‡ªå®šä¹‰æ ‡ç­¾ï¼Œå¯é€šè¿‡è‡ªå®šä¹‰æ ‡ç­¾ç­›é€‰æ•°æ®åŠæ›´åŠ ç²¾ç¡®çš„å‘Šè­¦æ¨é€ã€‚\nå“åº”è¶…æ—¶æ—¶é—´ä¸º5ç§’ã€‚\ncommandså­—æ®µæ­£ç¡®åº”ç”¨è„šæœ¬æ‰€åœ¨ä½ç½®ã€‚\n\n# ä¿®æ”¹exec.tomlæ–‡ä»¶é…ç½®\n```\n[root@aliyun input.exec]# vi exec.toml\n\n# # collect interval\n# interval = 15\n\n[[instances]]\n# # commands, support glob\ncommands = [\n     \"/opt/categraf/scripts/*/collect_*.sh\"\n     #\"/opt/categraf/scripts/*/collect_*.py\"\n     #\"/opt/categraf/scripts/*/collect_*.go\"\n     #\"/opt/categraf/scripts/*/collect_*.lua\"\n     #\"/opt/categraf/scripts/*/collect_*.java\"\n     #\"/opt/categraf/scripts/*/collect_*.bat\"\n     #\"/opt/categraf/scripts/*/collect_*.cmd\"\n     #\"/opt/categraf/scripts/*/collect_*.ps1\"\n]\n\n# # timeout for each command to complete\n# timeout = 5\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# # mesurement,labelkey1=labelval1,labelkey2=labelval2 field1=1.2,field2=2.3\ndata_format = \"influx\"\n```\n\n#  æµ‹è¯•é…ç½®\n```\nä»¥cert/collect_cert_expiretime.shä¸ºä¾‹ï¼š\nsh /opt/categraf/cert/collect_cert_expiretime.sh å‡ºç°ï¼š\ncert,cloud=huaweicloud,region=huabei-beijing-4,azone=az1,product=cert,domain_name=www.baidu.com expire_days=163\ncert,cloud=huaweicloud,region=huabei-beijing-4,azone=az1,product=cert,domain_name=www.weibo.com expire_days=85\ncert,cloud=huaweicloud,region=huabei-beijing-4,azone=az1,product=cert,domain_name=www.csdn.net expire_days=281\n```\n\n# é‡å¯æœåŠ¡\n```\né‡å¯categrafæœåŠ¡ç”Ÿæ•ˆ\nsystemctl daemon-reload && systemctl restart categraf && systemctl status categraf\n\næŸ¥çœ‹å¯åŠ¨æ—¥å¿—æ˜¯å¦æœ‰é”™è¯¯\njournalctl -f -n 500 -u categraf | grep \"E\\!\" | grep \"W\\!\"\n```\n\n# æ£€æŸ¥æ•°æ®å‘ˆç°\nå¦‚å›¾ï¼š\n![image](https://user-images.githubusercontent.com/12181410/220940504-04c47faa-790a-42c1-b3dd-1510ae55c217.png)\n\n# å‘Šè­¦è§„åˆ™\n```\nè„šæœ¬ä½œç”¨ä¸åŒï¼Œè§„åˆ™å°±ä¸åŒï¼Œå…ˆç•¥è¿‡ã€‚\n```\n\n# ç›‘æ§å›¾è¡¨\n```\nè„šæœ¬ä½œç”¨ä¸åŒï¼Œè§„åˆ™å°±ä¸åŒï¼Œå…ˆç•¥è¿‡ã€‚\n```\n\n# æ•…éšœè‡ªæ„ˆ\n```\nè„šæœ¬ä½œç”¨ä¸åŒï¼Œè§„åˆ™å°±ä¸åŒï¼Œå…ˆç•¥è¿‡ã€‚\n```\n', 'è¿è¡ŒæŒ‡å®šå‘½ä»¤æˆ–è„šæœ¬è‡ªå®šä¹‰å®ç°æŒ‡å®šä¸šåŠ¡çš„ç›‘æ§', '1731052453', 'Ã§Å½â€¹Ã¦ÂÂ¨(822032277)', '1732759673', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('71', 'RabbitMQ', '# As of 3.8.0, RabbitMQ ships with built-in Prometheus & Grafana support.\n# Support for Prometheus metric collector ships in the rabbitmq_prometheus plugin.\n# The plugin exposes all RabbitMQ metrics on a dedicated TCP port, in Prometheus text format.\n#\n# enable prometheus plugin:\n# `rabbitmq-plugins enable rabbitmq_prometheus`\n# `curl http://localhost:15692/metrics`\n# \n# then use categraf prometheus plugin scrape http://localhost:15692/metrics instead of this rabbitmq plugin\n\n# # collect interval\n# interval = 15\n\n[[instances]]\n# # Management Plugin url\n# url = \"http://localhost:15672\"\n# username = \"guest\"\n# password = \"guest\"\n\n## Optional TLS Config\n# use_tls = false\n# tls_min_version = \"1.2\"\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = true\n\n## Optional request timeouts\n##\n## ResponseHeaderTimeout, if non-zero, specifies the amount of time to wait\n## for a server\'s response headers after fully writing the request.\n# header_timeout = \"3s\"\n##\n## client_timeout specifies a time limit for requests made by this client.\n## Includes connection time, any redirects, and reading the response body.\n# client_timeout = \"4s\"\n\n## A list of nodes to gather as the rabbitmq_node measurement. If not\n## specified, metrics for all nodes are gathered.\n# nodes = [\"rabbit@node1\", \"rabbit@node2\"]\n\n## A list of exchanges to gather as the rabbitmq_exchange measurement. If not\n## specified, metrics for all exchanges are gathered.\n# exchanges = [\"categraf\"]\n\n## Metrics to include and exclude. Globs accepted.\n## Note that an empty array for both will include all metrics\n## Currently the following metrics are supported: \"exchange\", \"federation\", \"node\", \"overview\", \"queue\"\n# metric_include = []\n# metric_exclude = []\n\n## Queues to include and exclude. Globs accepted.\n## Note that an empty array for both will include all queues\n# queue_name_include = []\n# queue_name_exclude = []\n\n## Federation upstreams to include and exclude specified as an array of glob\n## pattern strings.  Federation links can also be limited by the queue and\n## exchange filters.\n# federation_upstream_include = []\n# federation_upstream_exclude = []\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# important! use global unique string to specify instance\n# labels = { instance=\"rabbitmq-001\" }', '# RabbitMQ\n\né«˜ç‰ˆæœ¬ï¼ˆ3.8ä»¥ä¸Šç‰ˆæœ¬ï¼‰çš„ RabbitMQï¼Œå·²ç»å†…ç½®æ”¯æŒäº†æš´éœ² Prometheus åè®®çš„ç›‘æ§æ•°æ®ã€‚æ‰€ä»¥ï¼Œç›´æ¥ä½¿ç”¨ categraf çš„ prometheus æ’ä»¶å³å¯é‡‡é›†ã€‚å¼€å¯ RabbitMQ Prometheus è®¿é—®ï¼š\n\n```bash\nrabbitmq-plugins enable rabbitmq_prometheus\n```\n\nå¯ç”¨æˆåŠŸçš„è¯ï¼Œrabbitmq é»˜è®¤ä¼šåœ¨ 15692 ç«¯å£èµ·ç›‘å¬ï¼Œè®¿é—® `http://localhost:15692/metrics` å³å¯çœ‹åˆ°ç¬¦åˆ prometheus åè®®çš„ç›‘æ§æ•°æ®ã€‚\n\nå¦‚æœä½äº 3.8 çš„ç‰ˆæœ¬ï¼Œè¿˜æ˜¯éœ€è¦ä½¿ç”¨ categraf çš„ rabbitmq æ’ä»¶æ¥é‡‡é›†ç›‘æ§æ•°æ®ã€‚\n\n## å‘Šè­¦è§„åˆ™\n\nå¤œèºå†…ç½®äº† RabbitMQ çš„å‘Šè­¦è§„åˆ™ï¼Œå…‹éš†åˆ°è‡ªå·±çš„ä¸šåŠ¡ç»„ä¸‹å³å¯ä½¿ç”¨ã€‚\n\n## ä»ªè¡¨ç›˜\n\nå¤œèºå†…ç½®äº† RabbitMQ çš„ä»ªè¡¨ç›˜ï¼Œå…‹éš†åˆ°è‡ªå·±çš„ä¸šåŠ¡ç»„ä¸‹å³å¯ä½¿ç”¨ã€‚`rabbitmq_v3.8_gt` æ˜¯å¤§äºç­‰äº 3.8 ç‰ˆæœ¬çš„ä»ªè¡¨ç›˜ï¼Œ`rabbitmq_v3.8_lt` æ˜¯å°äº 3.8 ç‰ˆæœ¬çš„ä»ªè¡¨ç›˜ã€‚\n\n![20230802082542](https://download.flashcat.cloud/ulric/20230802082542.png)', 'RabbitMQç›‘æ§æŒ‡æ ‡é‡‡é›†', '1732011995', 'admin888', '1732762205', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('72', 'AliYun', '# # collect interval\n# interval = 60\n[[instances]]\n# # endpoint region å‚è€ƒ https://help.aliyun.com/document_detail/28616.html#section-72p-xhs-6qt\n# region=\"cn-beijing\"\n# endpoint=\"metrics.cn-hangzhou.aliyuncs.com\"\n# access_key_id=\"your-access-key-id\"\n# access_key_secret=\"your-access-key-secret\"\n# interval_times=4\n# delay=\"10m\"\n# period=\"60s\"\n# # namespace å‚è€ƒ https://help.aliyun.com/document_detail/163515.htm?spm=a2c4g.11186623.0.0.44d65c58mhgNw3\n# namespaces=[\"acs_ecs_dashboard\"]\n# [[instances.metric_filters]]\n# # metric name å‚è€ƒ https://help.aliyun.com/document_detail/163515.htm?spm=a2c4g.11186623.0.0.401d15c73Z0dZh\n# # å‚è€ƒé¡µé¢ä¸­çš„Metric Id å¡«å…¥ä¸‹é¢çš„metricName ,é¡µé¢ä¸­åŒ…å«ä¸­æ–‡çš„Metric Nameå¯¹åº”æ¥å£ä¸­çš„Description\n# metric_names=[\"cpu_cores\",\"vm.TcpCount\"]\n# namespace=\"\"\n# ratelimit=25\n# catch_ttl=\"1h\"\n# timeout=\"5s\"', '# aliyun plugin\n\n## ç®€ä»‹\n\nä½¿ç”¨[categraf](https://github.com/flashcatcloud/categraf)ä¸­[aliyun](https://github.com/flashcatcloud/categraf/tree/main/inputs/aliyun)æ’ä»¶æ‹‰å–é˜¿é‡Œäº‘äº‘ç›‘æ§çš„æ•°æ®ï¼ˆé€šè¿‡ OpenAPIï¼‰ã€‚\n\n## æˆæƒ\n\nè·å–å‡­è¯ [https://usercenter.console.aliyun.com/#/manage/ak](https://usercenter.console.aliyun.com/#/manage/ak)\nRAM ç”¨æˆ·æˆæƒã€‚RAM ç”¨æˆ·è°ƒç”¨äº‘ç›‘æ§ API å‰ï¼Œéœ€è¦æ‰€å±çš„é˜¿é‡Œäº‘è´¦å·å°†æƒé™ç­–ç•¥æˆäºˆå¯¹åº”çš„ RAM ç”¨æˆ·ï¼Œå‚è§ [RAM ç”¨æˆ·æƒé™](https://help.aliyun.com/document_detail/43170.html?spm=a2c4g.11186623.0.0.30c841feqsoAAn)ã€‚\nå¯ä»¥åœ¨ [æˆæƒé¡µé¢](https://ram.console.aliyun.com/permissions) æ–°å¢æˆæƒï¼Œé€‰æ‹©å¯¹åº”çš„ç”¨æˆ·ï¼Œæˆäºˆäº‘ç›‘æ§åªè¯»æƒé™ `AliyunCloudMonitorReadOnlyAccess`, å¹¶ä¸ºæˆäºˆæƒé™çš„ç”¨æˆ·åˆ›å»ºaccessKey å³å¯ã€‚\n\n## Categrafä¸­conf/input.aliyun/cloud.tomlé…ç½®æ–‡ä»¶ï¼š\n\n```toml\n# # categrafé‡‡é›†å‘¨æœŸï¼Œé˜¿é‡Œäº‘æŒ‡æ ‡çš„ç²’åº¦ä¸€èˆ¬æ˜¯60ç§’ï¼Œå»ºè®®è®¾ç½®ä¸è¦å°‘äº60ç§’\ninterval = 120\n[[instances]]\n## é˜¿é‡Œäº‘èµ„æºæ‰€å¤„çš„region\n## endpoint region å‚è€ƒ https://help.aliyun.com/document_detail/28616.html#section-72p-xhs-6qt\nregion=\"cn-beijing\"\nendpoint=\"metrics.cn-hangzhou.aliyuncs.com\"\n## å¡«å…¥ä½ çš„access_key_id\naccess_key_id=\"\"\n## å¡«å…¥ä½ çš„access_key_secret\naccess_key_secret=\"\"\n\n## å¯èƒ½æ— æ³•è·å–å½“å‰æœ€æ–°æŒ‡æ ‡ï¼Œè¿™ä¸ªæŒ‡æ ‡æ˜¯æŒ‡ç›‘æ§æŒ‡æ ‡çš„æˆªæ­¢æ—¶é—´è·ç¦»ç°åœ¨å¤šä¹…\ndelay=\"50m\"\n## é˜¿é‡Œäº‘æŒ‡æ ‡çš„æœ€å°ç²’åº¦ï¼Œ60s æ˜¯æ¨èå€¼ï¼Œå†å°äº†éƒ¨åˆ†æŒ‡æ ‡ä¸æ”¯æŒ\nperiod=\"60s\"\n## æŒ‡æ ‡æ‰€å±çš„namespace ,ä¸ºç©ºï¼Œåˆ™è¡¨ç¤ºæ‰€æœ‰ç©ºé—´æŒ‡æ ‡éƒ½è¦é‡‡é›†\n## namespace å‚è€ƒ https://help.aliyun.com/document_detail/163515.htm?spm=a2c4g.11186623.0.0.44d65c58mhgNw3\nnamespaces=[\"acs_ecs_dashboard\"]\n## è¿‡æ»¤æŸä¸ªnamespaceä¸‹çš„ä¸€ä¸ªæˆ–å¤šä¸ªæŒ‡æ ‡\n## metric name å‚è€ƒ https://help.aliyun.com/document_detail/163515.htm?spm=a2c4g.11186623.0.0.401d15c73Z0dZh\n## å‚è€ƒé¡µé¢ä¸­çš„Metric Id å¡«å…¥ä¸‹é¢çš„metricName ,é¡µé¢ä¸­åŒ…å«ä¸­æ–‡çš„Metric Nameå¯¹åº”æ¥å£ä¸­çš„Description\n[[instances.metric_filters]]\nnamespace=\"\"\nmetric_names=[\"cpu_cores\",\"vm.TcpCount\", \"cpu_idle\"]\n\n# é˜¿é‡Œäº‘æŸ¥è¯¢æŒ‡æ ‡æ¥å£çš„QPSæ˜¯50ï¼Œ è¿™é‡Œé»˜è®¤è®¾ç½®ä¸ºä¸€åŠ\nratelimit=25\n# æŸ¥è¯¢æŒ‡å®šnamesapceæŒ‡æ ‡å, namespace/metric_nameç­‰metaä¿¡æ¯ä¼šç¼“å­˜èµ·æ¥ï¼Œcatch_ttl æ˜¯æŒ‡æ ‡çš„ç¼“å­˜æ—¶é—´\ncatch_ttl=\"1h\"\n# æ¯æ¬¡è¯·æ±‚é˜¿é‡Œäº‘endpointçš„è¶…æ—¶æ—¶é—´\ntimeout=\"5s\"\n```\n\n## æ•ˆæœå›¾\n\n### aliyun ecs\n\n![ecs](http://download.flashcat.cloud/uPic/R6LOcO.jpg)\n\n### aliyun rds\n\n![rds](http://download.flashcat.cloud/uPic/rds.png)\n\n### aliyun redis\n\n![redis](http://download.flashcat.cloud/uPic/redis.png)\n\n### aliyun slb\n\n![slb](http://download.flashcat.cloud/uPic/slb.png)\n\n### aliyun waf\n\n![waf](http://download.flashcat.cloud/uPic/waf.png)', 'æ‹‰å–é˜¿é‡Œäº‘äº‘ç›‘æ§çš„æ•°æ®ï¼ˆé€šè¿‡ OpenAPIï¼‰', '1732759885', 'ç‹æ¨(822032277)', '1732759885', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('73', 'Dns_Query', '# # collect interval\n# interval = 15\n\n[[instances]]\n# # append some labels for series\n# labels = { region=\"cloud\", product=\"n9e\" }\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# # \nauto_detect_local_dns_server  = false\n\n## servers to query\n# servers = [\"8.8.8.8\"]\nservers = []\n\n## Network is the network protocol name.\n# network = \"udp\"\n\n## Domains or subdomains to query.\n# domains = [\".\"]\n\n## Query record type.\n## Possible values: A, AAAA, CNAME, MX, NS, PTR, TXT, SOA, SPF, SRV.\n# record_type = \"A\"\n\n## Dns server port.\n# port = 53\n\n## Query timeout in seconds.\n# timeout = 2', '# åº”ç”¨åœºæ™¯\nä¸€èˆ¬ç”¨äºå¯¹DNSæœåŠ¡å™¨çš„å“åº”ç›‘æµ‹ï¼Œå¸®åŠ©è¿ç»´å¿«é€Ÿå®šä½ç½‘ç»œé—®é¢˜ã€‚\n\n# éƒ¨ç½²åœºæ™¯\nä¸éœ€è¦æ¯å°è™šæ‹Ÿæœºéƒ½å¯ç”¨æ­¤æ’ä»¶ï¼Œå»ºè®®æ˜¯ç‹¬ç«‹æˆ–å¤åˆçš„æŸä¸€å°è™šæ‹Ÿæœºå¯ç”¨æ­¤æ’ä»¶ã€‚\n\n# é…ç½®åœºæ™¯\n```\næœ¬é…ç½®å¯ç”¨æˆ–æ•°æ®å®šä¹‰å¦‚ä¸‹åŠŸèƒ½ï¼š\nä½¿ç”¨æœ¬æœºDNSæŸ¥è¯¢åŸŸåè§£æè´¨é‡ã€‚\nä½¿ç”¨å¤–éƒ¨DNSæŸ¥è¯¢åŸŸåè§£æè´¨é‡ã€‚\nä½¿ç”¨ä¸åŒè®°å½•ç±»å‹è¿›è¡ŒDNSæŸ¥è¯¢ã€‚\næ¯ç§æŸ¥è¯¢éƒ½è®¾ç½®è¶…æ—¶æ—¶é—´5ç§’ã€‚\nå¢åŠ è‡ªå®šä¹‰æ ‡ç­¾ï¼Œå¯é€šè¿‡è‡ªå®šä¹‰æ ‡ç­¾ç­›é€‰æ•°æ®åŠæ›´åŠ ç²¾ç¡®çš„å‘Šè­¦æ¨é€ã€‚\nåœ¨domainså­—æ®µå¤„å¢åŠ è‡ªå·±æƒ³è¦è¢«DNSæŸ¥è¯¢çš„åŸŸåï¼Œä¸€èˆ¬å¡«å†™å…¬å¸ä¸šåŠ¡ç³»ç»Ÿçš„åŸŸåæˆ–ç¬¬ä¸‰æ–¹ä¾èµ–çš„ä¸šåŠ¡ç³»ç»Ÿã€‚\n```\n\n# ä¿®æ”¹dns_query.tomlæ–‡ä»¶é…ç½®\n\n``` ä»¥ä¸‹æ–‡ä»¶å†…å®¹é…ç½®ä½œä¸ºå‚è€ƒ\n[root@aliyun input.dns_query]# cat dns_query.toml\n# # collect interval\n# interval = 15\n\n[[instances]]\n# # append some labels for series\nlabels = { cloud=\"huaweicloud\", region=\"huabei-beijing-4\",azone=\"az1\", product=\"n9e\" }\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# #\nauto_detect_local_dns_server  = true\n\n### A record\n\n## servers to query\nservers = [\"223.5.5.5\",\"114.114.114.114\",\"119.29.29.29\"]\n\n## Network is the network protocol name.\n# network = \"udp\"\n\n## Domains or subdomains to query.\ndomains = [\"www.huaweicloud.com\", \"www.baidu.com\", \"www.tapd.cn\"]\n\n## Query record type.\n## Possible values: A, AAAA, CNAME, MX, NS, PTR, TXT, SOA, SPF, SRV.\nrecord_type = \"A\"\n\n## Dns server port.\n# port = 53\n\n## Query timeout in seconds.\ntimeout = 5\n\n\n### CNAME record\n\n[[instances]]\n# # append some labels for series\nlabels = { cloud=\"huaweicloud\", region=\"huabei-beijing-4\",azone=\"az1\", product=\"n9e\" }\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# #\nauto_detect_local_dns_server  = false\n\n## servers to query\nservers = [\"223.5.5.5\",\"114.114.114.114\",\"119.29.29.29\"]\n\n## Network is the network protocol name.\n# network = \"udp\"\n\n## Domains or subdomains to query.\ndomains = [\"www.huaweicloud.com\", \"www.baidu.com\", \"www.tapd.cn\"]\n\n## Query record type.\n## Possible values: A, AAAA, CNAME, MX, NS, PTR, TXT, SOA, SPF, SRV.\nrecord_type = \"CNAME\"\n\n## Dns server port.\n# port = 53\n\n## Query timeout in seconds.\ntimeout = 5\n\n\n### NS record\n\n[[instances]]\n# # append some labels for series\nlabels = { cloud=\"huaweicloud\", region=\"huabei-beijing-4\",azone=\"az1\", product=\"n9e\" }\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# #\nauto_detect_local_dns_server  = false\n\n## servers to query\nservers = [\"223.5.5.5\",\"114.114.114.114\",\"119.29.29.29\"]\n\n## Network is the network protocol name.\n# network = \"udp\"\n\n## Domains or subdomains to query.\ndomains = [\"www.huaweicloud.com\", \"www.baidu.com\", \"www.tapd.cn\"]\n\n## Query record type.\n## Possible values: A, AAAA, CNAME, MX, NS, PTR, TXT, SOA, SPF, SRV.\nrecord_type = \"NS\"\n\n## Dns server port.\n# port = 53\n\n## Query timeout in seconds.\ntimeout = 5\n```\n\n# æµ‹è¯•é…ç½®\n```\n./categraf --test --inputs dns_query\n....... Aè®°å½•åŒç†å°±çœç•¥\n20:51:34 dns_query_rcode_value agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud domain=www.tapd.cn product=n9e record_type=CNAME region=huabei-beijing-4 server=119.29.29.29 0\n20:51:34 dns_query_result_code agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud domain=www.tapd.cn product=n9e record_type=CNAME region=huabei-beijing-4 server=119.29.29.29 0\n20:51:34 dns_query_query_time_ms agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud domain=www.tapd.cn product=n9e record_type=CNAME region=huabei-beijing-4 server=119.29.29.29 33.500371\n\n20:51:34 dns_query_rcode_value agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud domain=www.baidu.com product=n9e record_type=CNAME region=huabei-beijing-4 server=119.29.29.29 0\n20:51:34 dns_query_result_code agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud domain=www.baidu.com product=n9e record_type=CNAME region=huabei-beijing-4 server=119.29.29.29 0\n20:51:34 dns_query_query_time_ms agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud domain=www.baidu.com product=n9e record_type=CNAME region=huabei-beijing-4 server=119.29.29.29 34.328242\n\n20:51:34 dns_query_rcode_value agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud domain=www.huaweicloud.com product=n9e record_type=CNAME region=huabei-beijing-4 server=119.29.29.29 0\n20:51:34 dns_query_result_code agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud domain=www.huaweicloud.com product=n9e record_type=CNAME region=huabei-beijing-4 server=119.29.29.29 0\n20:51:34 dns_query_query_time_ms agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud domain=www.huaweicloud.com product=n9e record_type=CNAME region=huabei-beijing-4 server=119.29.29.29\n.....\n\n```\n# é‡å¯æœåŠ¡\n```\né‡å¯categrafæœåŠ¡ç”Ÿæ•ˆ\nsystemctl daemon-reload && systemctl restart categraf && systemctl status categraf\n\næŸ¥çœ‹å¯åŠ¨æ—¥å¿—æ˜¯å¦æœ‰é”™è¯¯\njournalctl -f -n 500 -u categraf | grep \"E\\!\" | grep \"W\\!\"\n```\n\n# æ£€æŸ¥æ•°æ®å‘ˆç°\nç­‰å¾…1-2åˆ†é’Ÿåæ•°æ®å°±ä¼šåœ¨å›¾è¡¨ä¸­å±•ç¤ºå‡ºæ¥ï¼Œå¦‚å›¾ï¼š\n![image](https://user-images.githubusercontent.com/12181410/220353480-e17a7822-7ccc-4fdf-b18b-a0be84cd5550.png)\n\n# ç›‘æ§å‘Šè­¦è§„åˆ™é…ç½®\n```\nä¸ªäººç»éªŒä»…ä¾›å‚è€ƒï¼Œä¸€èˆ¬DNSè§£æå»¶è¿Ÿæ—¶é—´ï¼š\nè¶…è¿‡2000æ¯«ç§’ï¼Œä¸ºP2çº§åˆ«ï¼Œå¯ç”¨ä¼ä¸šå¾®ä¿¡åº”ç”¨æ¨é€å‘Šè­¦ï¼Œ3åˆ†é’Ÿå†…æ¢å¤å‘å‡ºæ¢å¤å‘Šè­¦ã€‚\nè¶…è¿‡5000æ¯«ç§’ï¼Œä¸ºP1çº§åˆ«ï¼Œå¯ç”¨ç”µè¯è¯­éŸ³å‘Šè­¦&ä¼ä¸šå¾®ä¿¡åº”ç”¨å‘Šè­¦ï¼Œ3åˆ†é’Ÿå†…æ¢å¤å‘å‡ºæ¢å¤å‘Šè­¦ã€‚\n\nä¸ºä»€ä¹ˆä¼šè¿™ä¹ˆè€ƒé‡è®¾è®¡ï¼Ÿ\nåœ¨ç”¨åˆ°DNSç›‘æ§æ—¶ï¼Œä¸€èˆ¬å…¬å¸ä¸šåŠ¡æ˜¯éå¸ƒå…¨å›½çš„ï¼Œç„¶è€Œå…¨å›½å„ä¸ªåœ°åŒºåœ¨è§£æDNSå­˜åœ¨å„ç§åœºæ™¯å› ç´ å¯¼è‡´çš„DNSé—®é¢˜ï¼ˆå¦‚DNSè¢«åŠ«æŒã€ç‰‡åŒºDNSæœåŠ¡å™¨æ•…éšœç­‰ï¼‰ï¼Œæ‰€ä»¥éœ€è¦ä»¥é«˜çº§åˆ«å¯¹å¾…ã€‚\nä»æ”¶åˆ°å‘Šè­¦åˆ°æ¢å¤å‘Šè­¦è®¾ç½®3åˆ†é’Ÿçš„æ„å›¾æ˜¯é˜²æ­¢æœŸé—´æ˜¯çŸ­æš‚æ—¶é—´æœ‰é—®é¢˜,åŒæ—¶ä¹Ÿç»™SLA(99.99%)ç»™è¶³å¤„ç†æ—¶é•¿ã€‚\n```\n\n# ç›‘æ§å›¾è¡¨é…ç½®\n```\nå…ˆç•¥è¿‡\n```\n', 'å¯¹DNSæœåŠ¡å™¨çš„å“åº”ç›‘æµ‹', '1732760185', 'ç‹æ¨(822032277)', '1732764723', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('74', 'Mtail', '[[instances]]\n# progs = \"/path/to/prog1\" # prog dir1\n# logs = [\"/path/to/a.log\", \"path/to/b.log\"]\n# override_timezone = \"Asia/Shanghai\"\n# emit_metric_timestamp = \"true\" #string type\n\n# [[instances]]\n# progs = \"/path/to/prog2\" # prog dir2\n# logs = [\"/path/to/logdir/\"]\n# override_timezone = \"Asia/Shanghai\"\n# emit_metric_timestamp = \"true\" # string type', '# mtailæ’ä»¶\n\n## ç®€ä»‹\nåŠŸèƒ½ï¼šæå–æ—¥å¿—å†…å®¹ï¼Œè½¬æ¢ä¸ºç›‘æ§metrics\n\n+ è¾“å…¥ï¼š æ—¥å¿—\n+ è¾“å‡ºï¼š metrics æŒ‰ç…§mtailè¯­æ³•è¾“å‡º, ä»…æ”¯æŒcounterã€gaugeã€histogram\n+ å¤„ç†ï¼š æœ¬è´¨æ˜¯golangçš„æ­£åˆ™æå–+è¡¨è¾¾å¼è®¡ç®—\n\n## å¯åŠ¨\nç¼–è¾‘mtail.tomlæ–‡ä»¶, ä¸€èˆ¬æ¯ä¸ªinstanceéœ€è¦æŒ‡å®šä¸åŒçš„progså‚æ•°ï¼ˆä¸åŒçš„progsæ–‡ä»¶æˆ–è€…ç›®å½•ï¼‰,å¦åˆ™æŒ‡æ ‡ä¼šç›¸äº’å¹²æ‰°ã€‚\n**æ³¨æ„**: å¦‚æœä¸åŒinstanceä½¿ç”¨ç›¸åŒprogs, å¯ä»¥é€šè¿‡ç»™æ¯ä¸ªinstanceå¢åŠ labelsåšåŒºåˆ†ï¼Œ\n```\nlabels = { k1=v1 }\n```\næˆ–\n```\n[instances.labels]\nk1=v1\n```\n\n1. conf/inputs.mtail/mtail.tomlä¸­æŒ‡å®šinstance\n```toml\n\n[[instances]]\n## æŒ‡å®šmtail progçš„ç›®å½•\nprogs = \"/path/to/prog1\"\n## æŒ‡å®šmtailè¦è¯»å–çš„æ—¥å¿—\nlogs = [\"/path/to/a.log\", \"path/to/b.log\"] \n## æŒ‡å®šæ—¶åŒº\n# override_timezone = \"Asia/Shanghai\" \n## metricsæ˜¯å¦å¸¦æ—¶é—´æˆ³ï¼Œæ³¨æ„ï¼Œè¿™é‡Œæ˜¯\"true\"\n# emit_metric_timestamp = \"true\" \n\n```\n2. åœ¨/path/to/prog1 ç›®å½•ä¸‹ç¼–å†™è§„åˆ™æ–‡ä»¶\n```\ngauge xxx_errors\n/ERROR.*/ {\n    xxx_errros++\n}\n```\n\n3. ä¸€ä¸ªtabä¸­æ‰§è¡Œ `categraf --test --inputs mtail`ï¼Œç”¨äºæµ‹è¯• \n4. å¦ä¸€ä¸ªtabä¸­ï¼Œ\"/path/to/a.log\" æˆ–è€… \"path/to/b.log\" è¿½åŠ ä¸€è¡Œ ERRORï¼Œçœ‹çœ‹categrafçš„è¾“å‡º\n5. æµ‹è¯•é€šè¿‡åï¼Œå¯åŠ¨categraf\n\n### è¾“å…¥\nlogså‚æ•°æŒ‡å®šè¦å¤„ç†çš„æ—¥å¿—æº, æ”¯æŒæ¨¡ç³ŠåŒ¹é…, æ”¯æŒå¤šä¸ªlogæ–‡ä»¶ã€‚\n\n### å¤„ç†è§„åˆ™\n`progs`æŒ‡å®šå…·ä½“çš„è§„åˆ™æ–‡ä»¶ç›®å½•(æˆ–æ–‡ä»¶)\n\n\n## å¤„ç†è§„åˆ™ä¸è¯­æ³•\n\n### å¤„ç†æµç¨‹\n```python \nfor line in lines:\n  for regex in regexes:\n    if match:\n      do something\n```\n\n### è¯­æ³•\n\n``` golang\nexported variable \n\npattern { \n  action statements\n} \n\ndef decorator { \n  pattern and action statements\n}\n```\n\n#### å®šä¹‰æŒ‡æ ‡åç§°\nå‰é¢ä¹Ÿæè¿‡ï¼ŒæŒ‡æ ‡ä»…æ”¯æŒ counter gauge histogram ä¸‰ç§ç±»å‹ã€‚\nä¸€ä¸ªğŸŒ°\n```mtail\ncounter lines\n/INFO.*/ {\n    lines++\n}\n```\n\næ³¨æ„ï¼Œå®šä¹‰çš„åç§°åªæ”¯æŒ Cç±»å‹çš„å‘½åæ–¹å¼(å­—æ¯/æ•°å­—/ä¸‹åˆ’çº¿)ï¼Œ**å¦‚æœæƒ³ä½¿ç”¨\"-\" è¦ä½¿ç”¨\"as\"å¯¼å‡ºåˆ«å**ã€‚ä¾‹å¦‚ï¼Œ\n```mtail\ncounter lines_total as \"line-count\"\n```\nè¿™æ ·è·å–åˆ°çš„å°±æ˜¯line-countè¿™ä¸ªæŒ‡æ ‡åç§°äº†\n\n#### åŒ¹é…ä¸è®¡ç®—ï¼ˆpattern/action)\n\n```mtail\nPATTERN {\nACTION\n}\n```\n\nä¾‹å­\n```mtail\n/foo/ {\n  ACTION1\n}\n\nvariable > 0 {\n  ACTION2\n}\n\n/foo/ && variable > 0 {\n  ACTION3\n}\n```\næ”¯æŒRE2æ­£åˆ™åŒ¹é…\n```mtail\nconst PREFIX /^\\w+\\W+\\d+ /\n\nPREFIX {\n  ACTION1\n}\n\nPREFIX + /foo/ {\n  ACTION2\n}\n```\n\nè¿™æ ·ï¼ŒACTION1 æ˜¯åŒ¹é…ä»¥å°å†™å­—ç¬¦+å¤§å†™å­—ç¬¦+æ•°å­—+ç©ºæ ¼çš„è¡Œï¼ŒACTION2 æ˜¯åŒ¹é…å°å†™å­—ç¬¦+å¤§å†™å­—ç¬¦+æ•°å­—+ç©ºæ ¼+fooå¼€å¤´çš„è¡Œã€‚\n\n#### å…³ç³»è¿ç®—ç¬¦\n+ `<` å°äº `<=` å°äºç­‰äº\n+ `>` å¤§äº `>=` å¤§äºç­‰äº\n+ `==` ç›¸ç­‰ `!=` ä¸ç­‰\n+ `=~` åŒ¹é…(æ¨¡ç³Š) `!~` ä¸åŒ¹é…(æ¨¡ç³Š)\n+ `||` é€»è¾‘æˆ– `&&` é€»è¾‘ä¸ `!` é€»è¾‘é\n \n#### æ•°å­¦è¿ç®—ç¬¦\n+ `|` æŒ‰ä½æˆ–\n+ `&` æŒ‰ä½ä¸\n+ `^` æŒ‰ä½å¼‚æˆ–\n+ `+ - * /` å››åˆ™è¿ç®—\n+ `<<` æŒ‰ä½å·¦ç§»\n+ `>>` æŒ‰ä½å³ç§»\n+ `**` æŒ‡æ•°è¿ç®— \n+ `=` èµ‹å€¼\n+ `++` è‡ªå¢è¿ç®—\n+ `--` è‡ªå‡è¿ç®—\n+ `+=` åŠ ä¸”èµ‹å€¼\n\n#### æ”¯æŒelseä¸otherwise\n```mtail\n/foo/ {\nACTION1\n} else {\nACTION2\n}\n```\næ”¯æŒåµŒå¥—\n```mtail\n/foo/ {\n  /foo1/ {\n     ACTION1\n  }\n  /foo2/ {\n     ACTION2\n  }\n  otherwise {\n     ACTION3\n  }\n}\n```\n\næ”¯æŒå‘½åä¸éå‘½åæå–\n\n```mtail\n/(?P<operation>\\S+) (\\S+) \\[\\S+\\] (\\S+) \\(\\S*\\) \\S+ (?P<bytes>\\d+)/ {\n  bytes_total[$operation][$3] += $bytes\n}\n```\nå¢åŠ å¸¸é‡label \n```mtail\n# test.mtail\n# å®šä¹‰å¸¸é‡label env\nhidden text env\n# ç»™label èµ‹å€¼ è¿™æ ·å®šä¹‰æ˜¯globalèŒƒå›´;\n# å±€éƒ¨æ·»åŠ ï¼Œåˆ™åœ¨å¯¹åº”çš„conditionä¸­æ·»åŠ \nenv=\"production\"\ncounter line_total by logfile,env\n/^(?P<date>\\w+\\s+\\d+\\s+\\d+:\\d+:\\d+)/ {\n    line_total[getfilename()][env]++\n}\n```\nè·å–åˆ°çš„metricsä¸­ä¼šæ·»åŠ ä¸Š`env=production`çš„label å¦‚ä¸‹ï¼š\n```mtail\n# metrics\nline_total{env=\"production\",logfile=\"/path/to/xxxx.log\",prog=\"test.mtail\"} 4 1661165941788\n```\n\nå¦‚æœè¦ç»™metricså¢åŠ å˜é‡labelï¼Œå¿…é¡»è¦ä½¿ç”¨å‘½åæå–ã€‚ä¾‹å¦‚\n```python\n# æ—¥å¿—å†…å®¹\n192.168.0.1 GET /foo\n192.168.0.2 GET /bar\n192.168.0.1 POST /bar\n```\n\n``` mtail\n# test.mtail\ncounter my_http_requests_total by log_file, verb \n/^/ +\n/(?P<host>[0-9A-Za-z\\.:-]+) / +\n/(?P<verb>[A-Z]+) / +\n/(?P<URI>\\S+).*/ +\n/$/ {\n    my_http_requests_total[getfilename()][$verb]++\n}\n```\n\n```python\n# metrics\nmy_http_requests_total{logfile=\"xxx.log\",verb=\"GET\",prog=\"test.mtail\"} 4242\nmy_http_requests_total{logfile=\"xxx.log\",verb=\"POST\",prog=\"test.mtail\"} 42\n```\n\nå‘½åæå–çš„å˜é‡å¯ä»¥åœ¨æ¡ä»¶ä¸­ä½¿ç”¨\n```mtail\n/(?P<x>\\d+)/ && $x > 1 {\nnonzero_positives++\n}\n```\n\n#### æ—¶é—´å¤„ç†\nä¸æ˜¾ç¤ºå¤„ç†ï¼Œåˆ™é»˜è®¤ä½¿ç”¨ç³»ç»Ÿæ—¶é—´\n\né»˜è®¤emit_metric_timestamp=\"false\" ï¼ˆæ³¨æ„æ˜¯å­—ç¬¦ä¸²ï¼‰\n```\nhttp_latency_bucket{prog=\"histo.mtail\",le=\"1\"} 0\nhttp_latency_bucket{prog=\"histo.mtail\",le=\"2\"} 0\nhttp_latency_bucket{prog=\"histo.mtail\",le=\"4\"} 0\nhttp_latency_bucket{prog=\"histo.mtail\",le=\"8\"} 0\nhttp_latency_bucket{prog=\"histo.mtail\",le=\"+Inf\"} 0\nhttp_latency_sum{prog=\"histo.mtail\"} 0\nhttp_latency_count{prog=\"histo.mtail\"} 0\n```\n\nå‚æ•° emit_metric_timestamp=\"true\" (æ³¨æ„æ˜¯å­—ç¬¦ä¸²)\n```\nhttp_latency_bucket{prog=\"histo.mtail\",le=\"1\"} 1 1661152917471\nhttp_latency_bucket{prog=\"histo.mtail\",le=\"2\"} 2 1661152917471\nhttp_latency_bucket{prog=\"histo.mtail\",le=\"4\"} 2 1661152917471\nhttp_latency_bucket{prog=\"histo.mtail\",le=\"8\"} 2 1661152917471\nhttp_latency_bucket{prog=\"histo.mtail\",le=\"+Inf\"} 2 1661152917471\nhttp_latency_sum{prog=\"histo.mtail\"} 3 1661152917471\nhttp_latency_count{prog=\"histo.mtail\"} 4 1661152917471\n```\n\nä½¿ç”¨æ—¥å¿—çš„æ—¶é—´\n```\nAug 22 15:28:32 GET /api/v1/pods latency=2s code=200\nAug 22 15:28:32 GET /api/v1/pods latency=1s code=200\nAug 22 15:28:32 GET /api/v1/pods latency=0s code=200\n```\n\n```\nhistogram http_latency buckets 1, 2, 4, 8\n/^(?P<date>\\w+\\s+\\d+\\s+\\d+:\\d+:\\d+)/ {\n        strptime($date, \"Jan 02 15:04:05\")\n	/latency=(?P<latency>\\d+)/ {\n		http_latency=$latency\n	}\n}\n```\n\næ—¥å¿—æå–çš„æ—¶é—´ï¼Œä¸€å®šè¦æ³¨æ„æ—¶åŒºé—®é¢˜ï¼Œæœ‰ä¸€ä¸ªå‚æ•° `override_timezone` å¯ä»¥æ§åˆ¶æ—¶åŒºé€‰æ‹©ï¼Œå¦åˆ™é»˜è®¤ä½¿ç”¨UTCè½¬æ¢ã€‚\næ¯”å¦‚æˆ‘å¯åŠ¨æ—¶æŒ‡å®š `override_timezone=Asia/Shanghai`, è¿™ä¸ªæ—¶å€™æ—¥å¿—æå–çš„æ—¶é—´ä¼šå½“åšä¸œå…«åŒºæ—¶é—´ è½¬æ¢ä¸ºtimestampï¼Œ ç„¶åå†ä»timestampè½¬æ¢ä¸ºå„æ—¶åŒºæ—¶é—´æ—¶ å°±æ²¡æœ‰é—®é¢˜äº†,å¦‚å›¾ã€‚\n![timestamp](https://cdn.jsdelivr.net/gh/flashcatcloud/categraf@main/inputs/mtail/timestamp.png)\nå¦‚æœä¸å¸¦ `override_timezone=Asia/Shanghai`, åˆ™é»˜è®¤å°†`Aug 22 15:34:32` å½“åšUTCæ—¶é—´ï¼Œè½¬æ¢ä¸ºtimestampã€‚ è¿™æ ·å†è½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´æ—¶ï¼Œä¼šå¤šäº†8ä¸ªå°æ—¶, å¦‚å›¾ã€‚\n![timestamp](https://cdn.jsdelivr.net/gh/flashcatcloud/categraf@main/inputs/mtail/timezone.png)\n', 'æå–æ—¥å¿—å†…å®¹ï¼Œè½¬æ¢ä¸ºç›‘æ§metrics', '1732760387', 'ç‹æ¨(822032277)', '1732760387', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('75', 'PHPFPM', '# # collect interval\n# interval = 15\n\n[[instances]]\n## An array of Nginx stub_status URI to gather stats.\nurls = [\n## HTTP: the URL must start with http:// or https://, ie:\n#    \"http://localhost/status\",\n#    \"https://www.baidu.com/phpfpm-status\",\n## fcgi: the URL must start with fcgi:// or cgi://, and port must be present, ie:\n#    \"fcgi://127.0.0.1:9001\",\n#    \"cgi://192.168.0.1:9000/status\",\n## Unix socket: path to fpm socket, ie:\n#    \"/run/php/php7.2-fpm.sock\",\n##    or using a custom fpm status path:\n#    \"/var/run/php5-fpm.sock:/fpm-custom-status-path\",\n##    glob patterns are also supported:\n#    \"/var/run/php*.sock\"\n]\n\n## append some labels for series\n# labels = { region=\"cloud\", product=\"n9e\" }\n\n## interval = global.interval * interval_times\n# interval_times = 1\n\n## Set response_timeout (default 5 seconds),HTTP urls only\nresponse_timeout = \"5s\"\n\n## Whether to follow redirects from the server (defaults to false),HTTP urls only\n# follow_redirects = false\n\n## Optional HTTP Basic Auth Credentials,HTTP urls only\n#username = \"admin\"\n#password = \"admin\"\n\n## Optional headers,HTTP urls only\n# headers = [\"X-From\", \"categraf\", \"X-Xyz\", \"abc\"]\n\n## Optional TLS Config,only http\n# use_tls = false\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = false', '# PHP-FPM\n\n*PHP-FPM* (PHP FastCGI Process Manager) ç›‘æ§é‡‡é›†æ’ä»¶ã€‚\n\nè¯¥æ’ä»¶éœ€è¦æ›´æ”¹phpfpmçš„é…ç½®æ–‡ä»¶ï¼Œå¼€å¯ *pm.status_path*é…ç½®é¡¹\n```\npm.status_path = /status\n```\n\n\n## Configuration\n\n### æ³¨æ„äº‹é¡¹ï¼š\n1. å¦‚ä¸‹é…ç½® ä»…ç”Ÿæ•ˆäºHTTPçš„url\n    - response_timeout\n    - username & password\n    - headers\n    - TLS config\n2. å¦‚æœä½¿ç”¨ Unix socketï¼Œéœ€è¦ä¿è¯ categraf å’Œ socket path åœ¨åŒä¸€ä¸ªä¸»æœºä¸Šï¼Œä¸” categraf è¿è¡Œç”¨æˆ·æ‹¥æœ‰è¯»å–è¯¥ path çš„æƒé™ã€‚\n## ç›‘æ§å¤§ç›˜å’Œå‘Šè­¦è§„åˆ™\n\nå¾…æ›´æ–°...', '(PHP FastCGI Process Manager) ç›‘æ§é‡‡é›†æ’ä»¶', '1732760539', 'ç‹æ¨(822032277)', '1732760574', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('76', 'TomCat', '# # collect interval\n# interval = 15\n\n# Gather metrics from the Tomcat server status page.\n[[instances]]\n## URL of the Tomcat server status\n# url = \"http://127.0.0.1:8080/manager/status/all?XML=true\"\nurl = \"\"\n\n## HTTP Basic Auth Credentials\n# username = \"tomcat\"\n# password = \"s3cret\"\n\n## Request timeout\n# timeout = \"5s\"\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# important! use global unique string to specify instance\n# labels = { instance=\"192.168.1.2:8080\", url=\"-\" }\n\n## Optional TLS Config\n# use_tls = false\n# tls_min_version = \"1.2\"\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = true\n', '# tomcat\n\ntomcat é‡‡é›†å™¨ï¼Œæ˜¯è¯»å– tomcat çš„ç®¡ç†ä¾§æ¥å£ `/manager/status/all` è¿™ä¸ªæ¥å£éœ€è¦é‰´æƒã€‚ä¿®æ”¹ `tomcat-users.xml` ï¼Œå¢åŠ ä¸‹é¢çš„å†…å®¹ï¼š\n\n```xml\n<role rolename=\"admin-gui\" />\n<user username=\"tomcat\" password=\"s3cret\" roles=\"manager-gui\" />\n```\n\næ­¤å¤–ï¼Œè¿˜éœ€è¦æ³¨é‡Šæ–‡ä»¶**webapps/manager/META-INF/context.xml**çš„ä»¥ä¸‹å†…å®¹ï¼Œ\n```xml\n  <Valve className=\"org.apache.catalina.valves.RemoteAddrValve\"\n         allow=\"127\\.\\d+\\.\\d+\\.\\d+|::1|0:0:0:0:0:0:0:1\" />\n```\n\nå¦åˆ™ tomcat ä¼šæŠ¥ä»¥ä¸‹é”™è¯¯ï¼Œå¯¼è‡´ tomcat é‡‡é›†å™¨æ— æ³•é‡‡é›†åˆ°æ•°æ®ã€‚\n\n```html\n403 Access Denied\nYou are not authorized to view this page.\n\nBy default the Manager is only accessible from a browser running on the same machine as Tomcat. If you wish to modify this restriction, you\'ll need to edit the Manager\'s context.xml file.\n```\n\n## Configuration\n\né…ç½®æ–‡ä»¶åœ¨ `conf/input.tomcat/tomcat.toml`\n\n```toml\n[[instances]]\n## URL of the Tomcat server status\nurl = \"http://127.0.0.1:8080/manager/status/all?XML=true\"\n\n## HTTP Basic Auth Credentials\nusername = \"tomcat\"\npassword = \"s3cret\"\n\n## Request timeout\n# timeout = \"5s\"\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# important! use global unique string to specify instance\n# labels = { instance=\"192.168.1.2:8080\", url=\"-\" }\n\n## Optional TLS Config\n# use_tls = false\n# tls_min_version = \"1.2\"\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = true\n```\n\n## ç›‘æ§å¤§ç›˜\n\nå¤œèºå†…ç½®äº† tomcat ä»ªè¡¨ç›˜ï¼Œå…‹éš†åˆ°è‡ªå·±çš„ä¸šåŠ¡ç»„ä¸‹ä½¿ç”¨å³å¯ã€‚', 'tomcat é‡‡é›†å™¨', '1732762101', 'ç‹æ¨(822032277)', '1732762101', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('77', 'SQLServer', '# # collect interval\n# interval = 15\n\n[[instances]]\n\n## Specify instances to monitor with a list of connection strings.\n## All connection parameters are optional.\n## By default, the host is localhost, listening on default port, TCP 1433.\n##   for Windows, the user is the currently running AD user (SSO).\n##   See https://github.com/denisenkom/go-mssqldb for detailed connection\n##   parameters, in particular, tls connections can be created like so:\n##   \"encrypt=true;certificate=<cert>;hostNameInCertificate=<SqlServer host fqdn>\"\n# servers = [\"Server=server.xxx.com;Port=1433;User Id=monitor;Password=xxxxxx;app name=categraf;log=1;\"]\n# servers = [ ]\n\n## Authentication method\n## valid methods: \"connection_string\", \"AAD\"\n# auth_method = \"connection_string\"\n\n## \"database_type\" enables a specific set of queries depending on the database type. If specified, it replaces azuredb = true/false and query_version = 2\n## In the config file, the sql server plugin section should be repeated each with a set of servers for a specific database_type.\n## Possible values for database_type are - \"SQLServer\" or \"AzureSQLDB\" or \"AzureSQLManagedInstance\" or \"AzureSQLPool\"\n\ndatabase_type = \"SQLServer\"\n\n## A list of queries to include. If not specified, all the below listed queries are used.\ninclude_query = []\n\n## A list of queries to explicitly ignore.\nexclude_query = [\"SQLServerAvailabilityReplicaStates\", \"SQLServerDatabaseReplicaStates\"]\n\n## Queries enabled by default for database_type = \"SQLServer\" are -\n## SQLServerPerformanceCounters, SQLServerWaitStatsCategorized, SQLServerDatabaseIO, SQLServerProperties, SQLServerMemoryClerks,\n## SQLServerSchedulers, SQLServerRequests, SQLServerVolumeSpace, SQLServerCpu, SQLServerAvailabilityReplicaStates, SQLServerDatabaseReplicaStates,\n## SQLServerRecentBackups\n\n\n\n## Following are old config settings\n## You may use them only if you are using the earlier flavor of queries, however it is recommended to use\n## the new mechanism of identifying the database_type there by use it\'s corresponding queries\n\n## Optional parameter, setting this to 2 will use a new version\n## of the collection queries that break compatibility with the original\n## dashboards.\n## Version 2 - is compatible from SQL Server 2012 and later versions and also for SQL Azure DB\n# query_version = 2\n\n## Toggling this to true will emit an additional metric called \"sqlserver_telegraf_health\".\n## This metric tracks the count of attempted queries and successful queries for each SQL instance specified in \"servers\".\n## The purpose of this metric is to assist with identifying and diagnosing any connectivity or query issues.\n## This setting/metric is optional and is disabled by default.\n# health_metric = false\n\n## Possible queries accross different versions of the collectors\n## Queries enabled by default for specific Database Type\n\n## database_type =  SQLServer by default collects the following queries\n## - SQLServerPerformanceCounters\n## - SQLServerWaitStatsCategorized\n## - SQLServerDatabaseIO\n## - SQLServerProperties\n## - SQLServerMemoryClerks\n## - SQLServerSchedulers\n## - SQLServerRequests\n## - SQLServerVolumeSpace\n## - SQLServerCpu\n## - SQLServerRecentBackups\n## and following as optional (if mentioned in the include_query list)\n## - SQLServerAvailabilityReplicaStates\n## - SQLServerDatabaseReplicaStates\n\n## Version 2 by default collects the following queries\n## Version 2 is being deprecated, please consider using database_type.\n## - PerformanceCounters\n## - WaitStatsCategorized\n## - DatabaseIO\n## - ServerProperties\n## - MemoryClerk\n## - Schedulers\n## - SqlRequests\n## - VolumeSpace\n## - Cpu\n\n## Version 1 by default collects the following queries\n## Version 1 is deprecated, please consider using database_type.\n## - PerformanceCounters\n## - WaitStatsCategorized\n## - CPUHistory\n## - DatabaseIO\n## - DatabaseSize\n## - DatabaseStats\n## - DatabaseProperties\n## - MemoryClerk\n## - VolumeSpace\n## - PerformanceMetrics', '# sqlserver\n\nforked from telegraf/sqlserver. è¿™ä¸ªæ’ä»¶çš„ä½œç”¨æ˜¯è·å–sqlserverçš„ç›‘æ§æŒ‡æ ‡ï¼Œè¿™é‡Œå»æ‰äº†Azureç›¸å…³éƒ¨åˆ†ç›‘æ§ï¼Œåªä¿ç•™äº†æœ¬åœ°éƒ¨ç½²sqlserveræƒ…å†µã€‚\n\n# ä½¿ç”¨\næŒ‰ç…§ä¸‹é¢æ–¹æ³•åˆ›å»ºç›‘æ§è´¦å·ï¼Œç”¨äºè¯»å–ç›‘æ§æ•°æ®\nUSE master;\n\nCREATE LOGIN [categraf] WITH PASSWORD = N\'mystrongpassword\';\n\nGRANT VIEW SERVER STATE TO [categraf];\n\nGRANT VIEW ANY DEFINITION TO [categraf];\nData Source=10.19.1.1;Initial Catalog=hc;User ID=sa;Password=mystrongpassword;', 'sqlserverç›‘æ§æŒ‡æ ‡é‡‡é›†', '1732762184', 'ç‹æ¨(822032277)', '1732762184', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('78', 'Nginx', '# # collect interval\n# interval = 15\n\n[[instances]]\n## An array of Nginx stub_status URI to gather stats.\nurls = [\n    \"https://nginx.domains.com\"\n]\n\n## append some labels for series\nlabels = { cloud=\"my-cloud\", region=\"my-region\",azone=\"az1\", product=\"my-product\" }\n\n## interval = global.interval * interval_times\n# interval_times = 1\n\n## Set response_timeout (default 5 seconds)\nresponse_timeout = \"5s\"\n\n## Whether to follow redirects from the server (defaults to false)\n# follow_redirects = false\n\n## Optional HTTP Basic Auth Credentials\n#username = \"admin\"\n#password = \"admin\"\n\n## Optional headers\n# headers = [\"X-From\", \"categraf\", \"X-Xyz\", \"abc\"]\n\n## Optional TLS Config\n# use_tls = false\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = false', '- è¯¥æ’ä»¶ä¾èµ– **nginx** çš„ **http_stub_status_module**\n\n# åº”ç”¨åœºæ™¯\nä¸€èˆ¬ç”¨äºä¸šåŠ¡ç³»ç»Ÿåšå¯¹å¤–æˆ–å¯¹å¤–è·¯ç”±æ˜ å°„æ—¶ä½¿ç”¨ä»£ç†æœåŠ¡ï¼Œæ˜¯è¿ç»´æœ€å¸¸è§ä¸”æœ€é‡è¦çš„ä»£ç†å·¥å…·ã€‚\n\n# éƒ¨ç½²åœºæ™¯\néœ€è¦åœ¨è£…æœ‰nginxæœåŠ¡çš„è™šæ‹Ÿæœºå¯ç”¨æ­¤æ’ä»¶ã€‚\n\n\n# å‰ç½®æ¡ä»¶\n```\næ¡ä»¶1ï¼šnginxæœåŠ¡éœ€è¦å¯ç”¨http_stub_status_moduleæ¨¡å—\n\næ¨èæºç ç¼–è¯‘æ–¹å¼å®‰è£…æ¨¡å—ï¼Œå¦‚ä¸æ¸…æ¥šè¦å®‰è£…å“ªäº›æ¨¡å—ï¼Œå¯å‚è€ƒï¼š\ncd /opt/nginx-1.20.1 && ./configure \\\n--prefix=/usr/share/nginx \\\n--sbin-path=/usr/sbin/nginx \\\n--modules-path=/usr/lib64/nginx/modules \\\n--conf-path=/etc/nginx/nginx.conf \\\n--error-log-path=/var/log/nginx/error.log \\\n--http-log-path=/var/log/nginx/access.log \\\n--http-client-body-temp-path=/var/lib/nginx/tmp/client_body \\\n--http-proxy-temp-path=/var/lib/nginx/tmp/proxy \\\n--http-fastcgi-temp-path=/var/lib/nginx/tmp/fastcgi \\\n--http-uwsgi-temp-path=/var/lib/nginx/tmp/uwsgi \\\n--http-scgi-temp-path=/var/lib/nginx/tmp/scgi \\\n--pid-path=/var/run/nginx.pid \\\n--lock-path=/run/lock/subsys/nginx \\\n--user=nginx \\\n--group=nginx \\\n--with-compat \\\n--with-threads \\\n--with-http_addition_module \\\n--with-http_auth_request_module \\\n--with-http_dav_module \\\n--with-http_flv_module \\\n--with-http_gunzip_module \\\n--with-http_gzip_static_module \\\n--with-http_mp4_module \\\n--with-http_random_index_module \\\n--with-http_realip_module \\\n--with-http_secure_link_module \\\n--with-http_slice_module \\\n--with-http_ssl_module \\\n--with-http_stub_status_module \\\n--with-http_sub_module \\\n--with-http_v2_module \\\n--with-mail \\\n--with-mail_ssl_module \\\n--with-stream \\\n--with-stream_realip_module \\\n--with-stream_ssl_module \\\n--with-stream_ssl_preread_module \\\n--with-select_module \\\n--with-poll_module \\\n--with-file-aio \\\n--with-http_xslt_module=dynamic \\\n--with-http_image_filter_module=dynamic \\\n--with-http_perl_module=dynamic \\\n--with-stream=dynamic \\\n--with-mail=dynamic \\\n--with-http_xslt_module=dynamic \\\n--add-module=/etc/nginx/third-modules/nginx_upstream_check_module \\\n--add-module=/etc/nginx/third-modules/ngx_devel_kit-0.3.0 \\\n--add-module=/etc/nginx/third-modules/lua-nginx-module-0.10.13 \\\n--add-module=/etc/nginx/third-modules/nginx-module-vts \\\n--add-module=/etc/nginx/third-modules/ngx-fancyindex-0.5.2\n\n# æ ¹æ®cpuæ ¸æ•°\nmake -j2\nmake install\n\næ³¨æ„ï¼šç¬¬ä¸‰æ–¹æ¨¡å—nginx_upstream_check_module lua-nginx-module nginx-module-vts éƒ½æ˜¯ç›¸å…³æ’ä»¶æ‰€å¿…å¤‡çš„ä¾èµ–ã€‚\n```\n\n```\næ¡ä»¶2ï¼šnginxå¯ç”¨stub_statusé…ç½®ã€‚\n\n[root@aliyun conf.d]# cat nginx.domains.com.conf\nserver {\n    listen 80;\n    listen 443 ssl;\n    server_name nginx.domains.com;\n    include /etc/nginx/ssl_conf/domains.com.conf;\n\n    location / {\n        stub_status on;\n	    include /etc/nginx/ip_whitelist.conf;\n    }\n\n    access_log /var/log/nginx/nginx.domains.com.access.log main;\n    error_log /var/log/nginx/nginx.domains.com.error.log warn;\n}\n\næµè§ˆå™¨è®¿é—®https://nginx.domains.comå‡ºç°ï¼š\nActive connections: 5\nserver accepts handled requests\n 90837 90837   79582\nReading: 0 Writing: 1 Waiting: 4\n\nNginxçŠ¶æ€è§£é‡Šï¼š\nActive connections Nginxæ­£å¤„ç†çš„æ´»åŠ¨è¿æ¥æ•°5ä¸ª\nserver Nginxå¯åŠ¨åˆ°ç°åœ¨å…±å¤„ç†äº†90837ä¸ªè¿æ¥ã€‚\naccepts Nginxå¯åŠ¨åˆ°ç°åœ¨å…±æˆåŠŸåˆ›å»º90837æ¬¡æ¡æ‰‹ã€‚\nhandled requests Nginxæ€»å…±å¤„ç†äº†79582æ¬¡è¯·æ±‚ã€‚\nReading Nginxè¯»å–åˆ°å®¢æˆ·ç«¯çš„ Header ä¿¡æ¯æ•°ã€‚\nWriting Nginxè¿”å›ç»™å®¢æˆ·ç«¯çš„ Header ä¿¡æ¯æ•°ã€‚\nWaiting Nginxå·²ç»å¤„ç†å®Œæ­£åœ¨ç­‰å€™ä¸‹ä¸€æ¬¡è¯·æ±‚æŒ‡ä»¤çš„é©»ç•™é“¾æ¥ï¼ŒKeep-aliveå¯ç”¨æƒ…å†µä¸‹ï¼Œè¿™ä¸ªå€¼ç­‰äºactive-ï¼ˆreading + writingï¼‰ã€‚\nè¯·æ±‚ä¸¢å¤±æ•°=(æ¡æ‰‹æ•°-è¿æ¥æ•°)å¯ä»¥çœ‹å‡º,æœ¬æ¬¡çŠ¶æ€æ˜¾ç¤ºæ²¡æœ‰ä¸¢å¤±è¯·æ±‚ã€‚\n\n```\n\n# é…ç½®åœºæ™¯\n```\næœ¬é…ç½®å¯ç”¨æˆ–æ•°æ®å®šä¹‰å¦‚ä¸‹åŠŸèƒ½ï¼š\nå¢åŠ è‡ªå®šä¹‰æ ‡ç­¾ï¼Œå¯é€šè¿‡è‡ªå®šä¹‰æ ‡ç­¾ç­›é€‰æ•°æ®åŠæ›´åŠ ç²¾ç¡®çš„å‘Šè­¦æ¨é€ã€‚\nå“åº”è¶…æ—¶æ—¶é—´ä¸º5ç§’ã€‚\nurlså­—æ®µå¡«å†™æ¡ä»¶2æ‰€å®šä¹‰å¥½çš„åŸŸåã€‚\n```\n\n# ä¿®æ”¹nginx.tomlæ–‡ä»¶é…ç½®\n```\n[root@aliyun input.nginx]# cat nginx.toml\n# # collect interval\n# interval = 15\n\n[[instances]]\n## An array of Nginx stub_status URI to gather stats.\nurls = [\n    \"https://nginx.domains.com\"\n]\n\n## append some labels for series\nlabels = { cloud=\"my-cloud\", region=\"my-region\",azone=\"az1\", product=\"my-product\" }\n\n## interval = global.interval * interval_times\n# interval_times = 1\n\n## Set response_timeout (default 5 seconds)\nresponse_timeout = \"5s\"\n\n## Whether to follow redirects from the server (defaults to false)\n# follow_redirects = false\n\n## Optional HTTP Basic Auth Credentials\n#username = \"admin\"\n#password = \"admin\"\n\n## Optional headers\n# headers = [\"X-From\", \"categraf\", \"X-Xyz\", \"abc\"]\n\n## Optional TLS Config\n# use_tls = false\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = false\n\n```\n\n# æµ‹è¯•é…ç½®\n```\n./categraf --test --inputs nginx\n\n21:46:46 nginx_waiting agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud port=443 product=nginx region=huabei-beijing-4 server=nginx.devops.press 0\n21:46:46 nginx_active agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud port=443 product=nginx region=huabei-beijing-4 server=nginx.devops.press 1\n21:46:46 nginx_accepts agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud port=443 product=nginx region=huabei-beijing-4 server=nginx.devops.press 90794\n21:46:46 nginx_handled agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud port=443 product=nginx region=huabei-beijing-4 server=nginx.devops.press 90794\n21:46:46 nginx_requests agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud port=443 product=nginx region=huabei-beijing-4 server=nginx.devops.press 79458\n21:46:46 nginx_reading agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud port=443 product=nginx region=huabei-beijing-4 server=nginx.devops.press 0\n21:46:46 nginx_writing agent_hostname=aliyun.tjf.n9e.001 azone=az1 cloud=huaweicloud port=443 product=nginx region=huabei-beijing-4 server=nginx.devops.press 1\n\n```\n# é‡å¯æœåŠ¡\n```\né‡å¯categrafæœåŠ¡ç”Ÿæ•ˆ\nsystemctl daemon-reload && systemctl restart categraf && systemctl status categraf\n\næŸ¥çœ‹å¯åŠ¨æ—¥å¿—æ˜¯å¦æœ‰é”™è¯¯\njournalctl -f -n 500 -u categraf | grep \"E\\!\" | grep \"W\\!\"\n```\n\n# æ£€æŸ¥æ•°æ®å‘ˆç°\nç­‰å¾…1-2åˆ†é’Ÿåæ•°æ®å°±ä¼šåœ¨å›¾è¡¨ä¸­å±•ç¤ºå‡ºæ¥ï¼Œå¦‚å›¾ï¼š\n![image](https://user-images.githubusercontent.com/12181410/220639442-5d02a9ec-f0ae-48f5-91f0-4c7839b747b5.png)\n\n\n# ç›‘æ§å‘Šè­¦è§„åˆ™é…ç½®\n\nä¸ªäººç»éªŒä»…ä¾›å‚è€ƒï¼š\n\n- è¶…è¿‡2000æ¯«ç§’ï¼Œä¸ºP2çº§åˆ«ï¼Œå¯ç”¨ä¼ä¸šå¾®ä¿¡åº”ç”¨æ¨é€å‘Šè­¦ï¼Œ3åˆ†é’Ÿå†…æ¢å¤å‘å‡ºæ¢å¤å‘Šè­¦ã€‚\n- è¶…è¿‡5000æ¯«ç§’ï¼Œä¸ºP1çº§åˆ«ï¼Œå¯ç”¨ç”µè¯è¯­éŸ³å‘Šè­¦&ä¼ä¸šå¾®ä¿¡åº”ç”¨å‘Šè­¦ï¼Œ3åˆ†é’Ÿå†…æ¢å¤å‘å‡ºæ¢å¤å‘Šè­¦ã€‚\n\n\n# ç›‘æ§å›¾è¡¨é…ç½®\n\nhttps://github.com/flashcatcloud/categraf/blob/main/inputs/nginx_vts/dashboards.json\n\n# æ•…éšœè‡ªæ„ˆé…ç½®\n```\nå…ˆç•¥è¿‡\n```', 'ä¾èµ– nginxçš„http_stub_status_module', '1732762574', 'ç‹æ¨(822032277)', '1732762947', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('79', 'Nginx_Upstream_Check', '# # collect interval\n# interval = 15\n\n[[instances]]\n# è¿™ä¸ªé…ç½®æœ€å…³é”®ï¼Œæ˜¯è¦ç»™å‡ºè·å– status ä¿¡æ¯çš„æ¥å£åœ°å€\ntargets = [\n    \"https://nginx-upstream.domains.com/?format=json\"\n]\n\n# æ ‡ç­¾è¿™ä¸ªé…ç½®è¯·æ³¨æ„\n# å¦‚æœ Categraf å’Œ Nginx æ˜¯åœ¨ä¸€å°æœºå™¨ä¸Šï¼Œtarget å¯èƒ½é…ç½®çš„æ˜¯ 127.0.0.1\n# å¦‚æœ Nginx æœ‰å¤šå°æœºå™¨ï¼Œæ¯å°æœºå™¨éƒ½æœ‰ Categraf æ¥é‡‡é›†æœ¬æœºçš„ Nginx çš„ Status ä¿¡æ¯\n# å¯èƒ½ä¼šå¯¼è‡´æ—¶åºæ•°æ®æ ‡ç­¾ç›¸åŒï¼Œä¸æ˜“åŒºåˆ†ï¼Œå½“ç„¶ï¼ŒCategraf ä¼šè‡ªå¸¦ ident æ ‡ç­¾ï¼Œè¯¥æ ‡ç­¾æ ‡è¯†æœ¬æœºæœºå™¨å\n# å¦‚æœå¤§å®¶è§‰å¾— ident æ ‡ç­¾ä¸å¤Ÿç”¨ï¼Œå¯ä»¥ç”¨ä¸‹é¢ labels é…ç½®ï¼Œé™„åŠ  instanceã€region ä¹‹ç±»çš„æ ‡ç­¾\n\n# # append some labels for series\nlabels = { cloud=\"my-cloud\", region=\"my-region\",azone=\"az1\", product=\"my-product\" }\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n## Set http_proxy (categraf uses the system wide proxy settings if it\'s is not set)\n# http_proxy = \"http://localhost:8888\"\n\n## Interface to use when dialing an address\n# interface = \"eth0\"\n\n## HTTP Request Method\n# method = \"GET\"\n\n## Set timeout (default 5 seconds)\n# timeout = \"5s\"\n\n## Whether to follow redirects from the server (defaults to false)\n# follow_redirects = false\n\n## Optional HTTP Basic Auth Credentials\n# username = \"username\"\n# password = \"pa$$word\"\n\n## Optional headers\n# headers = [\"X-From\", \"categraf\", \"X-Xyz\", \"abc\"]\n\n## Optional TLS Config\n# use_tls = false\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = false', '# åº”ç”¨åœºæ™¯\nä¸€èˆ¬ç”¨äºä¸šåŠ¡ç³»ç»Ÿåšå¯¹å¤–æˆ–å¯¹å¤–è·¯ç”±æ˜ å°„æ—¶ä½¿ç”¨ä»£ç†æœåŠ¡ï¼Œæ˜¯è¿ç»´æœ€å¸¸è§ä¸”æœ€é‡è¦çš„ä»£ç†å·¥å…·ã€‚\n\n# éƒ¨ç½²åœºæ™¯\néœ€è¦åœ¨è£…æœ‰nginxæœåŠ¡çš„è™šæ‹Ÿæœºå¯ç”¨æ­¤æ’ä»¶ã€‚\n\n# é‡‡é›†åŸç†\n\n- è¯¥é‡‡é›†æ’ä»¶æ˜¯è¯»å– [nginx_upstream_check](https://github.com/yaoweibin/nginx_upstream_check_module) çš„çŠ¶æ€è¾“å‡ºã€‚[nginx_upstream_check](https://github.com/yaoweibin/nginx_upstream_check_module) å¯ä»¥å‘¨æœŸæ€§æ£€æŸ¥ upstream ä¸­çš„å„ä¸ª server æ˜¯å¦å­˜æ´»ï¼Œå¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œå°±ä¼šæ ‡è®°ä¸º `down`ï¼Œå¦‚æœæ£€æŸ¥æˆåŠŸï¼Œå°±æ ‡è®°ä¸º `up`ã€‚\n\n# æ³¨æ„äº‹é¡¹\n- ç”±äº TSDB é€šå¸¸æ— æ³•å¤„ç†å­—ç¬¦ä¸²ï¼Œæ‰€ä»¥ Categraf ä¼šåšè½¬æ¢ï¼Œå°† `down` è½¬æ¢ä¸º 2ï¼Œ `up` è½¬æ¢ä¸º 1ï¼Œå…¶ä»–çŠ¶æ€è½¬æ¢ä¸º 0ï¼Œä½¿ç”¨ `nginx_upstream_check_status_code` è¿™ä¸ªæŒ‡æ ‡æ¥è¡¨ç¤ºï¼Œæ‰€ä»¥ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦è¿™æ ·çš„å‘Šè­¦è§„åˆ™ï¼š\n\n# å‰ç½®æ¡ä»¶\n## æ¡ä»¶1ï¼šnginxæœåŠ¡éœ€è¦å¯ç”¨nginx_upstream_check_moduleæ¨¡å—\n```\næ¨èæºç ç¼–è¯‘æ–¹å¼å®‰è£…æ¨¡å—ï¼Œå¦‚ä¸æ¸…æ¥šè¦å®‰è£…å“ªäº›æ¨¡å—ï¼Œå¯å‚è€ƒï¼š\ncd /opt/nginx-1.20.1 && ./configure \\\n--prefix=/usr/share/nginx \\\n--sbin-path=/usr/sbin/nginx \\\n--modules-path=/usr/lib64/nginx/modules \\\n--conf-path=/etc/nginx/nginx.conf \\\n--error-log-path=/var/log/nginx/error.log \\\n--http-log-path=/var/log/nginx/access.log \\\n--http-client-body-temp-path=/var/lib/nginx/tmp/client_body \\\n--http-proxy-temp-path=/var/lib/nginx/tmp/proxy \\\n--http-fastcgi-temp-path=/var/lib/nginx/tmp/fastcgi \\\n--http-uwsgi-temp-path=/var/lib/nginx/tmp/uwsgi \\\n--http-scgi-temp-path=/var/lib/nginx/tmp/scgi \\\n--pid-path=/var/run/nginx.pid \\\n--lock-path=/run/lock/subsys/nginx \\\n--user=nginx \\\n--group=nginx \\\n--with-compat \\\n--with-threads \\\n--with-http_addition_module \\\n--with-http_auth_request_module \\\n--with-http_dav_module \\\n--with-http_flv_module \\\n--with-http_gunzip_module \\\n--with-http_gzip_static_module \\\n--with-http_mp4_module \\\n--with-http_random_index_module \\\n--with-http_realip_module \\\n--with-http_secure_link_module \\\n--with-http_slice_module \\\n--with-http_ssl_module \\\n--with-http_stub_status_module \\\n--with-http_sub_module \\\n--with-http_v2_module \\\n--with-mail \\\n--with-mail_ssl_module \\\n--with-stream \\\n--with-stream_realip_module \\\n--with-stream_ssl_module \\\n--with-stream_ssl_preread_module \\\n--with-select_module \\\n--with-poll_module \\\n--with-file-aio \\\n--with-http_xslt_module=dynamic \\\n--with-http_image_filter_module=dynamic \\\n--with-http_perl_module=dynamic \\\n--with-stream=dynamic \\\n--with-mail=dynamic \\\n--with-http_xslt_module=dynamic \\\n--add-module=/etc/nginx/third-modules/nginx_upstream_check_module \\\n--add-module=/etc/nginx/third-modules/ngx_devel_kit-0.3.0 \\\n--add-module=/etc/nginx/third-modules/lua-nginx-module-0.10.13 \\\n--add-module=/etc/nginx/third-modules/nginx-module-vts \\\n--add-module=/etc/nginx/third-modules/ngx-fancyindex-0.5.2\n\n# æ ¹æ®cpuæ ¸æ•°\nmake -j2\nmake install\n\næ³¨æ„ï¼šç¬¬ä¸‰æ–¹æ¨¡å—nginx_upstream_check_module lua-nginx-module nginx-module-vts éƒ½æ˜¯ç›¸å…³æ’ä»¶æ‰€å¿…å¤‡çš„ä¾èµ–ã€‚\n```\n\n## æ¡ä»¶2ï¼šnginxå¯ç”¨check_statusé…ç½®\n```\n[root@aliyun categraf]# cat /etc/nginx/conf.d/nginx-upstream.domains.com.conf\nserver {\n    listen 80;\n    listen 443 ssl;\n    server_name nginx-upstream.domains.com;\n    include /etc/nginx/ssl_conf/domains.com.conf;\n\n    location / {\n        check_status;\n        include /etc/nginx/ip_whitelist.conf;\n    }\n\n    access_log /var/log/nginx/nginx-upstream.domains.com.access.log main;\n    error_log /var/log/nginx/nginx-upstream.domains.com.error.log warn;\n}\n```\næµè§ˆå™¨è®¿é—®https://nginx-upstream.domains.com?format=jsonå‡ºç°ï¼š\n![image](https://user-images.githubusercontent.com/12181410/220912157-57f485de-6b4e-4ca4-869d-871244aabde1.png)\n\næµè§ˆå™¨è®¿é—®https://nginx-upstream.domains.comå‡ºç°ï¼š\n![image](https://user-images.githubusercontent.com/12181410/220909354-fc8ba53d-2384-41d3-8def-4447a104fb3c.png)\n\n## æ¡ä»¶3ï¼šåœ¨éœ€è¦å¯ç”¨upstreamç›‘æ§çš„åŸŸåé…ç½®ä¸‹è¿›è¡Œé…ç½®\nä¾‹å¦‚ï¼š\n```\n[root@aliyun upstream_conf]# cat upstream_n9e.conf\nupstream n9e {\n    server 127.0.0.1:18000 weight=10 max_fails=2 fail_timeout=5s;\n\n    check interval=3000 rise=2 fall=5 timeout=1000 type=tcp default_down=false port=18000;\n    check_http_send \"HEAD / HTTP/1.0\\r\\n\\r\\n\";\n    check_http_expect_alive http_2xx http_3xx;\n}\n\n[root@aliyun upstream_conf]# cat upstream_n9e_server_api.conf\nupstream n9e-server-api {\n    server 127.0.0.1:19000 weight=10 max_fails=2 fail_timeout=5s;\n\n    check interval=3000 rise=2 fall=5 timeout=1000 type=tcp default_down=false port=19000;\n    check_http_send \"HEAD / HTTP/1.0\\r\\n\\r\\n\";\n    check_http_expect_alive http_2xx http_3xx;\n}\n\n[root@aliyun upstream_conf]# cat upstream_vm.conf\nupstream vm {\n    server 127.0.0.1:8428 weight=10 max_fails=2 fail_timeout=5s;\n    keepalive 20;\n\n    check interval=3000 rise=2 fall=5 timeout=1000 type=tcp default_down=false port=8428;\n    check_http_send \"HEAD / HTTP/1.0\\r\\n\\r\\n\";\n    check_http_expect_alive http_2xx http_3xx;\n}\n\n```\n\n# é…ç½®åœºæ™¯\n```\næœ¬é…ç½®å¯ç”¨æˆ–æ•°æ®å®šä¹‰å¦‚ä¸‹åŠŸèƒ½ï¼š\nå¢åŠ è‡ªå®šä¹‰æ ‡ç­¾ï¼Œå¯é€šè¿‡è‡ªå®šä¹‰æ ‡ç­¾ç­›é€‰æ•°æ®åŠæ›´åŠ ç²¾ç¡®çš„å‘Šè­¦æ¨é€ã€‚\nå“åº”è¶…æ—¶æ—¶é—´ä¸º5ç§’ã€‚\nurlså­—æ®µå¡«å†™æ¡ä»¶2æ‰€å®šä¹‰å¥½çš„åŸŸåã€‚\n```\n\n# ä¿®æ”¹nginx.tomlæ–‡ä»¶é…ç½®\n```\n[root@aliyun conf]# cat input.nginx_upstream_check/nginx_upstream_check.toml\n\n# # collect interval\n# interval = 15\n\n[[instances]]\n# è¿™ä¸ªé…ç½®æœ€å…³é”®ï¼Œæ˜¯è¦ç»™å‡ºè·å– status ä¿¡æ¯çš„æ¥å£åœ°å€\ntargets = [\n    \"https://nginx-upstream.domains.com/?format=json\"\n]\n\n# æ ‡ç­¾è¿™ä¸ªé…ç½®è¯·æ³¨æ„\n# å¦‚æœ Categraf å’Œ Nginx æ˜¯åœ¨ä¸€å°æœºå™¨ä¸Šï¼Œtarget å¯èƒ½é…ç½®çš„æ˜¯ 127.0.0.1\n# å¦‚æœ Nginx æœ‰å¤šå°æœºå™¨ï¼Œæ¯å°æœºå™¨éƒ½æœ‰ Categraf æ¥é‡‡é›†æœ¬æœºçš„ Nginx çš„ Status ä¿¡æ¯\n# å¯èƒ½ä¼šå¯¼è‡´æ—¶åºæ•°æ®æ ‡ç­¾ç›¸åŒï¼Œä¸æ˜“åŒºåˆ†ï¼Œå½“ç„¶ï¼ŒCategraf ä¼šè‡ªå¸¦ ident æ ‡ç­¾ï¼Œè¯¥æ ‡ç­¾æ ‡è¯†æœ¬æœºæœºå™¨å\n# å¦‚æœå¤§å®¶è§‰å¾— ident æ ‡ç­¾ä¸å¤Ÿç”¨ï¼Œå¯ä»¥ç”¨ä¸‹é¢ labels é…ç½®ï¼Œé™„åŠ  instanceã€region ä¹‹ç±»çš„æ ‡ç­¾\n\n# # append some labels for series\nlabels = { cloud=\"my-cloud\", region=\"my-region\",azone=\"az1\", product=\"my-product\" }\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n## Set http_proxy (categraf uses the system wide proxy settings if it\'s is not set)\n# http_proxy = \"http://localhost:8888\"\n\n## Interface to use when dialing an address\n# interface = \"eth0\"\n\n## HTTP Request Method\n# method = \"GET\"\n\n## Set timeout (default 5 seconds)\n# timeout = \"5s\"\n\n## Whether to follow redirects from the server (defaults to false)\n# follow_redirects = false\n\n## Optional HTTP Basic Auth Credentials\n# username = \"username\"\n# password = \"pa$$word\"\n\n## Optional headers\n# headers = [\"X-From\", \"categraf\", \"X-Xyz\", \"abc\"]\n\n## Optional TLS Config\n# use_tls = false\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = false\n```\n\n# æµ‹è¯•é…ç½®\n```\n./categraf --test --inputs nginx_upstream_check\n\n```\n# é‡å¯æœåŠ¡\n```\né‡å¯categrafæœåŠ¡ç”Ÿæ•ˆ\nsystemctl daemon-reload && systemctl restart categraf && systemctl status categraf\n\næŸ¥çœ‹å¯åŠ¨æ—¥å¿—æ˜¯å¦æœ‰é”™è¯¯\njournalctl -f -n 500 -u categraf | grep \"E\\!\" | grep \"W\\!\"\n```\n\n# æ£€æŸ¥æ•°æ®å‘ˆç°\nç­‰å¾…1-2åˆ†é’Ÿåæ•°æ®å°±ä¼šåœ¨å›¾è¡¨ä¸­å±•ç¤ºå‡ºæ¥ï¼Œå¦‚å›¾ï¼š\n![image](https://user-images.githubusercontent.com/12181410/220914337-f97f6fd5-4763-4174-b64c-131aecf6664f.png)\n\n\n# ç›‘æ§å‘Šè­¦è§„åˆ™é…ç½®\n```\nä¸€èˆ¬æŸ¥çœ‹åç«¯æ˜¯å¦å¼‚å¸¸ä¸ºå…³é”®æ£€æŸ¥å¯¹è±¡ï¼Œnginx_upstream_check_status_codeè¿”å›1ä»£è¡¨æ­£å¸¸ï¼Œè¿”å›2ä»£è¡¨å¼‚å¸¸ï¼ˆå®é™…æµ‹è¯•å¯ä»ä¸Šå›¾çœ‹å‡ºï¼‰ã€‚\nnginx_upstream_check_status_code!=1åˆ™è§†ä¸ºå¼‚å¸¸éœ€ç«‹å³å‘Šè­¦ï¼Œçº§åˆ«ä¸ºä¸€çº§å‘Šè­¦ï¼Œæ‰§è¡Œé¢‘ç‡ä¸º60ç§’ï¼ŒæŒç»­æ—¶é•¿ä¸º60ç§’ï¼Œç•™è§‚æ—¶é•¿2åˆ†é’Ÿï¼Œé‡å¤å‘é€é¢‘ç‡5åˆ†é’Ÿï¼Œæœ€å¤§å‘é€æ¬¡æ•°0æ¬¡ï¼Œä½¿ç”¨ä¼ä¸šå¾®ä¿¡åº”ç”¨åŠç”µè¯è¯­éŸ³é€šé“å°†å‘Šè­¦å†…å®¹å‘é€ç»™ç³»ç»Ÿè¿ç»´ç»„ï¼Œæ­¤è§„åˆ™è¿ç”¨åˆ°å‘¨ä¸€åˆ°å‘¨æ—¥å…¨å¤©ã€‚\n```\n\n# ç›‘æ§å›¾è¡¨é…ç½®\nhttps://github.com/flashcatcloud/categraf/blob/main/inputs/nginx_upstream_check/dashboards.json\n\n# æ•…éšœè‡ªæ„ˆé…ç½®\n```\nå…ˆç•¥è¿‡\n```', 'é‡‡é›†nginx_upstream_checkçš„çŠ¶æ€è¾“å‡º', '1732762648', 'ç‹æ¨(822032277)', '1732762923', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('80', 'ElasticSearch', '# # collect interval\n# interval = 15\n\n############################################################################\n# !!! uncomment [[instances]] to enable this plugin\n[[instances]]\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# append some labels to metrics\n# labels = { cluster=\"cloud-n9e-es\" }\n\n## specify a list of one or more Elasticsearch servers\n# servers = [\"http://localhost:9200\"]\nservers = []\n\n## Timeout for HTTP requests to the elastic search server(s)\nhttp_timeout = \"10s\"\n\n# either /_nodes/stats or /_nodes/_local/stats depending on this setting\nlocal = false\n\n## Set cluster_health to true when you want to obtain cluster health stats\ncluster_health = true\n\n## Adjust cluster_health_level when you want to obtain detailed health stats\n## The options are\n##  - indices (default)\n##  - cluster\ncluster_health_level = \"cluster\"\n\n## Set cluster_stats to true when you want to obtain cluster stats.\ncluster_stats = true\n\n## Indices to collect; can be one or more indices names or _all\n## Use of wildcards is allowed. Use a wildcard at the end to retrieve index names that end with a changing value, like a date.\n# indices_include = [\"zipkin*\"]\n\n## use \"shards\" or blank string for indices level\nindices_level = \"\"\n\n## node_stats is a list of sub-stats that you want to have gathered. Valid options\n## are \"indices\", \"os\", \"process\", \"jvm\", \"thread_pool\", \"fs\", \"transport\", \"http\",\n## \"breaker\". Per default, all stats are gathered.\nnode_stats = [\"jvm\", \"breaker\", \"process\", \"os\", \"fs\", \"indices\", \"thread_pool\", \"transport\"]\n\n## HTTP Basic Authentication username and password.\nusername = \"elastic\"\npassword = \"password\"\n\n## Optional TLS Config\n# use_tls = false\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = true\n\n## Sets the number of most recent indices to return for indices that are configured with a date-stamped suffix.\n## Each \'indices_include\' entry ending with a wildcard (*) or glob matching pattern will group together all indices that match it, and \n## sort them by the date or number after the wildcard. Metrics then are gathered for only the \'num_most_recent_indices\' amount of most \n## recent indices.\nnum_most_recent_indices = 1', '# elasticsearch plugin\n\nElasticSearch é€šè¿‡ HTTP JSON çš„æ–¹å¼æš´éœ²äº†è‡ªèº«çš„ç›‘æ§æŒ‡æ ‡ï¼Œé€šè¿‡ categraf çš„ [elasticsearch](https://github.com/flashcatcloud/categraf/tree/main/inputs/elasticsearch) æ’ä»¶æŠ“å–ã€‚\n\nå¦‚æœæ˜¯å°è§„æ¨¡é›†ç¾¤ï¼Œè®¾ç½® `local=false`ï¼Œä»é›†ç¾¤ä¸­æŸä¸€ä¸ªèŠ‚ç‚¹æŠ“å–æ•°æ®ï¼Œå³å¯æ‹¿åˆ°æ•´ä¸ªé›†ç¾¤æ‰€æœ‰èŠ‚ç‚¹çš„ç›‘æ§æ•°æ®ã€‚å¦‚æœæ˜¯å¤§è§„æ¨¡é›†ç¾¤ï¼Œå»ºè®®è®¾ç½® `local=true`ï¼Œåœ¨é›†ç¾¤çš„æ¯ä¸ªèŠ‚ç‚¹ä¸Šéƒ½éƒ¨ç½²æŠ“å–å™¨ï¼ŒæŠ“å–æœ¬åœ° elasticsearch è¿›ç¨‹çš„ç›‘æ§æ•°æ®ã€‚\n\n\n## é…ç½®ç¤ºä¾‹\n\ncategraf é…ç½®æ–‡ä»¶ï¼š`conf/input.elasticsearch/elasticsearch.toml`\n\n```yaml\n[[instances]]\nservers = [\"http://192.168.11.177:9200\"]\nhttp_timeout = \"10s\"\nlocal = false\ncluster_health = true\ncluster_health_level = \"cluster\"\ncluster_stats = true\nindices_level = \"\"\nnode_stats = [\"jvm\", \"breaker\", \"process\", \"os\", \"fs\", \"indices\", \"thread_pool\", \"transport\"]\nusername = \"elastic\"\npassword = \"xxxxxxxx\"\nnum_most_recent_indices = 1\nlabels = { service=\"es\" }\n```\n\n## ä»ªè¡¨ç›˜æ•ˆæœ\n\nå¤œèºå†…ç½®ä»ªè¡¨ç›˜ä¸­å·²ç»å†…ç½®äº† Elasticsearch çš„ä»ªè¡¨ç›˜ï¼Œå¯¼å…¥å³å¯ä½¿ç”¨ã€‚', 'elasticsearchç›‘æ§æŒ‡æ ‡é‡‡é›†', '1732762849', 'ç‹æ¨(822032277)', '1732762849', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('81', 'HAProxy', '[[instances]]\n# URI on which to scrape HAProxy.\n# e.g. \n# uri = \"http://localhost:5000/baz?stats;csv\"\n# uri = \"http://user:pass@haproxy.example.com/haproxy?stats;csv\"\n# uri = \"unix:/run/haproxy/admin.sock\"\nuri = \"\"\n\n# Flag that enables SSL certificate verification for the scrape URI\nssl_verify = false\n\n# Comma-separated list of exported server metrics. See http://cbonte.github.io/haproxy-dconv/configuration-1.5.html#9.1\nserver_metric_fields = \"\"\n\n# Comma-separated list of exported server states to exclude. See https://cbonte.github.io/haproxy-dconv/1.8/management.html#9.1, field 17 status\nserver_exclude_states = \"\"\n\n# Timeout for trying to get stats from HAProxy.\ntimeout = \"5s\"\n\n# Flag that enables using HTTP proxy settings from environment variables ($http_proxy, $https_proxy, $no_proxy)\nproxy_from_env = false', '# HAProxy\n\nforked from [haproxy_exporter](https://github.com/prometheus/haproxy_exporter)\n\nNote: since HAProxy 2.0.0, the official source includes a Prometheus exporter module that can be built into your binary with a single flag during build time and offers an exporter-free Prometheus endpoint.\n\n\nhaproxy configurations for `/stats`:\n\n```\nfrontend stats\n    bind *:8404\n    stats enable\n    stats uri /stats\n    stats refresh 10s\n```\n', 'haproxyç›‘æ§æŒ‡æ ‡é‡‡é›†', '1732762902', 'ç‹æ¨(822032277)', '1732762902', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('82', 'PostgreSQL', '# Read metrics from one or many postgresql servers\n# # collect interval\n# interval = 15\n\n[[instances]]\n  ## specify address via a url matching:\n  ##   postgres://[pqgotest[:password]]@localhost[/dbname]?sslmode=[disable|verify-ca|verify-full]\n  ## or a simple string:\n  ##   host=localhost user=pqgotest password=... sslmode=... dbname=app_production\n  ##\n  ## All connection parameters are optional.\n  ##\n  ## Without the dbname parameter, the driver will default to a database\n  ## with the same name as the user. This dbname is just for instantiating a\n  ## connection with the server and doesn\'t restrict the databases we are trying\n  ## to grab metrics for.\n  ##\n  # address = \"host=localhost user=postgres sslmode=disable\"\n\n  ## A custom name for the database that will be used as the \"server\" tag in the\n  ## measurement output. If not specified, a default one generated from\n  ## the connection address is used.\n  # outputaddress = \"db01\"\n\n  ## connection configuration.\n  ## maxlifetime - specify the maximum lifetime of a connection.\n  ## default is forever (0s)\n  # max_lifetime = \"0s\"\n\n  ## A  list of databases to explicitly ignore.  If not specified, metrics for all\n  ## databases are gathered.  Do NOT use with the \'databases\' option.\n  # ignored_databases = [\"postgres\", \"template0\", \"template1\"]\n\n  ## A list of databases to pull metrics about. If not specified, metrics for all\n  ## databases are gathered.  Do NOT use with the \'ignored_databases\' option.\n  # databases = [\"app_production\", \"testing\"]\n\n  ## Whether to use prepared statements when connecting to the database.\n  ## This should be set to false when connecting through a PgBouncer instance\n  ## with pool_mode set to transaction.\n  #prepared_statements = true\n  # [[instances.metrics]]\n  # mesurement = \"sessions\"\n  # label_fields = [ \"status\", \"type\" ]\n  # metric_fields = [ \"value\" ]\n  # timeout = \"3s\"\n  # request = \'\'\'\n  # SELECT status, type, COUNT(*) as value FROM v$session GROUP BY status, type\n  # \'\'\'\n', '# PostgreSQL\n\ncategraf ä½œä¸ºä¸€ä¸ª client è¿ä¸Š pgï¼Œé‡‡é›†ç›¸å…³æŒ‡æ ‡ï¼Œé¦–å…ˆè¦ç¡®ä¿ç”¨æˆ·æˆæƒã€‚ä¸¾ä¾‹ï¼š\n\n```\ncreate user categraf with password \'categraf\';\nalter user categraf set default_transaction_read_only=on;\ngrant usage on schema public to categraf;\ngrant select on all tables in schema public to categraf ;\n```\n\n## é…ç½®æ–‡ä»¶ç¤ºä¾‹\n\n```toml\n[[instances]]\naddress = \"host=192.168.11.181 port=5432 user=postgres password=123456789 sslmode=disable\"\n## specify address via a url matching:\n##   postgres://[pqgotest[:password]]@localhost[/dbname]?sslmode=[disable|verify-ca|verify-full]\n## or a simple string:\n##   host=localhost user=pqgotest password=... sslmode=... dbname=app_production\n##\n## All connection parameters are optional.\n##\n## Without the dbname parameter, the driver will default to a database\n## with the same name as the user. This dbname is just for instantiating a\n## connection with the server and doesn\'t restrict the databases we are trying\n## to grab metrics for.\n##\n# address = \"host=localhost user=postgres sslmode=disable\"\n\n## A custom name for the database that will be used as the \"server\" tag in the\n## measurement output. If not specified, a default one generated from\n## the connection address is used.\n# outputaddress = \"db01\"\n\n## connection configuration.\n## maxlifetime - specify the maximum lifetime of a connection.\n## default is forever (0s)\n# max_lifetime = \"0s\"\n\n## A  list of databases to explicitly ignore.  If not specified, metrics for all\n## databases are gathered.  Do NOT use with the \'databases\' option.\n# ignored_databases = [\"postgres\", \"template0\", \"template1\"]\n\n## A list of databases to pull metrics about. If not specified, metrics for all\n## databases are gathered.  Do NOT use with the \'ignored_databases\' option.\n# databases = [\"app_production\", \"testing\"]\n\n## Whether to use prepared statements when connecting to the database.\n## This should be set to false when connecting through a PgBouncer instance\n## with pool_mode set to transaction.\n# prepared_statements = true\n#\n# [[instances.metrics]]\n# mesurement = \"sessions\"\n# label_fields = [ \"status\", \"type\" ]\n# metric_fields = [ \"value\" ]\n# timeout = \"3s\"\n# request = \'\'\'\n# SELECT status, type, COUNT(*) as value FROM v$session GROUP BY status, type\n# \'\'\'\n```\n\n## ä»ªè¡¨ç›˜\n\nå¤œèºå†…ç½®äº† Postgres çš„ä»ªè¡¨ç›˜ï¼Œå…‹éš†åˆ°è‡ªå·±çš„ä¸šåŠ¡ç»„ä¸‹å³å¯ä½¿ç”¨ã€‚\n\n![20230802073729](https://download.flashcat.cloud/ulric/20230802073729.png)\n\n## å‘Šè­¦è§„åˆ™\n\nå¤œèºå†…ç½®äº† Postgres çš„å‘Šè­¦è§„åˆ™ï¼Œå…‹éš†åˆ°è‡ªå·±çš„ä¸šåŠ¡ç»„ä¸‹å³å¯ä½¿ç”¨ã€‚\n\n![20230802073753](https://download.flashcat.cloud/ulric/20230802073753.png)', 'PostgreSQLç›‘æ§æŒ‡æ ‡é‡‡é›†', '1732763069', 'ç‹æ¨(822032277)', '1732763069', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('83', 'SMART', '# Read metrics from storage devices supporting S.M.A.R.T.\n[[instances]]\n    ## Optionally specify the path to the smartctl executable\n    # path_smartctl = \"/usr/bin/smartctl\"\n\n    ## Optionally specify the path to the nvme-cli executable\n    # path_nvme = \"/usr/bin/nvme\"\n\n    ## Optionally specify if vendor specific attributes should be propagated for NVMe disk case\n    ## [\"auto-on\"] - automatically find and enable additional vendor specific disk info\n    ## [\"vendor1\", \"vendor2\", ...] - e.g. \"Intel\" enable additional Intel specific disk info\n    #  enable_extensions = [\"auto-on\"]\n\n    ## On most platforms used cli utilities requires root access.\n    ## Setting \'use_sudo\' to true will make use of sudo to run smartctl or nvme-cli.\n    ## Sudo must be configured to allow the telegraf user to run smartctl or nvme-cli\n    ## without a password.\n    # use_sudo = true\n\n    ## Skip checking disks in this power mode. Defaults to\n    ## \"standby\" to not wake up disks that have stopped rotating.\n    ## See --nocheck in the man pages for smartctl.\n    ## smartctl version 5.41 and 5.42 have faulty detection of\n    ## power mode and might require changing this value to\n    ## \"never\" depending on your disks.\n    # nocheck = \"standby\"\n\n    ## Gather all returned S.M.A.R.T. attribute metrics and the detailed\n    ## information from each drive into the \'smart_attribute\' measurement.\n    # attributes = true\n\n    ## Optionally specify devices to exclude from reporting if disks auto-discovery is performed.\n    # excludes = [ \"/dev/pass6\" ]\n\n    ## Optionally specify devices and device type, if unset\n    ## a scan (smartctl --scan and smartctl --scan -d nvme) for S.M.A.R.T. devices will be done\n    ## and all found will be included except for the excluded in excludes.\n    # devices = [ \"/dev/ada0 -d atacam\", \"/dev/nvme0\"]\n    # devices = [\"dev/nvme0 -d nvme\", \"/dev/nvme0\"]\n\n    ## Timeout for the cli command to complete.\n      timeout = \"30s\"\n\n    ## Optionally call smartctl and nvme-cli with a specific concurrency policy.\n    ## By default, smartctl and nvme-cli are called in separate threads (goroutines) to gather disk attributes.\n    ## Some devices (e.g. disks in RAID arrays) may have access limitations that require sequential reading of\n    ## SMART data - one individual array drive at the time. In such case please set this configuration option\n    ## to \"sequential\" to get readings for all drives.\n    ## valid options: concurrent, sequential\n    # read_method = \"concurrent\"\n', '# S.M.A.R.T. æ’ä»¶\n\nä»[telegraf](https://github.com/influxdata/telegraf/blob/master/plugins/inputs/smart/README.md) forkï¼Œç•¥ä½œæ”¹åŠ¨\n\nGet metrics using the command line utility `smartctl` for\nS.M.A.R.T. (Self-Monitoring, Analysis and Reporting Technology) storage\ndevices. SMART is a monitoring system included in computer hard disk drives\n(HDDs) and solid-state drives (SSDs) that detects and reports on various\nindicators of drive reliability, with the intent of enabling the anticipation of\nhardware failures.  See smartmontools (<https://www.smartmontools.org/>).\n\nSMART information is separated between different measurements: `smart_device` is\nused for general information, while `smart_attribute` stores the detailed\nattribute information if `attributes = true` is enabled in the plugin\nconfiguration.\n\nIf no devices are specified, the plugin will scan for SMART devices via the\nfollowing command:\n\n```sh\nsmartctl --scan\n```\n\nMetrics will be reported from the following `smartctl` command:\n\n```sh\nsmartctl --info --attributes --health -n <nocheck> --format=brief <device>\n```\n\nThis plugin supports _smartmontools_ version 5.41 and above, but v. 5.41 and\nv. 5.42 might require setting `nocheck`, see the comment in the sample\nconfiguration.  Also, NVMe capabilities were introduced in version 6.5.\n\nTo enable SMART on a storage device run:\n\n```sh\nsmartctl -s on <device>\n```\n\n## NVMe vendor specific attributes\n\nFor NVMe disk type, plugin can use command line utility `nvme-cli`. It has a\nfeature to easy access a vendor specific attributes.  This plugin supports\nnmve-cli version 1.5 and above (<https://github.com/linux-nvme/nvme-cli>).  In\ncase of `nvme-cli` absence NVMe vendor specific metrics will not be obtained.\n\nVendor specific SMART metrics for NVMe disks may be reported from the following\n`nvme` command:\n\n```sh\nnvme <vendor> smart-log-add <device>\n```\n\nNote that vendor plugins for `nvme-cli` could require different naming\nconvention and report format.\n\nTo see installed plugin extensions, depended on the nvme-cli version, look at\nthe bottom of:\n\n```sh\nnvme help\n```\n\nTo gather disk vendor id (vid) `id-ctrl` could be used:\n\n```sh\nnvme id-ctrl <device>\n```\n\nAssociation between a vid and company can be found there:\n<https://pcisig.com/membership/member-companies>.\n\nDevices affiliation to being NVMe or non NVMe will be determined thanks to:\n\n```sh\nsmartctl --scan\n```\n\nand:\n\n```sh\nsmartctl --scan -d nvme\n```\n\n\n## Configuration\n\n```toml @ç¤ºä¾‹\n# Read metrics from storage devices supporting S.M.A.R.T.\n[[instances]]\n## Optionally specify the path to the smartctl executable\n# path_smartctl = \"/usr/bin/smartctl\"\n\n## Optionally specify the path to the nvme-cli executable\n# path_nvme = \"/usr/bin/nvme\"\n\n## Optionally specify if vendor specific attributes should be propagated for NVMe disk case\n## [\"auto-on\"] - automatically find and enable additional vendor specific disk info\n## [\"vendor1\", \"vendor2\", ...] - e.g. \"Intel\" enable additional Intel specific disk info\n# enable_extensions = [\"auto-on\"]\n\n## On most platforms used cli utilities requires root access.\n## Setting \'use_sudo\' to true will make use of sudo to run smartctl or nvme-cli.\n## Sudo must be configured to allow the categraf user to run smartctl or nvme-cli\n## Sudo must be configured to allow the categraf user to run smartctl or nvme-cli\n## without a password.\nuse_sudo = true\n\n## Skip checking disks in this power mode. Defaults to\n## \"standby\" to not wake up disks that have stopped rotating.\n## See --nocheck in the man pages for smartctl.\n## smartctl version 5.41 and 5.42 have faulty detection of\n## power mode and might require changing this value to\n## \"never\" depending on your disks.\n# nocheck = \"standby\"\n\n## Gather all returned S.M.A.R.T. attribute metrics and the detailed\n## information from each drive into the \'smart_attribute\' measurement.\nattributes = true\n\n## Optionally specify devices to exclude from reporting if disks auto-discovery is performed.\n# excludes = [ \"/dev/pass6\" ]\n\n## Optionally specify devices and device type, if unset\n## a scan (smartctl --scan and smartctl --scan -d nvme) for S.M.A.R.T. devices will be done\n## and all found will be included except for the excluded in excludes.\n# devices = [ \"/dev/ada0 -d atacam\", \"/dev/nvme0\"]\n# devices = [\"dev/nvme0 -d nvme\", \"/dev/nvme0\"]\n\n## Timeout for the cli command to complete.\ntimeout = \"30s\"\n\n## Optionally call smartctl and nvme-cli with a specific concurrency policy.\n## By default, smartctl and nvme-cli are called in separate threads (goroutines) to gather disk attributes.\n## Some devices (e.g. disks in RAID arrays) may have access limitations that require sequential reading of\n## SMART data - one individual array drive at the time. In such case please set this configuration option\n## to \"sequential\" to get readings for all drives.\n## valid options: concurrent, sequential\n# read_method = \"concurrent\"\n```\n\n## Permissions\né‡‡é›†éœ€è¦sudoæƒé™\n\n## Metrics\n\n- smart_device:\n  - tags:\n    - capacity\n    - device\n    - enabled\n    - model\n    - serial_no\n    - wwn\n  - fields:\n    - exit_status\n    - health_ok\n    - media_wearout_indicator\n    - percent_lifetime_remain\n    - read_error_rate\n    - seek_error\n    - temp_c\n    - udma_crc_errors\n    - wear_leveling_count\n\n- smart_attribute:\n  - tags:\n    - capacity\n    - device\n    - enabled\n    - fail\n    - flags\n    - id\n    - model\n    - name\n    - serial_no\n    - wwn\n  - fields:\n    - exit_status\n    - threshold\n    - value\n    - worst\n    - critical_warning\n    - temperature_celsius\n    - available_spare\n    - available_spare_threshold\n    - percentage_used\n    - data_units_read\n    - data_units_written\n    - host_read_commands\n    - host_write_commands\n    - controller_busy_time\n    - power_cycle_count\n    - power_on_hours\n    - unsafe_shutdowns\n    - media_and_data_integrity_errors\n    - error_information_log_entries\n    - warning_temperature_time\n    - critical_temperature_time\n    - program_fail_count\n    - erase_fail_count\n    - wear_leveling_count\n    - end_to_end_error_detection_count\n    - crc_error_count\n    - media_wear_percentage\n    - host_reads\n    - timed_workload_timer\n    - thermal_throttle_status\n    - retry_buffer_overflow_count\n    - pll_lock_loss_count\n\n### Flags\n\nThe interpretation of the tag `flags` is:\n\n- `K` auto-keep\n- `C` event count\n- `R` error rate\n- `S` speed/performance\n- `O` updated online\n- `P` prefailure warning\n\n### Exit Status\n\nThe `exit_status` field captures the exit status of the used cli utilities\ncommand which is defined by a bitmask. For the interpretation of the bitmask see\nthe man page for smartctl or nvme-cli.\n\n## Device Names\n\nDevice names, e.g., `/dev/sda`, are _not persistent_, and may be\nsubject to change across reboots or system changes. Instead, you can use the\n_World Wide Name_ (WWN) or serial number to identify devices. On Linux block\ndevices can be referenced by the WWN in the following location:\n`/dev/disk/by-id/`.\n\n## Troubleshooting\n\nIf you expect to see more SMART metrics than this plugin shows, be sure to use a\nproper version of smartctl or nvme-cli utility which has the functionality to\ngather desired data. Also, check your device capability because not every SMART\nmetrics are mandatory. For example the number of temperature sensors depends on\nthe device specification.\n\nIf this plugin is not working as expected for your SMART enabled device,\nplease run these commands and include the output in a bug report:\n\nFor non NVMe devices (from smartctl version >= 7.0 this will also return NVMe\ndevices by default):\n\n```sh\nsmartctl --scan\n```\n\nFor NVMe devices:\n\n```sh\nsmartctl --scan -d nvme\n```\n\nRun the following command replacing your configuration setting for NOCHECK and\nthe DEVICE (name of the device could be taken from the previous command):\n\n```sh\nsmartctl --info --health --attributes --tolerance=verypermissive --nocheck NOCHECK --format=brief -d DEVICE\n```\n\nIf you try to gather vendor specific metrics, please provide this command\nand replace vendor and device to match your case:\n\n```sh\nnvme VENDOR smart-log-add DEVICE\n```\n\nIf you have specified devices array in configuration file, and categraf only\nshows data from one device, you should change the plugin configuration to\nsequentially gather disk attributes instead of collecting it in separate threads\n(goroutines). To do this find in plugin configuration read_method and change it\nto sequential:\n\n```toml\n    ## Optionally call smartctl and nvme-cli with a specific concurrency policy.\n    ## By default, smartctl and nvme-cli are called in separate threads (goroutines) to gather disk attributes.\n    ## Some devices (e.g. disks in RAID arrays) may have access limitations that require sequential reading of\n    ## SMART data - one individual array drive at the time. In such case please set this configuration option\n    ## to \"sequential\" to get readings for all drives.\n    ## valid options: concurrent, sequential\n    read_method = \"sequential\"\n```\n\n## Example Output\n\n```text\nsmart_device_health_ok agent_hostname=1.2.3.4 device=nvme0 model=INTEL_SSDPE2KX040T8 serial_no=PHLJ830200CH4P0DGN 1\nsmart_device_temp_c agent_hostname=1.2.3.4 device=nvme0 model=INTEL_SSDPE2KX040T8 serial_no=PHLJ830200CH4P0DGN 53\nsmart_attribute_program_fail_count agent_hostname=1.2.3.4 device=nvme0 model= name=Program_Fail_Count serial_no=PHLJ830200CH4P0DGN 0\nsmart_attribute_erase_fail_count agent_hostname=1.2.3.4 device=nvme0 model= name=Erase_Fail_Count serial_no=PHLJ830200CH4P0DGN 0\nsmart_attribute_wear_leveling_count agent_hostname=1.2.3.4 device=nvme0 model= name=Wear_Leveling_Count serial_no=PHLJ830200CH4P0DGN 34360328200\n```', 'SMARTæ’ä»¶', '1732763178', 'ç‹æ¨(822032277)', '1732763178', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('84', 'HTTP_Response', '## collect interval\n# interval = 15\n\n## Set the mapping of extra tags in batches\n[mappings]\n# \"http://localhost\" = { \"job\" = \"local\" }\n# \"https://www.baidu.com\" = { \"job\" = \"baidu\" }\n\n[[instances]]\ntargets = [\n#     \"http://localhost\",\n#     \"https://www.baidu.com\"\n]\n\n## append some labels for series\n# labels = { region=\"cloud\", product=\"n9e\" }\n\n## interval = global.interval * interval_times\n# interval_times = 1\n\n## Set http_proxy (categraf uses the system wide proxy settings if it\'s is not set)\n# http_proxy = \"http://localhost:8888\"\n\n## Interface to use when dialing an address\n# interface = \"eth0\"\n\n## HTTP Request Method\n# method = \"GET\"\n\n## Set response_timeout (default 5 seconds)\n# response_timeout = \"5s\"\n\n## Whether to follow redirects from the server (defaults to false)\n# follow_redirects = false\n\n## Optional HTTP Basic Auth Credentials\n# username = \"username\"\n# password = \"pa$$word\"\n\n## Optional headers\n# headers = [\"Header-Key-1\", \"Header-Value-1\", \"Header-Key-2\", \"Header-Value-2\"]\n\n## Optional HTTP Request Body\n# body = \'\'\'\n# {\'fake\':\'data\'}\n# \'\'\'\n\n## Optional substring or regular expression match in body of the response(substring case sensitive).\n## When both of the following parameters are enabled, one of them can be satisfied.\n# expect_response_substring = \"ok\"\n# expect_response_regular_expression = \"green|yellow\"\n\n## Optional expected response status codes.\n## \"expect_response_status_codes\" Supports adding multiple codes by delimiter(\"|\" or \",\").\n## When both of the following parameters are enabled, one of them can be satisfied.\n# expect_response_status_code = 0\n# expect_response_status_codes = \"200|301\"\n\n## Optional TLS Config\n# use_tls = false\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = false\n', '# http_response plugin\n\nHTTP æ¢æµ‹æ’ä»¶ï¼Œç”¨äºæ£€æµ‹ HTTP åœ°å€çš„è¿é€šæ€§ã€å»¶è¿Ÿã€HTTPS è¯ä¹¦è¿‡æœŸæ—¶é—´ã€‚å› ä¸º Prometheus ç”Ÿæ€çš„æ—¶åºåº“åªèƒ½å­˜å‚¨ float64 ç±»å‹çš„å€¼ï¼Œæ‰€ä»¥ HTTP åœ°å€æ¢æµ‹çš„ç»“æœä¹Ÿæ˜¯ float64 ç±»å‹çš„å€¼ï¼Œä½†æ˜¯è¿™ä¸ªå€¼çš„å«ä¹‰æ˜¯ä¸åŒçš„ï¼Œå…·ä½“å«ä¹‰å¦‚ä¸‹ï¼š\n\n```\nSuccess          = 0\nConnectionFailed = 1\nTimeout          = 2\nDNSError         = 3\nAddressError     = 4\nBodyMismatch     = 5\nCodeMismatch     = 6\n```\n\nå¦‚æœä¸€åˆ‡æ­£å¸¸ï¼Œè¿™ä¸ªå€¼æ˜¯ 0ï¼Œå¦‚æœæœ‰å¼‚å¸¸ï¼Œè¿™ä¸ªå€¼æ˜¯ 1-6 ä¹‹é—´çš„å€¼ï¼Œå…·ä½“å«ä¹‰å¦‚ä¸Šã€‚è¿™ä¸ªå€¼å¯¹åº”çš„æŒ‡æ ‡åå­—æ˜¯ `http_response_result_code`ã€‚\n\n## Configuration\n\ncategraf çš„ `conf/input.http_response/http_response.toml`ã€‚æœ€æ ¸å¿ƒçš„é…ç½®å°±æ˜¯ targets é…ç½®ï¼Œé…ç½®ç›®æ ‡åœ°å€ï¼Œæ¯”å¦‚æƒ³è¦ç›‘æ§ä¸¤ä¸ªåœ°å€ï¼š\n\n```toml\n[[instances]]\ntargets = [\n    \"http://localhost:8080\",\n    \"https://www.baidu.com\"\n]\n```\n\ninstances ä¸‹é¢çš„æ‰€æœ‰ targets å…±äº«åŒä¸€ä¸ª `[[instances]]` ä¸‹é¢çš„é…ç½®ï¼Œæ¯”å¦‚è¶…æ—¶æ—¶é—´ï¼ŒHTTPæ–¹æ³•ç­‰ï¼Œå¦‚æœæœ‰äº›é…ç½®ä¸åŒï¼Œå¯ä»¥æ‹†æˆå¤šä¸ªä¸åŒçš„ `[[instances]]`ï¼Œæ¯”å¦‚ï¼š\n\n```toml\n[[instances]]\ntargets = [\n    \"http://localhost:8080\",\n    \"https://www.baidu.com\"\n]\nmethod = \"GET\"\n\n[[instances]]\ntargets = [\n    \"http://localhost:9090\"\n]\nmethod = \"POST\"\n```\n\nå®Œæ•´çš„å¸¦æœ‰æ³¨é‡Šçš„é…ç½®å¦‚ä¸‹ï¼š\n\n```toml\n[[instances]]\ntargets = [\n#     \"http://localhost\",\n#     \"https://www.baidu.com\"\n]\n\n# # append some labels for series\n# labels = { region=\"cloud\", product=\"n9e\" }\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n## Set http_proxy (categraf uses the system wide proxy settings if it\'s is not set)\n# http_proxy = \"http://localhost:8888\"\n\n## Interface to use when dialing an address\n# interface = \"eth0\"\n\n## HTTP Request Method\n# method = \"GET\"\n\n## Set response_timeout (default 5 seconds)\n# response_timeout = \"5s\"\n\n## Whether to follow redirects from the server (defaults to false)\n# follow_redirects = false\n\n## Optional HTTP Basic Auth Credentials\n# username = \"username\"\n# password = \"pa$$word\"\n\n## Optional headers\n# headers = [\"Header-Key-1\", \"Header-Value-1\", \"Header-Key-2\", \"Header-Value-2\"]\n\n## Optional HTTP Request Body\n# body = \'\'\'\n# {\'fake\':\'data\'}\n# \'\'\'\n\n## Optional substring match in body of the response (case sensitive)\n# expect_response_substring = \"ok\"\n\n## Optional expected response status code.\n# expect_response_status_code = 0\n\n## Optional TLS Config\n# use_tls = false\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = false\n```\n\n## dashboard and monitors\n\nå¤œèºæä¾›äº†å†…ç½®å¤§ç›˜å’Œå†…ç½®å‘Šè­¦è§„åˆ™ï¼Œå…‹éš†åˆ°è‡ªå·±çš„ä¸šåŠ¡ç»„ä¸‹å³å¯ä½¿ç”¨ã€‚', 'HTTP æ¢æµ‹æ’ä»¶', '1732763313', 'ç‹æ¨(822032277)', '1732763313', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('85', 'Net_Response', '# # collect interval\n# interval = 15\n\n[mappings]\n# \"127.0.0.1:22\"= {region=\"local\",ssh=\"test\"}\n# \"127.0.0.1:22\"= {region=\"local\",ssh=\"redis\"}\n\n[[instances]]\ntargets = [\n#     \"127.0.0.1:22\",\n#     \"localhost:6379\",\n#     \":9090\"\n]\n\n# # append some labels for series\n# labels = { region=\"cloud\", product=\"n9e\" }\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n## Protocol, must be \"tcp\" or \"udp\"\n## NOTE: because the \"udp\" protocol does not respond to requests, it requires\n## a send/expect string pair (see below).\n# protocol = \"tcp\"\n\n## Set timeout\n# timeout = \"1s\"\n\n## Set read timeout (only used if expecting a response)\n# read_timeout = \"1s\"\n\n## The following options are required for UDP checks. For TCP, they are\n## optional. The plugin will send the given string to the server and then\n## expect to receive the given \'expect\' string back.\n## string sent to the server\n# send = \"ssh\"\n## expected string in answer\n# expect = \"ssh', '# net_response\n\nç½‘ç»œæ¢æµ‹æ’ä»¶ï¼Œé€šå¸¸ç”¨äºç›‘æ§æœ¬æœºæŸä¸ªç«¯å£æ˜¯å¦åœ¨ç›‘å¬ï¼Œæˆ–è¿œç«¯æŸä¸ªç«¯å£æ˜¯å¦èƒ½è¿é€šã€‚å› ä¸º Prometheus ç”Ÿæ€çš„æ—¶åºåº“åªèƒ½å­˜å‚¨ float64 ç±»å‹çš„å€¼ï¼Œæ‰€ä»¥ç½‘ç»œæ¢æµ‹æ’ä»¶æ¢æµ‹çš„ç»“æœä¹Ÿæ˜¯ float64 ç±»å‹çš„å€¼ï¼Œä½†æ˜¯è¿™ä¸ªå€¼çš„å«ä¹‰æ˜¯ä¸åŒçš„ï¼Œå…·ä½“å«ä¹‰å¦‚ä¸‹ï¼š\n\n```\n- 0: Success\n- 1: Timeout\n- 2: ConnectionFailed\n- 3: ReadFailed\n- 4: StringMismatch\n```\n\nå¦‚æœä¸€åˆ‡æ­£å¸¸ï¼Œè¿™ä¸ªå€¼æ˜¯ 0ï¼Œå¦‚æœæœ‰å¼‚å¸¸ï¼Œè¿™ä¸ªå€¼æ˜¯ 1-4 ä¹‹é—´çš„å€¼ï¼Œå…·ä½“å«ä¹‰å¦‚ä¸Šã€‚è¿™ä¸ªå€¼å¯¹åº”çš„æŒ‡æ ‡åå­—æ˜¯ `net_response_result_code`ã€‚\n\n## Configuration\n\ncategraf çš„ `conf/input.net_response/net_response.toml`ã€‚æœ€æ ¸å¿ƒçš„é…ç½®å°±æ˜¯ targets éƒ¨åˆ†ï¼ŒæŒ‡å®šæ¢æµ‹çš„ç›®æ ‡ï¼Œä¸‹é¢çš„ä¾‹å­ï¼š\n\n```toml\n[[instances]]\ntargets = [\n    \"10.2.3.4:22\",\n    \"localhost:6379\",\n    \":9090\"\n]\n```\n\n- `10.2.3.4:22` è¡¨ç¤ºæ¢æµ‹ 10.2.3.4 è¿™ä¸ªæœºå™¨çš„ 22 ç«¯å£æ˜¯å¦å¯ä»¥è¿é€š\n- `localhost:6379` è¡¨ç¤ºæ¢æµ‹æœ¬æœºçš„ 6379 ç«¯å£æ˜¯å¦å¯ä»¥è¿é€š\n- `:9090` è¡¨ç¤ºæ¢æµ‹æœ¬æœºçš„ 9090 ç«¯å£æ˜¯å¦å¯ä»¥è¿é€š\n\nç›‘æ§æ•°æ®æˆ–å‘Šè­¦äº‹ä»¶ä¸­åªæ˜¯ä¸€ä¸ª IP å’Œç«¯å£ï¼Œæ¥æ”¶å‘Šè­¦çš„äººçœ‹åˆ°äº†ï¼Œå¯èƒ½ä¸æ¸…æ¥šåªæ˜¯å“ªä¸ªä¸šåŠ¡çš„æ¨¡å—å‘Šè­¦äº†ï¼Œå¯ä»¥é™„åŠ ä¸€äº›æ›´æœ‰ä»·å€¼çš„ä¿¡æ¯æ”¾åˆ°æ ‡ç­¾é‡Œï¼Œæ¯”å¦‚ï¼š\n\n```toml\nlabels = { region=\"cloud\", product=\"n9e\" }\n```\n\næ ‡è¯†äº†è¿™æ˜¯ cloud è¿™ä¸ª regionï¼Œn9e è¿™ä¸ªäº§å“ï¼Œè¿™ä¿©æ ‡ç­¾ä¼šé™„åˆ°æ—¶åºæ•°æ®ä¸Šï¼Œå‘Šè­¦çš„æ—¶å€™è‡ªç„¶ä¹Ÿä¼šæŠ¥å‡ºæ¥ã€‚\n\nå®Œæ•´é…ç½®æ ·ä¾‹å¦‚ä¸‹ï¼š\n\n```toml\n[mappings]\n# \"127.0.0.1:22\"= {region=\"local\",ssh=\"test\"}\n# \"127.0.0.1:22\"= {region=\"local\",ssh=\"redis\"}\n\n[[instances]]\ntargets = [\n#     \"127.0.0.1:22\",\n#     \"localhost:6379\",\n#     \":9090\"\n]\n\n# # append some labels for series\n# labels = { region=\"cloud\", product=\"n9e\" }\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n## Protocol, must be \"tcp\" or \"udp\"\n## NOTE: because the \"udp\" protocol does not respond to requests, it requires\n## a send/expect string pair (see below).\n# protocol = \"tcp\"\n\n## Set timeout\n# timeout = \"1s\"\n\n## Set read timeout (only used if expecting a response)\n# read_timeout = \"1s\"\n\n## The following options are required for UDP checks. For TCP, they are\n## optional. The plugin will send the given string to the server and then\n## expect to receive the given \'expect\' string back.\n## string sent to the server\n# send = \"ssh\"\n## expected string in answer\n# expect = \"ssh\"\n```\n\n## ç›‘æ§å¤§ç›˜å’Œå‘Šè­¦è§„åˆ™\n\nå¤œèºå†…ç½®äº†ä»ªè¡¨ç›˜å’Œå‘Šè­¦è§„åˆ™ï¼Œå…‹éš†åˆ°è‡ªå·±çš„ä¸šåŠ¡ç»„å³å¯ä½¿ç”¨ã€‚', 'ç›‘æ§æœ¬æœºæˆ–è¿œç«¯æŸä¸ªç«¯å£æ˜¯å¦åœ¨ç›‘å¬æˆ–è”é€š', '1732763419', 'ç‹æ¨(822032277)', '1732763441', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('86', 'Procstat', '# # collect interval\n# interval = 15\n\n# [[instances]]\n# # executable name (ie, pgrep <search_exec_substring>)\n# search_exec_substring = \"nginx\"\n\n# # pattern as argument for pgrep (ie, pgrep -f <search_cmdline_substring>)\n# search_cmdline_substring = \"n9e server\"\n\n# # windows service name\n# search_win_service = \"\"\n\n# # search process with specific user, option with exec_substring or cmdline_substring\n# search_user = \"\"\n\n# # append some labels for series\n# labels = { region=\"cloud\", product=\"n9e\" }\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# # mode to use when calculating CPU usage. can be one of \'solaris\' or \'irix\'\n# mode = \"irix\"\n\n# sum of threads/fd/io/cpu/mem, min of uptime/limit\ngather_total = true\n\n# will append pid as tag\ngather_per_pid = false\n\n#  gather jvm metrics only when jstat is ready\n# gather_more_metrics = [\n#     \"threads\",\n#     \"fd\",\n#     \"io\",\n#     \"uptime\",\n#     \"cpu\",\n#     \"mem\",\n#     \"limit\",\n#     \"jvm\"\n# ]', '# è¿›ç¨‹ç›‘æ§\n\nä½¿ç”¨ categraf procstat æ’ä»¶ã€‚\n\n## é…ç½®æ–‡ä»¶\n\nä½ç½®ï¼šcategraf çš„ `conf/input.procstat/procstat.toml`\n\næ ·ä¾‹é…ç½®ï¼š\n\n```toml\n[[instances]]\n# # executable name (ie, pgrep <search_exec_substring>)\nsearch_exec_substring = \"nginx\"\n\n# # pattern as argument for pgrep (ie, pgrep -f <search_cmdline_substring>)\n# search_cmdline_substring = \"n9e server\"\n\n# # windows service name\n# search_win_service = \"\"\n\n# # search process with specific user, option with exec_substring or cmdline_substring\n# search_user = \"\"\n\n# # append some labels for series\n# labels = { region=\"cloud\", product=\"n9e\" }\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# # mode to use when calculating CPU usage. can be one of \'solaris\' or \'irix\'\n# mode = \"irix\"\n\n# sum of threads/fd/io/cpu/mem, min of uptime/limit\ngather_total = true\n\n# will append pid as tag\ngather_per_pid = false\n\n#  gather jvm metrics only when jstat is ready\n# gather_more_metrics = [\n#     \"threads\",\n#     \"fd\",\n#     \"io\",\n#     \"uptime\",\n#     \"cpu\",\n#     \"mem\",\n#     \"limit\",\n#     \"jvm\"\n# ]\n```\n\næœºå™¨ä¸Šæœ‰å¾ˆå¤šè¿›ç¨‹ï¼Œè¦ç›‘æ§è¿›ç¨‹æ˜¯å¦å­˜æ´»ä»¥åŠè¿›ç¨‹çš„èµ„æºå ç”¨ï¼Œé¦–å…ˆå¾—å‘Šè¯‰ categrafï¼Œè¦ç›‘æ§çš„è¿›ç¨‹æ˜¯å•¥ã€‚æ‰€ä»¥ï¼Œæœ¬æ’ä»¶ä¸€å¼€å§‹çš„å‡ ä¸ªé…ç½®ï¼Œå°±æ˜¯åšè¿›ç¨‹è¿‡æ»¤çš„ï¼Œç”¨æ¥å‘Šè¯‰ categraf è¦ç›‘æ§çš„è¿›ç¨‹æ˜¯å“ªäº›ã€‚\n\n- search_exec_substring é…ç½®ä¸€ä¸ªæŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œç›¸å½“äºæ‰§è¡Œ `pgrep <search_exec_substring>`\n- search_cmdline_substring é…ç½®ä¸€ä¸ªæŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œç›¸å½“äºæ‰§è¡Œ `pgrep -f <search_cmdline_substring>`\n- search_win_service é…ç½®ä¸€ä¸ª windows æœåŠ¡åï¼Œç›¸å½“äºæ‰§è¡Œ `sc query <search_win_service>`\n\nä¸Šä¾‹é»˜è®¤æ˜¯é‡‡é›† nginxã€‚é»˜è®¤åªä¼šé‡‡é›†ä¸€ä¸ªæŒ‡æ ‡ï¼šprocstat_lookup_countï¼Œè¡¨ç¤ºé€šè¿‡è¿™äº›è¿‡æ»¤æ¡ä»¶ï¼ŒæŸ¥è¯¢åˆ°çš„è¿›ç¨‹çš„æ•°é‡ã€‚é‚£æ˜¾ç„¶ï¼Œå¦‚æœ `procstat_lookup_count <= 0` å°±è¯´æ˜è¿›ç¨‹ä¸å­˜åœ¨äº†ã€‚\n\n## CPU åˆ©ç”¨ç‡è®¡ç®—\n\nåœ¨è®¡ç®— CPU åˆ©ç”¨ç‡çš„æ—¶å€™æœ‰ä¸¤ç§æ¨¡å¼ï¼širixï¼ˆé»˜è®¤ï¼‰ã€solarisã€‚å¦‚æœæ˜¯ irix æ¨¡å¼ï¼ŒCPU åˆ©ç”¨ç‡ä¼šå‡ºç°å¤§äº 100% çš„æƒ…å†µï¼Œå¦‚æœæ˜¯ solaris æ¨¡å¼ï¼Œä¼šè€ƒè™‘ CPU æ ¸æ•°ï¼Œæ‰€ä»¥ CPU åˆ©ç”¨ç‡ä¸ä¼šå¤§äº 100%ã€‚\n\n## é‡‡é›†æ›´å¤šæŒ‡æ ‡\n\n`gather_more_metrics` é»˜è®¤æ²¡æœ‰æ‰“å¼€ï¼Œå³ä¸ä¼šé‡‡é›†è¿›ç¨‹èµ„æºåˆ©ç”¨æƒ…å†µã€‚å¦‚æœæƒ³è¦é‡‡é›†ï¼Œå°±æ‰“å¼€ `gather_more_metrics` è¿™ä¸ªé…ç½®å³å¯ã€‚å…¶ä¸­æœ€ä¸ºç‰¹æ®Šçš„æ˜¯ `jvm`ï¼Œå¦‚æœæƒ³è¦é‡‡é›† jvm æŒ‡æ ‡ï¼Œéœ€è¦å…ˆå®‰è£…å¥½ jstatï¼Œç„¶åå†æ‰“å¼€ `jvm` è¿™ä¸ªé…ç½®ã€‚\n\n## gather_total\n\næ¯”å¦‚è¿›ç¨‹åå­—æ˜¯ mysql çš„è¿›ç¨‹ï¼ŒåŒæ—¶å¯èƒ½è¿è¡Œäº†å¤šä¸ªï¼Œæˆ‘ä»¬æƒ³çŸ¥é“è¿™ä¸ªæœºå™¨ä¸Šçš„æ‰€æœ‰ mysql çš„è¿›ç¨‹å ç”¨çš„æ€»çš„ cpuã€memã€fd ç­‰ï¼Œå°±è®¾ç½® gather_total = trueï¼Œå½“ç„¶ï¼Œå¯¹äº uptime å’Œ limit çš„é‡‡é›†ï¼Œgather_total çš„æ—¶å€™æ˜¯å–çš„å¤šä¸ªè¿›ç¨‹çš„æœ€å°å€¼ã€‚\n\n## gather_per_pid\n\nè¿˜æ˜¯æ‹¿ mysql ä¸¾ä¾‹ï¼Œä¸€ä¸ªæœºå™¨ä¸Šå¯èƒ½åŒæ—¶è¿è¡Œäº†å¤šä¸ªï¼Œæˆ‘ä»¬å¯èƒ½æƒ³çŸ¥é“æ¯ä¸ª mysql è¿›ç¨‹çš„èµ„æºå ç”¨æƒ…å†µï¼Œæ­¤æ—¶å°±è¦å¯ç”¨ gather_per_pid çš„é…ç½®ï¼Œè®¾ç½®ä¸º trueï¼Œæ­¤æ—¶ä¼šé‡‡é›†æ¯ä¸ªè¿›ç¨‹çš„èµ„æºå ç”¨æƒ…å†µï¼Œå¹¶é™„ä¸Š pid ä½œä¸ºæ ‡ç­¾æ¥åŒºåˆ†\n\n## å‘Šè­¦è§„åˆ™\n\nå¤œèºå†…ç½®äº†è¿›ç¨‹ç›‘æ§çš„å‘Šè­¦è§„åˆ™ï¼Œå…‹éš†åˆ°è‡ªå·±çš„ä¸šåŠ¡ç»„ä¸‹å³å¯ä½¿ç”¨ã€‚\n\n## ä»ªè¡¨ç›˜\n\nå¤œèºå†…ç½®äº†è¿›ç¨‹ç›‘æ§çš„ä»ªè¡¨ç›˜ï¼Œå…‹éš†åˆ°è‡ªå·±çš„ä¸šåŠ¡ç»„ä¸‹å³å¯ä½¿ç”¨ã€‚', 'è¿›ç¨‹ç›‘æ§', '1732763490', 'ç‹æ¨(822032277)', '1732763490', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('87', 'IPMI', '# Read metrics from the bare metal servers via freeipmi\n[[instances]]\n# targetæŒ‡å®šæ˜¯æœ¬åœ°é‡‡é›†è¿˜æ˜¯è¿œç¨‹é‡‡é›†\n#target=\"localhost\"\n# æŒ‡å®šé‡‡é›†çš„ç”¨æˆ·åå’Œå¯†ç ï¼Œè¿™é‡ŒåŠ¡å¿…ä¿è¯ipmiå‘½ä»¤èƒ½è·å–æ­£ç¡®è¾“å‡ºï¼Œä¸æ˜¯ç½‘ä¸ŠæŸ¥åˆ°ä¸€ä¸ªç”¨æˆ·å å¯†ç å°±å¯ä»¥ã€‚\n#user = \"user\"\n#pass = \"1234\"\n\n# ipmiåè®®ç‰ˆæœ¬ï¼Œæ”¯æŒ1.5 å’Œ 2.0\n#driver = \"LAN_2_0\"\n\n# æŒ‡å®šç‰¹æƒç”¨æˆ·å\n#privilege = \"user\"\n\n## session-timeout, ms\n#timeout = 100000\n\n# æ”¯æŒçš„é‡‡é›†å™¨  bmc, bmc-watchdog, ipmi, chassis, dcmi, selï¼Œsm-lan-mode\n# é»˜è®¤ä½¿ç”¨ bmc, ipmi, chassiså’Œdcmiï¼Œå»ºè®®ä¿æŒä¸‹åˆ—é…ç½®ä¾¿äºä»ªè¡¨ç›˜æ›´å¥½çš„å±•ç¤º\ncollectors = [ \"bmc\", \"ipmi\", \"chassis\", \"sel\", \"dcmi\"]\n\n# ä¸å…³æ³¨çš„ä¼ æ„Ÿå™¨ï¼ŒæŒ‡å®šid æ’é™¤æ‰\n#exclude_sensor_ids = [ 2, 29, 32, 50, 52, 55 ]\n\n# å¦‚æœä½ æƒ³ä½¿ç”¨å®šåˆ¶åŒ–çš„å‚æ•°è¦†ç›–å†…ç½®çš„å‘½ä»¤ï¼Œå¯ä»¥ä¿®æ”¹ä»¥ä¸‹å†…å®¹ï¼› å»ºè®®ä¿æŒæ³¨é‡Š\n#[instances.collector_cmd]\n#ipmi = \"sudo\"\n#sel = \"sudo\"\n#[instances.default_args]\n#ipmi = [ \"--bridge-sensors\" ]\n#[instances.custom_args]\n#ipmi = [ \"--bridge-sensors\" ]\n#sel = [ \"ipmi-sel\" ]\n', 'ipmiæ’ä»¶æ˜¯ä»ipmi exporterè¿ç§»è¿‡æ¥ã€‚ åŸºæœ¬åŸç†æ˜¯é€šè¿‡æ‰§è¡Œipmiçš„ä¸€ç³»åˆ—å‘½ä»¤å¹¶å°†å‘½ä»¤è¾“å‡ºè½¬æ¢ä¸ºæŒ‡æ ‡ï¼Œå¦‚æœipmiæ²¡æœ‰é…ç½®å¥½ï¼Œæ˜¯æ— æ³•é‡‡é›†åˆ°æŒ‡æ ‡çš„ï¼Œè¯·åŠ¡å¿…å°†ipmié…ç½®å¥½ã€‚\n\ncategrafçš„ipmiæ’ä»¶é…ç½®ä¸¾ä¾‹å¦‚ä¸‹ï¼š\n```toml\n# Read metrics from the bare metal servers via freeipmi\n[[instances]]\n# targetæŒ‡å®šæ˜¯æœ¬åœ°é‡‡é›†è¿˜æ˜¯è¿œç¨‹é‡‡é›†\n#target=\"localhost\"\n# æŒ‡å®šé‡‡é›†çš„ç”¨æˆ·åå’Œå¯†ç ï¼Œè¿™é‡ŒåŠ¡å¿…ä¿è¯ipmiå‘½ä»¤èƒ½è·å–æ­£ç¡®è¾“å‡ºï¼Œä¸æ˜¯ç½‘ä¸ŠæŸ¥åˆ°ä¸€ä¸ªç”¨æˆ·å å¯†ç å°±å¯ä»¥ã€‚\n#user = \"user\"\n#pass = \"1234\"\n\n# ipmiåè®®ç‰ˆæœ¬ï¼Œæ”¯æŒ1.5 å’Œ 2.0 \n#driver = \"LAN_2_0\"\n\n# æŒ‡å®šç‰¹æƒç”¨æˆ·å\n#privilege = \"user\"\n\n## session-timeout, ms\n#timeout = 100000\n\n# æ”¯æŒçš„é‡‡é›†å™¨  bmc, bmc-watchdog, ipmi, chassis, dcmi, selï¼Œsm-lan-mode\n# é»˜è®¤ä½¿ç”¨ bmc, ipmi, chassiså’Œdcmiï¼Œå»ºè®®ä¿æŒä¸‹åˆ—é…ç½®ä¾¿äºä»ªè¡¨ç›˜æ›´å¥½çš„å±•ç¤º\ncollectors = [ \"bmc\", \"ipmi\", \"chassis\", \"sel\", \"dcmi\"]\n\n# ä¸å…³æ³¨çš„ä¼ æ„Ÿå™¨ï¼ŒæŒ‡å®šid æ’é™¤æ‰\n#exclude_sensor_ids = [ 2, 29, 32, 50, 52, 55 ]\n\n# å¦‚æœä½ æƒ³ä½¿ç”¨å®šåˆ¶åŒ–çš„å‚æ•°è¦†ç›–å†…ç½®çš„å‘½ä»¤ï¼Œå¯ä»¥ä¿®æ”¹ä»¥ä¸‹å†…å®¹ï¼› å»ºè®®ä¿æŒæ³¨é‡Š\n#[instances.collector_cmd]\n#ipmi = \"sudo\"\n#sel = \"sudo\"\n#[instances.default_args]\n#ipmi = [ \"--bridge-sensors\" ]\n#[instances.custom_args]\n#ipmi = [ \"--bridge-sensors\" ]\n#sel = [ \"ipmi-sel\" ]\n```', 'ipmiç›‘æ§æŒ‡æ ‡é‡‡é›†', '1732763646', 'ç‹æ¨(822032277)', '1732763646', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('88', 'SNMP', '#SNMPä¸åŒå‚å•†è®¾å¤‡æœ‰ä¸åŒçš„ç§æœ‰ oidï¼Œä»¥ä¸‹ä»¥Ciscoè®¾å¤‡ä¸ºä¾‹\n[[instances]]\n\nagents = [\"udp://127.0.0.1\"]\n\ntimeout = \"5s\"\n\nversion = 2\n\n## Path to mib files\n## Used by the gosmi translator.\n## To add paths when translating with netsnmp, use the MIBDIRS environment variable\n##path = [\"/usr/share/snmp/DCN\"]\n##translator = \"gosmi\"\n\ncommunity = \"public\"\n\nagent_host_tag = \"DCN\"\n\nretries = 3\n\nmax_repetitions = 100\n\n##è¿è¡Œæ—¶é—´\n[[instances.field]]\noid = \"1.3.6.1.2.1.1.3.0\"\nname = \"sys_uptime\"\nconversion = \"float(2)\"\n\n[[instances.field]]\noid = \"1.3.6.1.4.1.6339.100.1.11.10.0\"\nname = \"cpu_usage\"\n\n[[instances.field]]\noid = \"1.3.6.1.4.1.6339.100.1.11.6.0\"\nname = \"mem_max\"\n\n[[instances.field]]\noid = \"1.3.6.1.4.1.6339.100.1.11.7.0\"\nname = \"mem_use\"\n\n#ç«¯å£æ€»å’Œ\n[[instances.field]]\nname = \"TotalPorts\"\noid = \"1.3.6.1.4.1.6339.100.3.1.0\"\n\n##è®¾å¤‡åç§°\n[[instances.field]]\noid = \"1.3.6.1.2.1.1.5.0\"\nname = \"sys_name\"\nis_tag = true\n\n##äº§å“å‹å·\n[[instances.field]]\nname = \"sys_pm\"\noid = \"1.3.6.1.4.1.6339.100.25.1.1.1.0\"\nis_tag = true\n\n#æœ¬æœºIP\n[[instances.field]]\nname = \"LocalIP\"\noid = \"1.3.6.1.2.1.4.20.1.1\"\nis_tag = true\n\n#æ¥å£è¡¨ä¿¡æ¯\n[[instances.table]]\nname = \"interface\"\ninherit_tags = [\"sys_name\",\"sys_pm\",\"LocalIP\"]\n\n#å„ä¸ªç«¯å£\n[[instances.table.field]]\nname = \"ifDescr\"\noid = \"1.3.6.1.2.1.2.2.1.2\"\nis_tag = true\n\n[[instances.table.field]]\nname = \"ifSpeed\"\noid = \"1.3.6.1.2.1.2.2.1.5\"\nconversion = \"float(6)\"\n#is_tag = true\n\n[[instances.table.field]]\nname = \"ifOperStatus\"\noid = \"1.3.6.1.2.1.2.2.1.8\"\n#is_tag = true\n\n[[instances.table.field]]\nname = \"ifOutOctets\"\noid = \"1.3.6.1.2.1.2.2.1.16\"\n\n[[instances.table.field]]\nname = \"ifInOctets\"\noid = \"1.3.6.1.2.1.2.2.1.10\"\n\n\n#èšåˆçŠ¶æ€\n#oid = \"1.3.6.1.4.1.6339.100.14.2.1.4.1.1.*\"\n\n#èšåˆç«¯å£\n#oid = \"1.3.6.1.4.1.6339.100.14.3.1.2\"\n', '# snmp\n\n> ç›‘æ§ç½‘ç»œè®¾å¤‡ï¼Œä¸»è¦æ˜¯é€šè¿‡ SNMP åè®®ï¼ŒCategrafã€Telegrafã€Datadog-Agentã€snmp_exporter éƒ½æä¾›äº†è¿™ä¸ªèƒ½åŠ›ã€‚\n\nCategraf ä» v0.2.13 ç‰ˆæœ¬å¼€å§‹æŠŠ Telegraf çš„ snmp æ’ä»¶é›†æˆäº†è¿›æ¥ï¼Œæ¨èå¤§å®¶é‡‡ç”¨è¿™ä¸ªæ’ä»¶æ¥ç›‘æ§ç½‘ç»œè®¾å¤‡ã€‚è¿™ä¸ªæ’ä»¶çš„æ ¸å¿ƒé€»è¾‘æ˜¯ï¼šè¦é‡‡é›†ä»€ä¹ˆæŒ‡æ ‡ï¼Œç›´æ¥é…ç½®å¯¹åº”çš„ oid å³å¯ï¼Œè€Œä¸”å¯ä»¥æŠŠä¸€äº› oid é‡‡é›†åˆ°çš„æ•°æ®å½“åšæ—¶åºæ•°æ®çš„æ ‡ç­¾ï¼Œéå¸¸éå¸¸çµæ´»ã€‚\n\nå½“ç„¶ï¼Œå¼Šç«¯ä¹Ÿæœ‰ï¼Œå› ä¸º SNMP ä½“ç³»é‡Œæœ‰å¤§é‡çš„ç§æœ‰ oidï¼Œæ¯”å¦‚ä¸åŒçš„è®¾å¤‡è·å– CPUã€å†…å­˜åˆ©ç”¨ç‡çš„ oid éƒ½ä¸ä¸€æ ·ï¼Œè¿™å°±éœ€è¦ä¸ºä¸åŒçš„å‹å·çš„è®¾å¤‡é‡‡ç”¨ä¸åŒçš„é…ç½®ï¼Œç»´æŠ¤èµ·æ¥æ¯”è¾ƒéº»çƒ¦ï¼Œéœ€è¦å¤§é‡çš„ç§¯ç´¯ã€‚è¿™é‡Œæˆ‘å€¡è®®å¤§å®¶æŠŠä¸åŒçš„è®¾å¤‡å‹å·çš„é‡‡é›†é…ç½®ç§¯ç´¯åˆ° [è¿™é‡Œ](https://github.com/flashcatcloud/categraf/tree/main/inputs/snmp)ï¼Œæ¯ä¸ªå‹å·ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œé•¿æœŸç§¯ç´¯ä¸‹æ¥ï¼Œé‚£å°†æ˜¯åˆ©äººåˆ©å·±çš„å¥½äº‹ã€‚ä¸çŸ¥é“å¦‚ä½•æ PR çš„å¯ä»¥è”ç³»æˆ‘ä»¬ã€‚\n\nå¦å¤–ï¼Œä¹Ÿä¸ç”¨å¤ªæ‚²è§‚ï¼Œé’ˆå¯¹ç½‘ç»œè®¾å¤‡è€Œè¨€ï¼Œå¤§éƒ¨åˆ†ç›‘æ§æ•°æ®çš„é‡‡é›†éƒ½æ˜¯é€šç”¨ oid å°±å¯ä»¥æå®šçš„ï¼Œä¸¾ä¸ªä¾‹å­ï¼š\n\n```toml\ninterval = 120\n\n[[instances]]\nagents = [\"udp://172.30.15.189:161\"]\n\ninterval_times = 1\ntimeout = \"5s\"\nversion = 2\ncommunity = \"public\"\nagent_host_tag = \"switch_ip\"\nretries = 1\n\n[[instances.field]]\noid = \"RFC1213-MIB::sysUpTime.0\"\nname = \"uptime\"\n\n[[instances.field]]\noid = \"RFC1213-MIB::sysName.0\"\nname = \"source\"\nis_tag = true\n\n[[instances.table]]\noid = \"IF-MIB::ifTable\"\nname = \"interface\"\ninherit_tags = [\"source\"]\n\n[[instances.table.field]]\noid = \"IF-MIB::ifDescr\"\nname = \"ifDescr\"\nis_tag = true\n\n```\n\nä¸Šé¢çš„æ ·ä¾‹æ˜¯ v2 ç‰ˆæœ¬çš„é…ç½®ï¼Œå¦‚æœæ˜¯ v3 ç‰ˆæœ¬ï¼Œæ ¡éªŒæ–¹å¼ä¸¾ä¾‹ï¼š\n\n```toml\nversion = 3\nsec_name = \"managev3user\"\nauth_protocol = \"SHA\"\nauth_password = \"example.Demo.c0m\"\n```\n\nå¦å¤–ï¼Œsnmp çš„é‡‡é›†ï¼Œå»ºè®®å¤§å®¶éƒ¨ç½²å•ç‹¬çš„ Categraf æ¥åšï¼Œå› ä¸ºä¸åŒç›‘æ§å¯¹è±¡é‡‡é›†é¢‘ç‡å¯èƒ½ä¸åŒï¼Œæ¯”å¦‚è¾¹ç¼˜äº¤æ¢æœºï¼Œæˆ‘ä»¬ 5min é‡‡é›†ä¸€æ¬¡å°±å¤Ÿäº†ï¼Œæ ¸å¿ƒäº¤æ¢æœºå¯ä»¥é…ç½®çš„é¢‘ç¹ä¸€äº›ï¼Œæ¯”å¦‚ 60s æˆ–è€… 120sã€‚\n\n> æ³¨æ„ï¼šå¦‚æœé‡‡é›†çš„è¿‡äºé¢‘ç¹ï¼Œæœ‰äº›è€æ¬¾çš„äº¤æ¢æœºå¯èƒ½ä¼šè¢«æ‰“æŒ‚ï¼Œæˆ–è€…è¢«é™æµï¼Œè¢«é™æµçš„ç»“æœå°±æ˜¯å›¾ä¸Šçœ‹åˆ°çš„æ˜¯æ–­ç‚¹ã€‚\n\n## æ‰©å±•é˜…è¯»\n\n- [SNMP(ç®€å•ç½‘ç»œç®¡ç†åè®®)ç®€ä»‹](https://flashcat.cloud/blog/snmp-introduction/)\n- [SNMPå‘½ä»¤ç›¸å…³å‚æ•°ä»‹ç»](https://flashcat.cloud/blog/snmp-command-arguments/)\n- [é€šè¿‡ Categraf SNMP æ’ä»¶é‡‡é›†ç›‘æ§æ•°æ®](https://flashcat.cloud/blog/snmp-metrics-collect-by-categraf/)\n\n## æ’é”™\n\nè¦æƒ³é€šè¿‡ categraf é‡‡é›†åˆ° snmp æ•°æ®ï¼Œé¦–å…ˆè¦ä¿è¯ categraf æ‰€åœ¨çš„æœºå™¨èƒ½å¤Ÿè¿é€šç½‘ç»œè®¾å¤‡ï¼Œå¯ä»¥é€šè¿‡ snmpget å‘½ä»¤æ¥åšæµ‹è¯•ï¼š\n\n```bash\nsnmpget -v2c -c public 172.30.15.189 RFC1213-MIB::sysUpTime.0\n```\n\nå¦‚æœ snmpget éƒ½è·‘ä¸é€šï¼Œå°±å¾—å…ˆè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ¯”å¦‚æ˜¯ snmpd æ²¡æœ‰å¯åŠ¨ï¼Œæˆ–è€…é˜²ç«å¢™é™åˆ¶äº† snmp çš„è®¿é—®ï¼Œè¿˜æ˜¯ snmpget å‘½ä»¤æ²¡æœ‰å®‰è£…ï¼Œç­‰ç­‰ã€‚è¿™äº›é—®é¢˜ï¼Œgpt å’Œ google éƒ½å¯ä»¥è§£å†³ï¼Œè¿™é‡Œä¸å†èµ˜è¿°ã€‚', 'ç›‘æ§ç½‘ç»œè®¾å¤‡ç­‰', '1732763965', 'ç‹æ¨(822032277)', '1732763965', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('89', 'Kafka', '# # collect interval\n# interval = 15\n\n############################################################################\n# !!! uncomment [[instances]] to enable this plugin\n[[instances]]\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# append some labels to metrics\n# cluster is a preferred tag with the cluster name. If none is provided, the first of kafka_uris will be used\nlabels = { cluster=\"kafka-cluster-01\" }\n\n# log level only for kafka exporter\nlog_level = \"error\"\n\n# Address (host:port) of Kafka server.\n# kafka_uris = [\"127.0.0.1:9092\",\"127.0.0.1:9092\",\"127.0.0.1:9092\"]\nkafka_uris = []\n\n# Connect using SASL/PLAIN\n# Default is false\n# use_sasl = false\n\n# Only set this to false if using a non-Kafka SASL proxy\n# Default is true\n# use_sasl_handshake = false\n\n# SASL user name\n# sasl_username = \"username\"\n\n# SASL user password\n# sasl_password = \"password\"\n\n# The SASL SCRAM SHA algorithm sha256 or sha512 as mechanism\n# sasl_mechanism = \"\"\n\n# Connect using TLS\n# use_tls = false\n\n# The optional certificate authority file for TLS client authentication\n# ca_file = \"\"\n\n# The optional certificate file for TLS client authentication\n# cert_file = \"\"\n\n# The optional key file for TLS client authentication\n# key_file = \"\"\n\n# If true, the server\'s certificate will not be checked for validity. This will make your HTTPS connections insecure\n# insecure_skip_verify = true\n\n# Kafka broker version\n# Default is 2.0.0\n# kafka_version = \"2.0.0\"\n\n# if you need to use a group from zookeeper\n# Default is false\n# use_zookeeper_lag = false\n\n# Address array (hosts) of zookeeper server.\n# zookeeper_uris = []\n\n# Metadata refresh interval\n# Default is 1m\n# metadata_refresh_interval = \"1m\"\n\n# Whether show the offset/lag for all consumer group, otherwise, only show connected consumer groups, default is true\n# Default is true\n# offset_show_all = true\n\n# If true, all scrapes will trigger kafka operations otherwise, they will share results. WARN: This should be disabled on large clusters\n# Default is false\n# allow_concurrency = false\n\n# Maximum number of offsets to store in the interpolation table for a partition\n# Default is 1000\n# max_offsets = 1000\n\n# How frequently should the interpolation table be pruned, in seconds.\n# Default is 30\n# prune_interval_seconds = 30\n\n# Regex filter for topics to be monitored\n# Default is \".*\"\n# topics_filter_regex = \".*\"\n\n# Regex filter for consumer groups to be monitored\n# Default is \".*\"\n# groups_filter_regex = \".*\"\n\n# if rename  kafka_consumergroup_uncommitted_offsets to kafka_consumergroup_lag\n# Default is false\n# rename_uncommit_offset_to_lag = false\n\n\n# if disable calculating lag rate\n# Default is false\n# disable_calculate_lag_rate = false', '# kafka plugin\n\nKafka çš„æ ¸å¿ƒæŒ‡æ ‡ï¼Œå…¶å®éƒ½æ˜¯é€šè¿‡ JMX çš„æ–¹å¼æš´éœ²çš„ã€‚å¯¹äº JMX æš´éœ²çš„æŒ‡æ ‡ï¼Œä½¿ç”¨ jolokia æˆ–è€…ä½¿ç”¨ jmx_exporter é‚£ä¸ª jar åŒ…æ¥é‡‡é›†å³å¯ï¼Œä¸éœ€è¦æœ¬æ’ä»¶ã€‚\n\næœ¬æ’ä»¶ä¸»è¦æ˜¯é‡‡é›†çš„æ¶ˆè´¹è€…å»¶è¿Ÿæ•°æ®ï¼Œè¿™ä¸ªæ•°æ®æ— æ³•é€šè¿‡ Kafka æœåŠ¡ç«¯çš„ JMX æ‹¿åˆ°ã€‚\n\næœ¬æ’ä»¶ fork è‡ª [https://github.com/davidmparrott/kafka_exporter](https://github.com/davidmparrott/kafka_exporter)ï¼ˆä»¥ä¸‹ç®€ç§° davidmparrott ç‰ˆæœ¬ï¼‰ï¼Œdavidmparrott ç‰ˆæœ¬ fork è‡ª [https://github.com/danielqsj/kafka_exporter](https://github.com/danielqsj/kafka_exporter)ï¼ˆä»¥ä¸‹ç®€ç§° danielqsj ç‰ˆæœ¬ï¼‰ã€‚\n\ndanielqsj ç‰ˆæœ¬ä½œä¸ºåŸå§‹ç‰ˆæœ¬, github ç‰ˆæœ¬ä¹Ÿç›¸å¯¹æ´»è·ƒ, prometheus ç”Ÿæ€ä½¿ç”¨è¾ƒå¤šã€‚davidmparrott ç‰ˆæœ¬ä¸ danielqsj ç‰ˆæœ¬ç›¸æ¯”, æœ‰ä»¥ä¸‹ metric åå­—ä¸åŒï¼š\n\n| davidmparrott ç‰ˆæœ¬  | danielqsj ç‰ˆæœ¬ |\n| ---- | ---- |\n| kafka_consumergroup_uncommit_offsets  | kafka_consumergroup_lag |\n| kafka_consumergroup_uncommit_offsets_sum  | kafka_consumergroup_lag_sum |\n| kafka_consumergroup_uncommitted_offsets_zookeeper | kafka_consumergroup_lag_zookeeper |\n\nå¦‚æœæƒ³ä½¿ç”¨ danielqsj ç‰ˆæœ¬çš„ metric, åœ¨ `[[instances]]` ä¸­è¿›è¡Œå¦‚ä¸‹é…ç½®:\n\n```toml\nrename_uncommit_offset_to_lag = true\n```\n\ndavidmparrott ç‰ˆæœ¬æ¯” danielqsj ç‰ˆæœ¬å¤šäº†ä»¥ä¸‹ metricï¼Œè¿™äº›æŒ‡æ ‡æ˜¯å¯¹å»¶è¿Ÿé€Ÿç‡åšäº†é¢„ä¼°è®¡ç®—ï¼š\n\n- kafka_consumer_lag_millis\n- kafka_consumer_lag_interpolation\n- kafka_consumer_lag_extrapolation\n\nä¸ºä»€ä¹ˆè¦è®¡ç®—é€Ÿç‡ï¼Ÿå› ä¸º lag å¾ˆå¤§ï¼Œä½†æ˜¯æ¶ˆè´¹å¾ˆå¿«ï¼Œæ˜¯ä¸ä¼šç§¯å‹çš„ï¼Œè€Œ lag å¾ˆå°ï¼Œæ¶ˆè´¹å¾ˆæ…¢ï¼Œä»ç„¶ä¼šç§¯å‹ï¼Œæ‰€ä»¥ï¼Œé€šè¿‡ lag å¤§å°æ˜¯æ²¡æ³•åˆ¤æ–­ç§¯å‹é£é™©çš„ã€‚é€šè¿‡è®¡ç®—å†å²æ¶ˆè´¹é€Ÿç‡ï¼Œæ¥åˆ¤æ–­ç§¯å‹é£é™©ä¼šæ›´ä¸ºåˆç†ã€‚è¦è®¡ç®—è¿™ä¸ªé€Ÿç‡ï¼Œéœ€è¦å ç”¨è¾ƒå¤šå†…å­˜ï¼Œå¯ä»¥é€šè¿‡å¦‚ä¸‹é…ç½®å…³é—­è¿™ä¸ªè®¡ç®—é€»è¾‘ï¼š\n\n```toml\ndisable_calculate_lag_rate = true\n```\n\n## é‡‡é›†é…ç½®\n\ncategraf é…ç½®æ–‡ä»¶ï¼š`conf/input.kafka/kafka.toml`ã€‚é…ç½®æ ·ä¾‹å¦‚ä¸‹ï¼š\n\n```toml\n[[instances]]\nlog_level = \"error\"\nkafka_uris = [\"192.168.0.250:9092\"]\nlabels = { cluster=\"kafka-cluster-01\", service=\"kafka\" }\n```\n\nå®Œæ•´çš„å¸¦æœ‰æ³¨é‡Šçš„é…ç½®å¦‚ä¸‹ï¼š\n\n```toml\n[[instances]]\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# append some labels to metrics\n# cluster is a preferred tag with the cluster name. If none is provided, the first of kafka_uris will be used\nlabels = { cluster=\"kafka-cluster-01\" }\n\n# log level only for kafka exporter\nlog_level = \"error\"\n\n# Address (host:port) of Kafka server.\n# kafka_uris = [\"127.0.0.1:9092\",\"127.0.0.1:9092\",\"127.0.0.1:9092\"]\nkafka_uris = []\n\n# Connect using SASL/PLAIN\n# Default is false\n# use_sasl = false\n\n# Only set this to false if using a non-Kafka SASL proxy\n# Default is true\n# use_sasl_handshake = false\n\n# SASL user name\n# sasl_username = \"username\"\n\n# SASL user password\n# sasl_password = \"password\"\n\n# The SASL SCRAM SHA algorithm sha256 or sha512 as mechanism\n# sasl_mechanism = \"\"\n\n# Connect using TLS\n# use_tls = false\n\n# The optional certificate authority file for TLS client authentication\n# ca_file = \"\"\n\n# The optional certificate file for TLS client authentication\n# cert_file = \"\"\n\n# The optional key file for TLS client authentication\n# key_file = \"\"\n\n# If true, the server\'s certificate will not be checked for validity. This will make your HTTPS connections insecure\n# insecure_skip_verify = true\n\n# Kafka broker version\n# Default is 2.0.0\n# kafka_version = \"2.0.0\"\n\n# if you need to use a group from zookeeper\n# Default is false\n# use_zookeeper_lag = false\n\n# Address array (hosts) of zookeeper server.\n# zookeeper_uris = []\n\n# Metadata refresh interval\n# Default is 1m\n# metadata_refresh_interval = \"1m\"\n\n# Whether show the offset/lag for all consumer group, otherwise, only show connected consumer groups, default is true\n# Default is true\n# offset_show_all = true\n\n# If true, all scrapes will trigger kafka operations otherwise, they will share results. WARN: This should be disabled on large clusters\n# Default is false\n# allow_concurrency = false\n\n# Maximum number of offsets to store in the interpolation table for a partition\n# Default is 1000\n# max_offsets = 1000\n\n# How frequently should the interpolation table be pruned, in seconds.\n# Default is 30\n# prune_interval_seconds = 30\n\n# Regex filter for topics to be monitored\n# Default is \".*\"\n# topics_filter_regex = \".*\"\n\n# Regex filter for consumer groups to be monitored\n# Default is \".*\"\n# groups_filter_regex = \".*\"\n\n# if rename  kafka_consumergroup_uncommitted_offsets to kafka_consumergroup_lag\n# Default is false\n# rename_uncommit_offset_to_lag = false\n\n\n# if disable calculating lag rate\n# Default is false\n# disable_calculate_lag_rate = false\n```\n\n## å‘Šè­¦è§„åˆ™\n\nå¤œèºæä¾›äº†å†…ç½®çš„ Kafka å‘Šè­¦è§„åˆ™ï¼Œå…‹éš†åˆ°è‡ªå·±çš„ä¸šåŠ¡ç»„ä¸‹å³å¯ä½¿ç”¨ã€‚\n\n![20230801162030](https://download.flashcat.cloud/ulric/20230801162030.png)\n\n## ä»ªè¡¨ç›˜ï¼š\n\nå¤œèºæä¾›äº†å†…ç½®çš„ Kafka ä»ªè¡¨ç›˜ï¼Œå…‹éš†åˆ°è‡ªå·±çš„ä¸šåŠ¡ç»„ä¸‹å³å¯ä½¿ç”¨ã€‚\n\n![20230801162017](https://download.flashcat.cloud/ulric/20230801162017.png)', 'kafkaæ¶ˆè´¹æ•°æ®é‡‡é›†', '1732764105', 'ç‹æ¨(822032277)', '1732764105', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('90', 'Filecount', '# # collect interval\n# interval = 15\n\n[[instances]]\n# # append some labels for series\n# labels = { region=\"cloud\", product=\"n9e\" }\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n## Directories to gather stats about.\n## This accept standard unit glob matching rules, but with the addition of\n## ** as a \"super asterisk\". ie:\n##   /var/log/**    -> recursively find all directories in /var/log and count files in each directories\n##   /var/log/*/*   -> find all directories with a parent dir in /var/log and count files in each directories\n##   /var/log       -> count all files in /var/log and all of its subdirectories\n## directories = [\"/var/cache/apt\", \"/tmp\"]\ndirectories = [\"/tmp\"]\n\n## Only count files that match the name pattern. Defaults to \"*\".\nfile_name = \"*\"\n\n## Count files in subdirectories. Defaults to true.\nrecursive = true\n\n## Only count regular files. Defaults to true.\nregular_only = true\n\n## Follow all symlinks while walking the directory tree. Defaults to false.\nfollow_symlinks = false\n\n## Only count files that are at least this size. If size is\n## a negative number, only count files that are smaller than the\n## absolute value of size. Acceptable units are B, KiB, MiB, KB, ...\n## Without quotes and units, interpreted as size in bytes.\nsize = \"0B\"\n\n## Only count files that have not been touched for at least this\n## duration. If mtime is negative, only count files that have been\n## touched in this duration. Defaults to \"0s\".\nmtime = \"0s\"\n', '# Filecount Input Plugin\n\nforked from telegraf/inputs.filecount\n\nReports the number and total size of files in specified directories.\n\n\n## Configuration\n\n```toml filecount.toml\n# # collect interval\n# interval = 15\n\n[[instances]]\n# # append some labels for series\n# labels = { region=\"cloud\", product=\"n9e\" }\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n## Directories to gather stats about.\n## This accept standard unit glob matching rules, but with the addition of\n## ** as a \"super asterisk\". ie:\n##   /var/log/**    -> recursively find all directories in /var/log and count files in each directories\n##   /var/log/*/*   -> find all directories with a parent dir in /var/log and count files in each directories\n##   /var/log       -> count all files in /var/log and all of its subdirectories\n## directories = [\"/var/cache/apt\", \"/tmp\"]\ndirectories = [\"/tmp\", \"/root\"]\n\n## Only count files that match the name pattern. Defaults to \"*\".\nfile_name = \"*\"\n\n## Count files in subdirectories. Defaults to true.\nrecursive = true\n\n## Only count regular files. Defaults to true.\nregular_only = true\n\n## Follow all symlinks while walking the directory tree. Defaults to false.\nfollow_symlinks = false\n\n## Only count files that are at least this size. If size is\n## a negative number, only count files that are smaller than the\n## absolute value of size. Acceptable units are B, KiB, MiB, KB, ...\n## Without quotes and units, interpreted as size in bytes.\nsize = \"0B\"\n\n## Only count files that have not been touched for at least this\n## duration. If mtime is negative, only count files that have been\n## touched in this duration. Defaults to \"0s\".\nmtime = \"0s\"\n\n```\n\n## Metrics\n\n- filecount\n    - tags:\n        - directory (the directory path)\n    - fields:\n        - count (integer)\n        - size_bytes (integer)\n        - oldest_file_timestamp (int, unix time nanoseconds)\n        - newest_file_timestamp (int, unix time nanoseconds)\n\n## Example Output\n\n```text\n13:25:07 filecount_count agent_hostname=host1 directory=/tmp 319\n13:25:07 filecount_size_bytes agent_hostname=host1 directory=/tmp 83196547\n13:25:07 filecount_oldest_file_timestamp agent_hostname=host1 directory=/tmp 0\n13:25:07 filecount_newest_file_timestamp agent_hostname=host1 directory=/tmp 1692336254306413522\n```\n', 'ç›‘æ§ç›®å½•æ–‡ä»¶æ•°é‡åŠå¤§å°', '1732764200', 'ç‹æ¨(822032277)', '1732764200', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('91', 'MongoDB', '[[instances]]\n# log level, enum: panic, fatal, error, warn, warning, info, debug, trace, defaults to info.\nlog_level = \"info\"\n# append some const labels to metrics\n# NOTICE! the instance label is required for dashboards\nlabels = { instance=\"mongo-cluster-01\" }\n\n# mongodb dsn, see https://www.mongodb.com/docs/manual/reference/connection-string/\n# mongodb_uri = \"mongodb://127.0.0.1:27017\"\nmongodb_uri = \"\"\n# if you don\'t specify the username or password in the mongodb_uri, you can set here. \n# This will overwrite the dsn, it would be helpful when special characters existing in the username or password and you don\'t want to encode them.\n# NOTICE! this user must be granted enough rights to query needed stats, see ../inputs/mongodb/README.md\nusername = \"username@Bj\"\npassword = \"password@Bj\"\n# if set to true, use the direct connection way\n# direct_connect = true\n\n# collect all means you collect all the metrics, if set, all below enable_xxx flags in this section will be ignored\ncollect_all = true\n# if set to true, collect databases metrics\n# enable_db_stats = true\n# if set to true, collect getDiagnosticData metrics\n# enable_diagnostic_data = true\n# if set to true, collect replSetGetStatus metrics\n# enable_replicaset_status = true\n# if set to true, collect top metrics by admin command\n# enable_top_metrics = true\n# if set to true, collect index metrics. You should specify one of the coll_stats_namespaces and the discovering_mode flags.\n# enable_index_stats = true\n# if set to true, collect collections metrics. You should specify one of the coll_stats_namespaces and the discovering_mode flags.\n# enable_coll_stats = true\n\n# Only get stats for the collections matching this list of namespaces. if none set, discovering_mode will be enabled.\n# Example: db1.col1,db.col1\n# coll_stats_namespaces = []\n# Only get stats for index with the collections matching this list of namespaces.\n# Example: db1.col1,db.col1\n# index_stats_collections = []\n# if set to true, replace -1 to DESC for label key_name of the descending_index metrics\n# enable_override_descending_index = true\n\n# which exposes metrics with 0.1x compatible metric names has been implemented which simplifies migration from the old version to the current version.\n# compatible_mode = true\n\n\n# [[instances]]\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# log_level = \"error\"\n\n# append some labels to metrics\n# labels = { instance=\"mongo-cluster-02\" }\n# mongodb_uri = \"mongodb://username:password@127.0.0.1:27017\"\n# collect_all = true\n# compatible_mode = true', '# mongodb\n\nmongodb ç›‘æ§é‡‡é›†æ’ä»¶ï¼Œç”± [mongodb-exporter](https://github.com/percona/mongodb_exporter)å°è£…è€Œæ¥ã€‚\n\n## Configuration\n\né…ç½®æ–‡ä»¶ç¤ºä¾‹ï¼š\n\n```toml\n[[instances]]\n# log level, enum: panic, fatal, error, warn, warning, info, debug, trace, defaults to info.\nlog_level = \"info\"\n# append some const labels to metrics\n# NOTICE! the instance label is required for dashboards\nlabels = { instance=\"mongo-cluster-01\" }\n\n# mongodb dsn, see https://www.mongodb.com/docs/manual/reference/connection-string/\n# mongodb_uri = \"mongodb://127.0.0.1:27017\"\nmongodb_uri = \"\"\n# if you don\'t specify the username or password in the mongodb_uri, you can set here. \n# This will overwrite the dsn, it would be helpful when special characters existing in the username or password and you don\'t want to encode them.\n# NOTICE! this user must be granted enough rights to query needed stats, see ../inputs/mongodb/README.md\nusername = \"username@Bj\"\npassword = \"password@Bj\"\n# if set to true, use the direct connection way\n# direct_connect = true\n\n# collect all means you collect all the metrics, if set, all below enable_xxx flags in this section will be ignored\ncollect_all = true\n# if set to true, collect databases metrics\n# enable_db_stats = true\n# if set to true, collect getDiagnosticData metrics\n# enable_diagnostic_data = true\n# if set to true, collect replSetGetStatus metrics\n# enable_replicaset_status = true\n# if set to true, collect top metrics by admin command\n# enable_top_metrics = true\n# if set to true, collect index metrics. You should specify one of the coll_stats_namespaces and the discovering_mode flags.\n# enable_index_stats = true\n# if set to true, collect collections metrics. You should specify one of the coll_stats_namespaces and the discovering_mode flags.\n# enable_coll_stats = true\n\n# Only get stats for the collections matching this list of namespaces. if none set, discovering_mode will be enabled.\n# Example: db1.col1,db.col1\n# coll_stats_namespaces = []\n# Only get stats for index with the collections matching this list of namespaces.\n# Example: db1.col1,db.col1\n# index_stats_collections = []\n# if set to true, replace -1 to DESC for label key_name of the descending_index metrics\n# enable_override_descending_index = true\n\n# which exposes metrics with 0.1x compatible metric names has been implemented which simplifies migration from the old version to the current version.\n# compatible_mode = true\n\n\n# [[instances]]\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# log_level = \"error\"\n\n# append some labels to metrics\n# labels = { instance=\"mongo-cluster-02\" }\n# mongodb_uri = \"mongodb://username:password@127.0.0.1:27017\"\n# collect_all = true\n# compatible_mode = true\n```\n\ncategraf ä½œä¸ºä¸€ä¸ª client è¿æ¥ MongoDBï¼Œéœ€è¦æœ‰è¶³å¤Ÿçš„æƒé™æ¥æ”¶é›†æŒ‡æ ‡ï¼Œå…·ä½“çš„æƒé™é…ç½®è¯·å‚è€ƒ[å®˜æ–¹æ–‡æ¡£](https://www.mongodb.com/docs/manual/reference/built-in-roles/#mongodb-authrole-clusterMonitor)ã€‚è‡³å°‘å…·æœ‰ä»¥ä¸‹æƒé™æ‰å¯ä»¥ï¼š\n\n```json\n{\n    \"role\":\"clusterMonitor\",\n    \"db\":\"admin\"\n},\n{\n    \"role\":\"read\",\n    \"db\":\"local\"\n}\n```\n\næˆæƒæ“ä½œæ ·ä¾‹ï¼š\n\n```shell\nmongo -h xxx -u xxx -p xxx --authenticationDatabase admin\n> use admin\n> db.createUser({user:\"categraf\",pwd:\"categraf\",roles: [{role:\"read\",db:\"local\"},{\"role\":\"clusterMonitor\",\"db\":\"admin\"}]})\n```\n\n## ç›‘æ§å¤§ç›˜å’Œå‘Šè­¦è§„åˆ™\n\nå¤œèºå†…ç½®äº† MongoDB çš„å‘Šè­¦è§„åˆ™å’Œç›‘æ§å¤§ç›˜ï¼Œå…‹éš†åˆ°è‡ªå·±çš„ä¸šåŠ¡ç»„ä½¿ç”¨å³å¯ã€‚è™½ç„¶æ–‡ä»¶åç¼€æ˜¯ `_exporter` ä¹Ÿå¯ä»¥ä½¿ç”¨ï¼Œå› ä¸º categraf è¿™ä¸ªæ’ä»¶æ˜¯åŸºäº mongodb-exporter å°è£…çš„ã€‚', 'mongodbç›‘æ§æŒ‡æ ‡é‡‡é›†', '1732764267', 'ç‹æ¨(822032277)', '1732764267', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('92', 'Prometheus', '# prometheus å„ç§exporterçš„æŒ‡æ ‡é‡‡é›†,ä¸»è¦é‡‡é›†/metricsæš´éœ²å‡ºæ¥çš„æŒ‡æ ‡\n# # collect interval\n# interval = 15\n\n[[instances]]\nurls = [\n#     \"http://localhost:19000/metrics\"\n]\n\nurl_label_key = \"instance\"\nurl_label_value = \"{{.Host}}\"\n\n## Scrape Services available in Consul Catalog\n# [instances.consul]\n#   enabled = false\n#   agent = \"http://localhost:8500\"\n#   query_interval = \"5m\"\n\n#   [[instances.consul.query]]\n#     name = \"a service name\"\n#     tag = \"a service tag\"\n#     url = \'http://{{if ne .ServiceAddress \"\"}}{{.ServiceAddress}}{{else}}{{.Address}}{{end}}:{{.ServicePort}}/{{with .ServiceMeta.metrics_path}}{{.}}{{else}}metrics{{end}}\'\n#     [instances.consul.query.tags]\n#       host = \"{{.Node}}\"\n\n# bearer_token_string = \"\"\n\n# e.g. /run/secrets/kubernetes.io/serviceaccount/token\n# bearer_token_file = \"\"\n\n# # basic auth\n# username = \"\"\n# password = \"\"\n\n# headers = [\"X-From\", \"categraf\"]\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n\n# labels = {}\n\n# support glob\n# ignore_metrics = [ \"go_*\" ]\n\n# support glob\n# ignore_label_keys = []\n\n# timeout for every url\n# timeout = \"3s\"\n\n## Optional TLS Config\n# use_tls = false\n# tls_min_version = \"1.2\"\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = true', '# prometheus\n\nprometheus æ’ä»¶çš„ä½œç”¨ï¼Œå°±æ˜¯æŠ“å– `/metrics` æ¥å£çš„æ•°æ®ï¼Œä¸ŠæŠ¥ç»™æœåŠ¡ç«¯ã€‚é€šè¿‡ï¼Œå„ç±» exporter ä¼šæš´éœ² `/metrics` æ¥å£æ•°æ®ï¼Œè¶Šæ¥è¶Šå¤šçš„å¼€æºç»„ä»¶ä¹Ÿä¼šå†…ç½® prometheus SDKï¼Œåå‡º prometheus æ ¼å¼çš„ç›‘æ§æ•°æ®ï¼Œæ¯”å¦‚ rabbitmq æ’ä»¶ï¼Œå…¶ README ä¸­å°±æœ‰ä»‹ç»ã€‚\n\nå¢åŠ äº†ä¸¤ä¸ªé…ç½®ï¼šurl_label_key å’Œ url_label_valueã€‚ä¸ºäº†æ ‡è¯†ç›‘æ§æ•°æ®æ˜¯ä»å“ªä¸ª scrape url æ‹‰å–çš„ï¼Œä¼šä¸ºç›‘æ§æ•°æ®é™„ä¸€ä¸ªæ ‡ç­¾æ¥æ ‡è¯†è¿™ä¸ª urlï¼Œé»˜è®¤çš„æ ‡ç­¾ KEY æ˜¯ç”¨ instanceï¼Œå½“ç„¶ï¼Œä¹Ÿå¯ä»¥æ”¹æˆåˆ«çš„ï¼Œä¸è¿‡ä¸å»ºè®®ã€‚url_label_value æ˜¯æ ‡ç­¾å€¼ï¼Œæ”¯æŒ go template è¯­æ³•ï¼Œå¦‚æœä¸ºç©ºï¼Œå°±æ˜¯æ•´ä¸ª url çš„å†…å®¹ï¼Œä¹Ÿå¯ä»¥é€šè¿‡æ¨¡æ¿å˜é‡åªå–ä¸€éƒ¨åˆ†ï¼Œæ¯”å¦‚ `http://localhost:9104/metrics`ï¼Œåªæƒ³å– IP å’Œç«¯å£éƒ¨åˆ†ï¼Œå°±å¯ä»¥å†™æˆï¼š\n\n```ini\nurl_label_value = \"{{.Host}}\"\n```\n\nå¦‚æœ HTTP scheme éƒ¨åˆ†å’Œ `/metrics` Path éƒ¨åˆ†éƒ½æƒ³å–ï¼Œå¯ä»¥è¿™ä¹ˆå†™ï¼š\n\n```ini\nurl_label_value = \"{{.Scheme}}://{{.Host}}{{.Path}}\"\n```\n\nç›¸å…³å˜é‡æ˜¯ç”¨è¿™ä¸ªæ–¹æ³•ç”Ÿæˆçš„ï¼Œä¾›å¤§å®¶å‚è€ƒï¼š\n\n```go\nfunc (ul *UrlLabel) GenerateLabel(u *url.URL) (string, string, error) {\n	if ul.LabelValue == \"\" {\n		return ul.LabelKey, u.String(), nil\n	}\n\n	dict := map[string]string{\n		\"Scheme\":   u.Scheme,\n		\"Host\":     u.Host,\n		\"Hostname\": u.Hostname(),\n		\"Port\":     u.Port(),\n		\"Path\":     u.Path,\n		\"Query\":    u.RawQuery,\n		\"Fragment\": u.Fragment,\n	}\n\n	var buffer bytes.Buffer\n	err := ul.LabelValueTpl.Execute(&buffer, dict)\n	if err != nil {\n		return \"\", \"\", err\n	}\n\n	return ul.LabelKey, buffer.String(), nil\n}\n```', 'prometheus exporteræŒ‡æ ‡é‡‡é›†', '1732764577', 'ç‹æ¨(822032277)', '1732764577', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('93', 'Netstat_Filter', '# # collect interval\n# interval = 15\n[[instances]]\n# laddr_ip = \"\"\n# laddr_port = 0\n# raddr_ip = \"\"\n# raddr_port = 0', '# netstat_filter\n\nè¯¥æ’ä»¶é‡‡é›†ç½‘ç»œè¿æ¥æƒ…å†µï¼Œå¹¶æ ¹æ®ç”¨æˆ·æ¡ä»¶è¿›è¡Œè¿‡æ»¤ç»Ÿè®¡ï¼Œä»¥è¾¾åˆ°ç›‘æ§ç”¨æˆ·å…³å¿ƒé“¾æ¥æƒ…å†µ\n## æŒ‡æ ‡åˆ—è¡¨\ntcp_established  \ntcp_syn_sent\ntcp_syn_recv\ntcp_fin_wait1\ntcp_fin_wait2\ntcp_time_wait\ntcp_close\ntcp_close_wait\ntcp_last_ack\ntcp_listen\ntcp_closing\ntcp_none\ntcp_send_queue\ntcp_recv_queue\n\n## åŠŸèƒ½è¯´æ˜\nå¯¹æºIPã€æºç«¯å£ã€ç›®æ ‡IPå’Œç›®æ ‡ç«¯å£è¿‡æ»¤åè¿›è¡Œç½‘å¡recv-Qã€send-Qè¿›è¡Œé‡‡é›†ï¼Œè¯¥æŒ‡æ ‡å¯ä»¥å¾ˆå¥½ååº”å‡ºæŒ‡å®šè¿æ¥çš„è´¨é‡ï¼Œä¾‹å¦‚rttæ—¶é—´è¿‡é•¿ï¼Œå¯¼è‡´æ”¶åˆ°æœåŠ¡ç«¯ackç¡®è®¤å¾ˆæ…¢å°±ä¼šä½¿send-Qé•¿æœŸå¤§äº0ï¼Œå¯ä»¥åŠæ—¶é€šè¿‡ç›‘æ§å‘ç°ï¼Œä»è€Œæå‰ä¼˜åŒ–ç½‘ç»œæˆ–ç¨‹åº\n\nå½“è¿‡æ»¤ç»“æœä¸ºå¤šä¸ªè¿æ¥æ—¶ä¼šå°†sendå’Œrecvå€¼è¿›è¡ŒåŠ å’Œ\nä¾‹å¦‚ï¼š\né…ç½®æ–‡ä»¶``raddr_port = 11883``\nå½“æœ¬åœ°å’Œä¸åŒIPçš„11883éƒ½æœ‰è¿æ¥å»ºç«‹çš„æƒ…å†µä¸‹ï¼Œä¼šå°†å¤šæ¡è¿æ¥çš„ç»“æœè¿›è¡ŒåŠ å’Œã€‚æˆ–åœ¨å¹¶å‘å¤šè¿æ¥çš„æƒ…å†µä¸‹ï¼Œä¼šåˆå¹¶åŠ åˆï¼Œæ€»ä¹‹è¿‡æ»¤çš„è¶Šç²—ç•¥è¢«åŠ åˆæ•°å°±ä¼šè¶Šå¤šã€‚\n\nå¤šæ¡è§„åˆ™è¯·å¤åˆ¶``[[instances]]``è¿›è¡Œé…ç½®\n\n## æ³¨æ„äº‹é¡¹\nnetstat_filter_tcp_send_queueå’Œnetstat_filter_tcp_recv_queueæŒ‡æ ‡ç›®å‰åªæ”¯æŒlinuxã€‚windowsç”¨æˆ·é»˜è®¤ä¸º0ã€‚', 'ç½‘ç»œè¿æ¥æƒ…å†µ', '1732764646', 'ç‹æ¨(822032277)', '1732764646', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('94', 'Oracle', '# # collect interval\n# interval = 15\n\n#[[instances]]\n# address = \"10.1.2.3:1521/orcl\"\n# username = \"monitor\"\n# password = \"123456\"\n# is_sys_dba = false\n# is_sys_oper = false\n# disable_connection_pool = false\n# max_open_connections = 5\n# # interval = global.interval * interval_times\n# interval_times = 1\n# labels = { region=\"cloud\" }\n\n# [[instances.metrics]]\n# mesurement = \"sessions\"\n# label_fields = [ \"status\", \"type\" ]\n# metric_fields = [ \"value\" ]\n# timeout = \"3s\"\n# request = \'\'\'\n# SELECT status, type, COUNT(*) as value FROM v$session GROUP BY status, type\n# \'\'\'\n\n# [[instances]]\n# address = \"192.168.10.10:1521/orcl\"\n# username = \"monitor\"\n# password = \"123456\"\n# is_sys_dba = false\n# is_sys_oper = false\n# disable_connection_pool = false\n# max_open_connections = 5\n# # labels = { region=\"local\" }\n\n\n[[metrics]]\nmesurement = \"sessions\"\nlabel_fields = [ \"status\", \"type\" ]\nmetric_fields = [ \"value\" ]\ntimeout = \"3s\"\nrequest = \'\'\'\nSELECT status, type, COUNT(*) as value FROM v$session GROUP BY status, type\n\'\'\'\n\n[[metrics]]\nmesurement = \"lock\"\nmetric_fields = [ \"cnt\" ]\ntimeout = \"3s\"\nrequest = \'\'\'\nSELECT COUNT(*) AS cnt\n  FROM ALL_OBJECTS A, V$LOCKED_OBJECT B, SYS.GV_$SESSION C\n WHERE A.OBJECT_ID = B.OBJECT_ID\n   AND B.PROCESS = C.PROCESS\n\'\'\'\n\n[[metrics]]\nmesurement = \"slow_queries\"\nmetric_fields = [ \"p95_time_usecs\" , \"p99_time_usecs\"]\ntimeout = \"3s\"\nrequest = \'\'\'\nselect  percentile_disc(0.95)  within group (order by elapsed_time) as p95_time_usecs,\n  percentile_disc(0.99)  within group (order by elapsed_time) as p99_time_usecs\nfrom v$sql where last_active_time >= sysdate - 5/(24*60)\n\'\'\'\n\n[[metrics]]\nmesurement = \"resource\"\nlabel_fields = [ \"resource_name\" ]\nmetric_fields = [ \"current_utilization\", \"limit_value\" ]\ntimeout = \"3s\"\nrequest = \'\'\'\nSELECT resource_name,current_utilization,CASE WHEN TRIM(limit_value) LIKE \'UNLIMITED\' THEN \'-1\' ELSE TRIM(limit_value) END as limit_value FROM v$resource_limit\n\'\'\'\n\n[[metrics]]\nmesurement = \"asm_diskgroup\"\nlabel_fields = [ \"name\" ]\nmetric_fields = [ \"total\", \"free\" ]\ntimeout = \"3s\"\nrequest = \'\'\'\nSELECT name,total_mb*1024*1024 as total,free_mb*1024*1024 as free FROM v$asm_diskgroup_stat where exists (select 1 from v$datafile where name like \'+%\')\n\'\'\'\nIgnoreZeroResult = true\n\n[[metrics]]\nmesurement = \"activity\"\nmetric_fields = [ \"value\" ]\nfield_to_append = \"name\"\ntimeout = \"3s\"\nrequest = \'\'\'\nSELECT name, value FROM v$sysstat WHERE name IN (\'parse count (total)\', \'execute count\', \'user commits\', \'user rollbacks\')\n\'\'\'\n\n[[metrics]]\nmesurement = \"process\"\nmetric_fields = [ \"count\" ]\ntimeout = \"3s\"\nrequest = \'\'\'\nSELECT COUNT(*) as count FROM v$process\n\'\'\'\n\n[[metrics]]\nmesurement = \"wait_time\"\nmetric_fields = [ \"value\" ]\nlabel_fields = [\"wait_class\"]\ntimeout = \"3s\"\nrequest = \'\'\'\nSELECT\n  n.wait_class as WAIT_CLASS,\n  round(m.time_waited/m.INTSIZE_CSEC,3) as VALUE\nFROM\n  v$waitclassmetric  m, v$system_wait_class n\nWHERE\n  m.wait_class_id=n.wait_class_id AND n.wait_class != \'Idle\'\n\'\'\'\n\n[[metrics]]\nmesurement = \"tablespace\"\nlabel_fields = [ \"tablespace\", \"type\" ]\nmetric_fields = [ \"bytes\", \"max_bytes\", \"free\" ]\ntimeout = \"3s\"\nrequest = \'\'\'\nSELECT\n    dt.tablespace_name as tablespace,\n    dt.contents as type,\n    dt.block_size * dtum.used_space as bytes,\n    dt.block_size * dtum.tablespace_size as max_bytes,\n    dt.block_size * (dtum.tablespace_size - dtum.used_space) as free\nFROM  dba_tablespace_usage_metrics dtum, dba_tablespaces dt\nWHERE dtum.tablespace_name = dt.tablespace_name\nORDER by tablespace\n\'\'\'\n\n[[metrics]]\nmesurement = \"sysmetric\"\nmetric_fields = [ \"value\" ]\nfield_to_append = \"metric_name\"\ntimeout = \"3s\"\nrequest = \'\'\'\nselect METRIC_NAME,VALUE from v$sysmetric where group_id=2\n\'\'\'', '# Oracle plugin\n\nOracle æ’ä»¶ï¼Œç”¨äºç›‘æ§ Oracle æ•°æ®åº“ã€‚é»˜è®¤æ— æ³•è·‘åœ¨ Windows ä¸Šã€‚å¦‚æœä½ çš„ Oracle éƒ¨ç½²åœ¨ Windows ä¸Šï¼Œä¹Ÿæ²¡é—®é¢˜ï¼Œä½¿ç”¨éƒ¨ç½²åœ¨ Linux ä¸Šçš„ Categraf è¿œç¨‹ç›‘æ§ Windows ä¸Šçš„ Oracleï¼Œä¹Ÿè¡Œå¾—é€šã€‚\n\nOracle æ’ä»¶çš„æ ¸å¿ƒç›‘æ§åŸç†ï¼Œå°±æ˜¯æ‰§è¡Œä¸‹é¢ [è¿™äº› SQL è¯­å¥](https://github.com/flashcatcloud/categraf/blob/main/conf/input.oracle/metric.toml)ï¼Œç„¶åè§£æå‡ºç»“æœï¼Œä¸ŠæŠ¥åˆ°ç›‘æ§æœåŠ¡ç«¯ã€‚\n\nä»¥å…¶ä¸­ä¸€ä¸ªä¸ºä¾‹ï¼š\n\n```toml\n[[metrics]]\nmesurement = \"activity\"\nmetric_fields = [ \"value\" ]\nfield_to_append = \"name\"\ntimeout = \"3s\"\nrequest = \'\'\'\nSELECT name, value FROM v$sysstat WHERE name IN (\'parse count (total)\', \'execute count\', \'user commits\', \'user rollbacks\')\n\'\'\'\n```\n\n- mesurementï¼šæŒ‡æ ‡ç±»åˆ«\n- label_fieldsï¼šä½œä¸º label çš„å­—æ®µ\n- metric_fieldsï¼šä½œä¸º metric çš„å­—æ®µï¼Œå› ä¸ºæ˜¯ä½œä¸º metric çš„å­—æ®µï¼Œæ‰€ä»¥è¿™ä¸ªå­—æ®µçš„å€¼å¿…é¡»æ˜¯æ•°å­—\n- field_to_appendï¼šè¡¨ç¤ºè¿™ä¸ªå­—æ®µé™„åŠ åˆ° metric_name åé¢ï¼Œä½œä¸º metric_name çš„ä¸€éƒ¨åˆ†\n- timeoutï¼šè¶…æ—¶æ—¶é—´\n- requestï¼šå…·ä½“æŸ¥è¯¢çš„ SQL è¯­å¥\n\nå¦‚æœä½ æƒ³ç›‘æ§çš„æŒ‡æ ‡ï¼Œé»˜è®¤æ²¡æœ‰é‡‡é›†ï¼Œåªéœ€è¦å¢åŠ è‡ªå®šä¹‰çš„ `[[metrics]]` é…ç½®å³å¯ã€‚', 'Oracleæ•°æ®åº“ç›‘æ§', '1732764712', 'ç‹æ¨(822032277)', '1732764712', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('95', 'Tengine', '## collect interval\n# interval = 15\n\n## Set the mapping of extra tags in batches\n[mappings]\n# \"http://127.0.0.1/us\" = { \"job\" = \"local\" }\n# \"https://www.baidu.com/us\" = { \"job\" = \"baidu\" }\n\n[[instances]]\n## An array of Tengine reqstat module URI to gather stats.\nurls = [\n#    \"http://127.0.0.1/us\",\n#    \"https://www.baidu.com/us\"\n]\n\n## append some labels for series\n# labels = { region=\"cloud\", product=\"n9e\" }\n\n## interval = global.interval * interval_times\n# interval_times = 1\n\n## HTTP response timeout (default: 5s)\n# response_timeout = \"5s\"\n\n## Whether to follow redirects from the server (defaults to false)\n# follow_redirects = false\n\n## Optional HTTP Basic Auth Credentials\n#username = \"admin\"\n#password = \"admin\"\n\n## Optional headers\n# headers = [\"X-From\", \"categraf\", \"X-Xyz\", \"abc\"]\n\n## Optional TLS Config\n# use_tls = false\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = false', '# Tengine Input Plugin\n\nThe tengine plugin gathers metrics from the\n[Tengine Web Server](http://tengine.taobao.org/) via the\n[reqstat](http://tengine.taobao.org/document/http_reqstat.html) module.\n\n## Tengine Configuration Example\n\n```\nhttp {\n\n    req_status_zone server \"$host,$server_addr:$server_port\" 10M;\n    #req_status_zone_add_indicator server $limit;\n    req_status server;\n    \n    server {\n        location /us {\n            req_status_show;\n            #req_status_show_field req_total $limit;\n            #allow 127.0.0.1/32;\n            #deny all;\n        }\n        \n        #set $limit 0;\n        #if ($arg_limit = \'1\') {\n        #    set $limit 1;\n        #}\n    }\n}\n```\n\n## Metrics\n\n- Measurement\n    - tags:\n        - target\n        - target_port\n        - server_name\n        - server_schema\n    - fields:\n        - bytes_in (integer, total number of bytes received from client)\n        - bytes_out (integer, total number of bytes sent to client)\n        - conn_total (integer, total number of accepted connections)\n        - req_total (integer, total number of processed requests)\n        - http_2xx (integer, total number of 2xx requests)\n        - http_3xx (integer, total number of 3xx requests)\n        - http_4xx (integer, total number of 4xx requests)\n        - http_5xx (integer, total number of 5xx requests)\n        - http_other_status (integer, total number of other requests)\n        - rt (integer, accumulation or rt)\n        - ups_req (integer, total number of requests calling for upstream)\n        - ups_rt (integer, accumulation or upstream rt)\n        - ups_tries (integer, total number of times calling for upstream)\n        - http_200 (integer, total number of 200 requests)\n        - http_206 (integer, total number of 206 requests)\n        - http_302 (integer, total number of 302 requests)\n        - http_304 (integer, total number of 304 requests)\n        - http_403 (integer, total number of 403 requests)\n        - http_404 (integer, total number of 404 requests)\n        - http_416 (integer, total number of 416 requests)\n        - http_499 (integer, total number of 499 requests)\n        - http_500 (integer, total number of 500 requests)\n        - http_502 (integer, total number of 502 requests)\n        - http_503 (integer, total number of 503 requests)\n        - http_504 (integer, total number of 504 requests)\n        - http_508 (integer, total number of 508 requests)\n        - http_other_detail_status (integer, total number of requests of other status codes*http_ups_4xx total number of requests of upstream 4xx)\n        - http_ups_5xx (integer, total number of requests of upstream 5xx)\n\n## Example Output\n\n```text\ntengine_rt agent_hostname=zy-fat project=matrix server_name=www.baidu.com server_schema=http target=127.0.0.1 target_port=80 37634\ntengine_ups_rt agent_hostname=zy-fat project=matrix server_name=www.baidu.com server_schema=http target=127.0.0.1 target_port=80 37394\ntengine_http_499 agent_hostname=zy-fat project=matrix server_name=www.baidu.com server_schema=http target=127.0.0.1 target_port=80 0\ntengine_http_504 agent_hostname=zy-fat project=matrix server_name=www.baidu.com server_schema=http target=127.0.0.1 target_port=80 0\ntengine_bytes_in agent_hostname=zy-fat project=matrix server_name=www.baidu.com server_schema=http target=127.0.0.1 target_port=80 129592\ntengine_http_4xx agent_hostname=zy-fat project=matrix server_name=www.baidu.com server_schema=http target=127.0.0.1 target_port=80 535\ntengine_http_other_status agent_hostname=zy-fat project=matrix server_name=www.baidu.com server_schema=https target=127.0.0.1 target_port=80 0\ntengine_http_200 agent_hostname=zy-fat project=matrix server_name=www.baidu.com server_schema=https target=127.0.0.1 target_port=80 14452\ntengine_http_499 agent_hostname=zy-fat project=matrix server_name=www.baidu.com server_schema=https target=127.0.0.1 target_port=80 0\ntengine_http_503 agent_hostname=zy-fat project=matrix server_name=www.baidu.com server_schema=https target=127.0.0.1 target_port=80 0\ntengine_http_504 agent_hostname=zy-fat project=matrix server_name=www.baidu.com server_schema=https target=127.0.0.1 target_port=80 0\ntengine_http_500 agent_hostname=zy-fat project=matrix server_name=www.baidu.com server_schema=https target=127.0.0.1 target_port=80 0\ntengine_http_ups_4xx agent_hostname=zy-fat project=matrix server_name=www.baidu.com server_schema=https target=127.0.0.1 target_port=80 13\ntengine_http_ups_5xx agent_hostname=zy-fat project=matrix server_name=www.baidu.com server_schema=https target=127.0.0.1 target_port=80 1\n```', 'é€šè¿‡reqstatæ¨¡å—ç›‘æ§', '1732765027', 'ç‹æ¨(822032277)', '1732765027', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('96', 'Supervisor', '# Gathers information about processes that running under supervisor using XML-RPC API\n[[instances]]\n## Url of supervisor\'s XML-RPC endpoint if basic auth enabled in supervisor http server,\n## than you have to add credentials to url (ex. http://login:pass@localhost:9001/RPC2)\n# url = \"http://login:pass@localhost:9001/RPC2\", eg: url = \"http://localhost:9001/RPC2\"\nurl =\"\"\n## With settings below you can manage gathering additional information about processes\n## If both of them empty, then all additional information will be collected.\n## Currently supported supported additional metrics are: pid, rc\n# metrics_include = []\n# metrics_exclude = [\"pid\", \"rc\"]', '# Supervisor\n\næ­¤æ’ä»¶é€šè¿‡ä½¿ç”¨XML-RPC APIæ”¶é›†åœ¨supervisorä¸‹è¿è¡Œçš„è¿›ç¨‹ä¿¡æ¯ã€‚\n\nsupervisorçš„æœ€ä½æµ‹è¯•ç‰ˆæœ¬ä¸º3.3.2ã€‚\n\n## Supervisor é…ç½®\n\nè¿™ä¸ªæ’ä»¶éœ€è¦åœ¨supervisorä¸­å¯ç”¨HTTPæœåŠ¡å™¨ï¼ŒåŒæ—¶å»ºè®®åœ¨HTTPæœåŠ¡å™¨ä¸Šå¯ç”¨åŸºæœ¬èº«ä»½éªŒè¯ã€‚ä½¿ç”¨åŸºæœ¬è®¤è¯æ—¶ï¼Œè¯·ç¡®ä¿åœ¨æ’ä»¶çš„urlè®¾ç½®ä¸­åŒ…å«ç”¨æˆ·åå’Œå¯†ç ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ª`inet_http_server`éƒ¨åˆ†çš„supervisoré…ç½®ç¤ºä¾‹ï¼Œè¯¥é…ç½®å¯ä»¥ä¸é»˜è®¤æ’ä»¶é…ç½®ä¸€èµ·å·¥ä½œï¼š\n\n```ini\n[inet_http_server]\nport = 127.0.0.1:9001\nusername = user\npassword = pass\n```\n\n## å…¨å±€é…ç½®é€‰é¡¹\n\né™¤äº†ç‰¹å®šäºæ’ä»¶çš„é…ç½®è®¾ç½®å¤–ï¼Œæ’ä»¶è¿˜æ”¯æŒé¢å¤–çš„å…¨å±€å’Œæ’ä»¶é…ç½®è®¾ç½®ã€‚è¿™äº›è®¾ç½®ç”¨äºä¿®æ”¹æŒ‡æ ‡ã€æ ‡ç­¾å’Œå­—æ®µæˆ–åˆ›å»ºåˆ«åå’Œé…ç½®æ’åºç­‰ã€‚\n\n## é…ç½®\n\n```toml\n# Gathers information about processes that running under supervisor using XML-RPC API\n[[instances]]\n  ## Url of supervisor\'s XML-RPC endpoint if basic auth enabled in supervisor http server,\n  ## than you have to add credentials to url (ex. http://login:pass@localhost:9001/RPC2)\n  # url = \"http://login:pass@localhost:9001/RPC2\"\n  ## With settings below you can manage gathering additional information about processes\n  ## If both of them empty, then all additional information will be collected.\n  ## Currently supported supported additional metrics are: pid, rc\n  # metrics_include = []\n  # metrics_exclude = [\"pid\", \"rc\"]\n```\n\næ³¨æ„ï¼Œ`url = \"http://login:pass@localhost:9001/RPC2\"`ä¸­çš„`login:pass`æ˜¯ç”¨æˆ·åå’Œå¯†ç ã€‚ç›¸å…³ä¿¡æ¯å¯ä»¥å‚è§æ‚¨çš„supervisoré…ç½®æ–‡ä»¶ã€‚\n\n### å¯é€‰æŒ‡æ ‡\n\né€šè¿‡åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®`metrics_include`å’Œ`metrics_exclude`å‚æ•°ï¼Œç”¨äºæ§åˆ¶å“ªäº›æŒ‡æ ‡(metrics)åº”è¯¥è¢«åŒ…æ‹¬(`include`)æˆ–æ’é™¤(`exclude`)åœ¨ç›‘æ§æ•°æ®ä¸­ã€‚è¿™ä¸¤ä¸ªé…ç½®é€‰é¡¹ä¸ºç”¨æˆ·æä¾›äº†ç»†ç²’åº¦æ§åˆ¶ï¼Œä»¥ä¾¿æ ¹æ®ç‰¹å®šéœ€è¦å®šåˆ¶æ”¶é›†çš„æ•°æ®ã€‚è¿™åœ¨å¤„ç†å¤§é‡æŒ‡æ ‡æˆ–åªå…³å¿ƒæŸäº›ç‰¹å®šæŒ‡æ ‡çš„æƒ…å†µä¸‹å°¤å…¶æœ‰ç”¨ã€‚\n\n#### metrics_include\n\n- `metrics_include` é€‰é¡¹å…è®¸ä½ æŒ‡å®šä¸€ä¸ªæŒ‡æ ‡åç§°åˆ—è¡¨ï¼Œä»…è¿™äº›æŒ‡æ ‡ä¼šè¢«æ”¶é›†å’Œå‘é€ã€‚å¦‚æœè®¾ç½®äº†è¿™ä¸ªé€‰é¡¹ï¼Œé‚£ä¹ˆåªæœ‰åˆ—è¡¨ä¸­çš„æŒ‡æ ‡ä¼šè¢«åŒ…å«ï¼Œå…¶ä»–æ‰€æœ‰æŒ‡æ ‡éƒ½ä¼šè¢«å¿½ç•¥ã€‚\n- è¿™ä¸ªé€‰é¡¹é€šå¸¸ç”¨äºé™åˆ¶æ•°æ®çš„æ”¶é›†èŒƒå›´ï¼Œä»¥å‡å°‘ç½‘ç»œæµé‡ã€å­˜å‚¨éœ€æ±‚æˆ–è€…ä»…ä»…å…³æ³¨ä¸€å°éƒ¨åˆ†é‡è¦æŒ‡æ ‡ã€‚\n- æ ¼å¼é€šå¸¸æ˜¯ä¸€ä¸ªæŒ‡æ ‡åç§°çš„æ•°ç»„ï¼Œä¾‹å¦‚ï¼š`metrics_include = [\"cpu_usage_idle\", \"cpu_usage_user\"]`ã€‚\n\n#### metrics_exclude\n\n- ç›¸åï¼Œ`metrics_exclude` é€‰é¡¹å…è®¸ä½ æŒ‡å®šä¸€ä¸ªæŒ‡æ ‡åç§°åˆ—è¡¨ï¼Œè¿™äº›æŒ‡æ ‡å°†ä¸ä¼šè¢«æ”¶é›†å’Œå‘é€ã€‚å¦‚æœè®¾ç½®äº†è¿™ä¸ªé€‰é¡¹ï¼Œé‚£ä¹ˆåˆ—è¡¨ä¸­çš„æŒ‡æ ‡ä¼šè¢«æ’é™¤ï¼Œå…¶ä»–æ‰€æœ‰æŒ‡æ ‡éƒ½ä¼šè¢«åŒ…å«ã€‚\n- è¿™ä¸ªé€‰é¡¹ç”¨äºä»æ”¶é›†çš„æ•°æ®ä¸­æ’é™¤ä¸æ„Ÿå…´è¶£æˆ–ä¸ç›¸å…³çš„æŒ‡æ ‡ï¼Œæœ‰åŠ©äºå‡å°‘å¤„ç†å’Œå­˜å‚¨æ— ç”¨æ•°æ®çš„è´Ÿæ‹…ã€‚\n- æ ¼å¼åŒæ ·æ˜¯ä¸€ä¸ªæŒ‡æ ‡åç§°çš„æ•°ç»„ï¼Œä¾‹å¦‚ï¼š`metrics_exclude = [\"memory_free\", \"memory_cached\"]`ã€‚\n\n#### ä½¿ç”¨æ³¨æ„äº‹é¡¹\n\n- å¦‚æœåŒæ—¶ä½¿ç”¨`metrics_include`å’Œ`metrics_exclude`ï¼Œé¦–å…ˆåº”ç”¨`metrics_include`è¿‡æ»¤è§„åˆ™ï¼Œç„¶ååº”ç”¨`metrics_exclude`ã€‚è¿™æ„å‘³ç€å¦‚æœä¸€ä¸ªæŒ‡æ ‡åœ¨`metrics_include`ä¸­è¢«æ˜ç¡®åŒ…å«ï¼Œåœ¨`metrics_exclude`ä¸­ä¹Ÿè¢«æ˜ç¡®æ’é™¤ï¼Œé‚£ä¹ˆè¿™ä¸ªæŒ‡æ ‡æœ€ç»ˆå°†è¢«æ’é™¤ã€‚\n- è¿™ä¸¤ä¸ªé…ç½®é€‰é¡¹çš„å·¥ä½œåŸç†å’Œå…·ä½“å¯ç”¨å€¼å¯èƒ½ä¾èµ–äºå…·ä½“çš„æ’ä»¶ã€‚æœ‰çš„æ’ä»¶å¯èƒ½å…è®¸æ ¹æ®æŒ‡æ ‡çš„æŸäº›å±æ€§æˆ–æ ‡ç­¾æ¥è¿›è¡ŒåŒ…å«æˆ–æ’é™¤ã€‚\n- æ­£ç¡®ä½¿ç”¨è¿™ä¸¤ä¸ªé…ç½®é€‰é¡¹å¯ä»¥æ˜¾è‘—æ”¹å–„Telegrafçš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨èµ„æºå—é™çš„ç¯å¢ƒä¸­æˆ–å½“ç›‘æ§ç³»ç»Ÿè§„æ¨¡è¾ƒå¤§æ—¶ã€‚\n\n#### ç¤ºä¾‹\n\nå‡è®¾ä½ ä½¿ç”¨Categrafç›‘æ§ç³»ç»Ÿæ€§èƒ½ï¼Œå¹¶ä½¿ç”¨`cpu`æ’ä»¶æ”¶é›†CPUä½¿ç”¨æƒ…å†µçš„æŒ‡æ ‡ã€‚å¦‚æœä½ åªå¯¹CPUçš„é—²ç½®æ—¶é—´å’Œç”¨æˆ·æ—¶é—´æ„Ÿå…´è¶£ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹é…ç½®ï¼š\n\n```toml\n[[instances]]\n  ## ä»…æ”¶é›†CPUçš„é—²ç½®æ—¶é—´å’Œç”¨æˆ·ä½¿ç”¨æ—¶é—´çš„æŒ‡æ ‡\n  metrics_include = [\"cpu_usage_idle\", \"cpu_usage_user\"]\n```\n\næˆ–è€…ï¼Œå¦‚æœä½ æƒ³æ”¶é›†æ‰€æœ‰CPUç›¸å…³æŒ‡æ ‡ï¼Œä½†æ’é™¤é—²ç½®æ—¶é—´å’Œç”¨æˆ·æ—¶é—´ï¼Œå¯ä»¥ä½¿ç”¨ï¼š\n\n```toml\n[[instances]]\n  ## æ’é™¤CPUçš„é—²ç½®æ—¶é—´å’Œç”¨æˆ·ä½¿ç”¨æ—¶é—´çš„æŒ‡æ ‡\n  metrics_exclude = [\"cpu_usage_idle\", \"cpu_usage_user\"]\n```\n\né€šè¿‡ç²¾ç»†æ§åˆ¶æŒ‡æ ‡çš„æ”¶é›†ï¼Œä½ å¯ä»¥ä¼˜åŒ–ç›‘æ§è®¾ç½®ï¼Œç¡®ä¿åªå¤„ç†å¯¹ä½ æœ€é‡è¦çš„ä¿¡æ¯ã€‚\n\n### æœåŠ¡å™¨æ ‡ç­¾\n\næœåŠ¡å™¨æ ‡ç­¾ç”¨äºæ ‡è¯†æŒ‡æ ‡æºæœåŠ¡å™¨ã€‚ä½ å¯ä»¥é€‰æ‹©é»˜è®¤ä½¿ç”¨supervisorçš„httpç«¯ç‚¹çš„`host:port`ï¼Œæˆ–è€…ä½ å¯ä»¥ä½¿ç”¨åœ¨supervisoré…ç½®æ–‡ä»¶ä¸­è®¾ç½®çš„supervisorçš„æ ‡è¯†å­—ç¬¦ä¸²ã€‚\n\n## æŒ‡æ ‡\n\n- supervisor_processes\n    - tagsï¼š\n        - sourceï¼ˆsupervisorå®ä¾‹çš„ä¸»æœºåæˆ–IPåœ°å€ï¼‰\n        - portï¼ˆsupervisorçš„HTTPæœåŠ¡å™¨ç«¯å£å·ï¼‰\n        - idï¼ˆsupervisorçš„æ ‡è¯†å­—ç¬¦ä¸²ï¼‰\n        - nameï¼ˆè¿›ç¨‹åï¼‰\n        - groupï¼ˆè¿›ç¨‹ç»„ï¼‰\n    - fieldsï¼š\n        - stateï¼ˆintï¼Œå‚è§å‚è€ƒè¡¨ï¼‰\n        - uptimeï¼ˆintï¼Œç§’ï¼‰\n        - pidï¼ˆintï¼Œå¯é€‰ï¼‰\n        - exitCodeï¼ˆintï¼Œå¯é€‰ï¼‰\n\n- supervisor_instance\n    - tagsï¼š\n        - sourceï¼ˆsupervisorå®ä¾‹çš„ä¸»æœºåæˆ–IPåœ°å€ï¼‰\n        - portï¼ˆsupervisorçš„HTTPæœåŠ¡å™¨ç«¯å£å·ï¼‰\n        - idï¼ˆsupervisorçš„æ ‡è¯†å­—ç¬¦ä¸²ï¼‰\n    - fieldsï¼š\n        - stateï¼ˆintï¼Œå‚è§å‚è€ƒè¡¨ï¼‰\n\n### Supervisorè¿›ç¨‹çŠ¶æ€å­—æ®µå‚è€ƒè¡¨\n\n| çŠ¶æ€ç   | çŠ¶æ€å      | æè¿°                                    |\n|------|----------|---------------------------------------|\n| 0    | STOPPED  | è¿›ç¨‹å› åœæ­¢è¯·æ±‚åœæ­¢äº†ï¼Œæˆ–è€…ä»æœªå¯åŠ¨ã€‚                    |\n| 10   | STARTING | è¿›ç¨‹å› å¯åŠ¨è¯·æ±‚æ­£åœ¨å¯åŠ¨ã€‚                          |\n| 20   | RUNNING  | è¿›ç¨‹æ­£åœ¨è¿è¡Œã€‚                               |\n| 30   | BACKOFF  | è¿›ç¨‹è¿›å…¥STARTINGçŠ¶æ€ä½†éšåè¿‡å¿«é€€å‡ºï¼Œæœªèƒ½ç§»åŠ¨åˆ°RUNNINGçŠ¶æ€ã€‚ |\n| 40   | STOPPING | è¿›ç¨‹å› åœæ­¢è¯·æ±‚æ­£åœ¨åœæ­¢ã€‚                          |\n| 100  | EXITED   | è¿›ç¨‹å·²ä»RUNNINGçŠ¶æ€é€€å‡ºï¼ˆé¢„æœŸåœ°æˆ–æ„å¤–åœ°ï¼‰ã€‚             |\n| 200  | FATAL    | æ— æ³•æˆåŠŸå¯åŠ¨è¿›ç¨‹ã€‚                             |\n| 1000 | UNKNOWN  | è¿›ç¨‹å¤„äºæœªçŸ¥çŠ¶æ€ï¼ˆsupervisordç¼–ç¨‹é”™è¯¯ï¼‰ã€‚            |\n\n### Supervisorå®ä¾‹çŠ¶æ€å­—æ®µå‚è€ƒ\n\n| çŠ¶æ€ç  | çŠ¶æ€å     | æè¿°                 |\n|-----|---------|--------------------|\n| 2   | FATAL   | Supervisoré‡åˆ°äº†ä¸¥é‡é”™è¯¯ã€‚ |\n| 1   | RUNNING | Supervisoræ­£åœ¨æ­£å¸¸å·¥ä½œã€‚  |\n| 0   |         |                    |', 'é€šè¿‡XML-RPC APIé‡‡é›†supervisorä¸‹è¿è¡Œçš„è¿›ç¨‹', '1732765165', 'ç‹æ¨(822032277)', '1732765165', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('97', 'Processes', '# # collect interval\n# interval = 15\n\n# # force use ps command to gather\n# force_ps = false\n\n# # force use /proc to gather\n# force_proc = false', '# processes\n\nç»Ÿè®¡è¿›ç¨‹æ•°é‡ï¼Œæ¯”å¦‚ running çš„æœ‰å¤šå°‘ï¼Œsleeping çš„æœ‰å¤šå°‘ï¼Œtotal æœ‰å¤šå°‘\n\n## ç›‘æ§å¤§ç›˜\n\nè¯¥æ’ä»¶æ²¡æœ‰å•ç‹¬çš„ç›‘æ§å¤§ç›˜ï¼ŒOS çš„ç›‘æ§å¤§ç›˜ç»Ÿä¸€æ”¾åˆ° system ä¸‹é¢äº†', 'ç›‘æ§è¿›ç¨‹æ•°é‡', '1732765569', 'ç‹æ¨(822032277)', '1732765569', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('98', 'ClickHouse', '# # collect interval\n# interval = 15\n\n# Read metrics from one or many ClickHouse servers\n[[instances]]\n  ## Username for authorization on ClickHouse server\n  username = \"default\"\n\n  ## Password for authorization on ClickHouse server\n  # password = \"\"\n\n  ## HTTP(s) timeout while getting metrics values\n  ## The timeout includes connection time, any redirects, and reading the\n  ## response body.\n  # timeout = 5\n\n  ## List of servers for metrics scraping\n  ## metrics scrape via HTTP(s) clickhouse interface\n  ## https://clickhouse.tech/docs/en/interfaces/http/\n  # servers = [\"http://127.0.0.1:8123\"]\n\n  ## If \"auto_discovery\"\" is \"true\" plugin tries to connect to all servers\n  ## available in the cluster with using same \"user:password\" described in\n  ## \"user\" and \"password\" parameters and get this server hostname list from\n  ## \"system.clusters\" table. See\n  ## - https://clickhouse.tech/docs/en/operations/system_tables/#system-clusters\n  ## - https://clickhouse.tech/docs/en/operations/server_settings/settings/#server_settings_remote_servers\n  ## - https://clickhouse.tech/docs/en/operations/table_engines/distributed/\n  ## - https://clickhouse.tech/docs/en/operations/table_engines/replication/#creating-replicated-tables\n  # auto_discovery = true\n\n  ## Filter cluster names in \"system.clusters\" when \"auto_discovery\" is \"true\"\n  ## when this filter present then \"WHERE cluster IN (...)\" filter will apply\n  ## please use only full cluster names here, regexp and glob filters is not\n  ## allowed for \"/etc/clickhouse-server/config.d/remote.xml\"\n  ## <yandex>\n  ##  <remote_servers>\n  ##    <my-own-cluster>\n  ##        <shard>\n  ##          <replica><host>clickhouse-ru-1.local</host><port>9000</port></replica>\n  ##          <replica><host>clickhouse-ru-2.local</host><port>9000</port></replica>\n  ##        </shard>\n  ##        <shard>\n  ##          <replica><host>clickhouse-eu-1.local</host><port>9000</port></replica>\n  ##          <replica><host>clickhouse-eu-2.local</host><port>9000</port></replica>\n  ##        </shard>\n  ##    </my-onw-cluster>\n  ##  </remote_servers>\n  ##\n  ## </yandex>\n  ##\n  ## example: cluster_include = [\"my-own-cluster\"]\n  # cluster_include = []\n\n  ## Filter cluster names in \"system.clusters\" when \"auto_discovery\" is\n  ## \"true\" when this filter present then \"WHERE cluster NOT IN (...)\"\n  ## filter will apply\n  ##    example: cluster_exclude = [\"my-internal-not-discovered-cluster\"]\n  # cluster_exclude = []\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n    # [[instances.metrics]]\n    # mesurement = \"sessions\"\n    # label_fields = [ \"status\", \"type\" ]\n    # metric_fields = [ \"value\" ]\n    # timeout = \"3s\"\n    # request = \'\'\'\n    # SELECT status, type, COUNT(*) as value FROM v$session GROUP BY status, type\n    # \'\'\'', '# ClickHouse Input Plugin\n\nThis plugin gathers the statistic data from\n[ClickHouse](https://github.com/ClickHouse/ClickHouse) server.\n\n## Global configuration options\n\nIn addition to the plugin-specific configuration settings, plugins support\nadditional global and plugin configuration settings. These settings are used to\nmodify metrics, tags, and field or create aliases and configure ordering, etc.\nSee the [CONFIGURATION.md][CONFIGURATION.md] for more details.\n\n## Configuration\n\n```toml\n# # collect interval\n# interval = 15\n\n# Read metrics from one or many ClickHouse servers\n[[instances]]\n  ## Username for authorization on ClickHouse server\n  username = \"default\"\n\n  ## Password for authorization on ClickHouse server\n  # password = \"\"\n\n  ## HTTP(s) timeout while getting metrics values\n  ## The timeout includes connection time, any redirects, and reading the\n  ## response body.\n  # timeout = 5\n\n  ## List of servers for metrics scraping\n  ## metrics scrape via HTTP(s) clickhouse interface\n  ## https://clickhouse.tech/docs/en/interfaces/http/\n  servers = [\"http://127.0.0.1:8123\"]\n\n  ## If \"auto_discovery\"\" is \"true\" plugin tries to connect to all servers\n  ## available in the cluster with using same \"user:password\" described in\n  ## \"user\" and \"password\" parameters and get this server hostname list from\n  ## \"system.clusters\" table. See\n  ## - https://clickhouse.tech/docs/en/operations/system_tables/#system-clusters\n  ## - https://clickhouse.tech/docs/en/operations/server_settings/settings/#server_settings_remote_servers\n  ## - https://clickhouse.tech/docs/en/operations/table_engines/distributed/\n  ## - https://clickhouse.tech/docs/en/operations/table_engines/replication/#creating-replicated-tables\n  # auto_discovery = true\n\n  ## Filter cluster names in \"system.clusters\" when \"auto_discovery\" is \"true\"\n  ## when this filter present then \"WHERE cluster IN (...)\" filter will apply\n  ## please use only full cluster names here, regexp and glob filters is not\n  ## allowed for \"/etc/clickhouse-server/config.d/remote.xml\"\n  ## <yandex>\n  ##  <remote_servers>\n  ##    <my-own-cluster>\n  ##        <shard>\n  ##          <replica><host>clickhouse-ru-1.local</host><port>9000</port></replica>\n  ##          <replica><host>clickhouse-ru-2.local</host><port>9000</port></replica>\n  ##        </shard>\n  ##        <shard>\n  ##          <replica><host>clickhouse-eu-1.local</host><port>9000</port></replica>\n  ##          <replica><host>clickhouse-eu-2.local</host><port>9000</port></replica>\n  ##        </shard>\n  ##    </my-onw-cluster>\n  ##  </remote_servers>\n  ##\n  ## </yandex>\n  ##\n  ## example: cluster_include = [\"my-own-cluster\"]\n  # cluster_include = []\n\n  ## Filter cluster names in \"system.clusters\" when \"auto_discovery\" is\n  ## \"true\" when this filter present then \"WHERE cluster NOT IN (...)\"\n  ## filter will apply\n  ##    example: cluster_exclude = [\"my-internal-not-discovered-cluster\"]\n  # cluster_exclude = []\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain & host verification\n  # insecure_skip_verify = false\n\n```\n\n## Metrics\n\n- clickhouse_events (see [system.events][system.events] for details)\n\n    - tags:\n        - source (ClickHouse server hostname)\n        - cluster (Name of the cluster [optional])\n        - shard_num (Shard number in the cluster [optional])\n    - fields:\n        - all rows from [system.events][system.events]\n- clickhouse_metrics (see [system.metrics][system.metrics] for details)\n\n    - tags:\n        - source (ClickHouse server hostname)\n        - cluster (Name of the cluster [optional])\n        - shard_num (Shard number in the cluster [optional])\n    - fields:\n        - all rows from [system.metrics][system.metrics]\n- clickhouse_asynchronous_metrics (see [system.asynchronous_metrics][system.asynchronous_metrics]\n  for details)\n\n    - tags:\n        - source (ClickHouse server hostname)\n        - cluster (Name of the cluster [optional])\n        - shard_num (Shard number in the cluster [optional])\n    - fields:\n        - all rows from [system.asynchronous_metrics][system.asynchronous_metrics]\n- clickhouse_tables\n\n    - tags:\n        - source (ClickHouse server hostname)\n        - table\n        - database\n        - cluster (Name of the cluster [optional])\n        - shard_num (Shard number in the cluster [optional])\n    - fields:\n        - bytes\n        - parts\n        - rows\n- clickhouse_zookeeper (see [system.zookeeper][system.zookeeper] for details)\n\n    - tags:\n        - source (ClickHouse server hostname)\n        - cluster (Name of the cluster [optional])\n        - shard_num (Shard number in the cluster [optional])\n    - fields:\n        - root_nodes (count of node where path=/)\n- clickhouse_replication_queue (see [system.replication_queue][system.replication_queue] for details)\n\n    - tags:\n        - source (ClickHouse server hostname)\n        - cluster (Name of the cluster [optional])\n        - shard_num (Shard number in the cluster [optional])\n    - fields:\n        - too_many_tries_replicas (count of replicas which have `num_tries > 1`)\n- clickhouse_detached_parts (see [system.detached_parts][system.detached_parts] for details)\n\n    - tags:\n        - source (ClickHouse server hostname)\n        - cluster (Name of the cluster [optional])\n        - shard_num (Shard number in the cluster [optional])\n    - fields:\n        - detached_parts (total detached parts for all tables and databases\n          from [system.detached_parts][system.detached_parts])\n- clickhouse_dictionaries (see [system.dictionaries][system.dictionaries] for details)\n\n    - tags:\n        - source (ClickHouse server hostname)\n        - cluster (Name of the cluster [optional])\n        - shard_num (Shard number in the cluster [optional])\n        - dict_origin (xml Filename when dictionary created from *_dictionary.xml,\n          database.table when dictionary created from DDL)\n    - fields:\n        - is_loaded (0 - when dictionary data not successful load, 1 - when\n          dictionary data loading fail\n        - bytes_allocated (bytes allocated in RAM after a dictionary loaded)\n- clickhouse_mutations (see [system.mutations][system.mutations] for details)\n\n    - tags:\n        - source (ClickHouse server hostname)\n        - cluster (Name of the cluster [optional])\n        - shard_num (Shard number in the cluster [optional])\n    - fields:\n        - running - gauge which show how much mutation doesn\'t complete now\n        - failed - counter which show total failed mutations from first\n          clickhouse-server run\n        - completed - counter which show total successful finished mutations\n          from first clickhouse-server run\n- clickhouse_disks (see [system.disks][system.disks] for details)\n\n    - tags:\n        - source (ClickHouse server hostname)\n        - cluster (Name of the cluster [optional])\n        - shard_num (Shard number in the cluster [optional])\n        - name (disk name in storage configuration)\n        - path (path to disk)\n    - fields:\n        - free_space_percent - 0-100, gauge which show current percent of\n          free disk space bytes relative to total disk space bytes\n        - keep_free_space_percent - 0-100, gauge which show current percent\n          of required keep free disk bytes relative to total disk space bytes\n- clickhouse_processes (see [system.processes][system.processes] for details)\n\n    - tags:\n        - source (ClickHouse server hostname)\n        - cluster (Name of the cluster [optional])\n        - shard_num (Shard number in the cluster [optional])\n    - fields:\n        - percentile_50 - float gauge which show 50% percentile (quantile 0.5) for\n          `elapsed` field of running processes\n        - percentile_90 - float gauge which show 90% percentile (quantile 0.9) for\n          `elapsed` field of running processes\n        - longest_running - float gauge which show maximum value for `elapsed`\n          field of running processes\n- clickhouse_text_log (see [system.text_log][system.text_log] for details)\n\n    - tags:\n        - source (ClickHouse server hostname)\n        - cluster (Name of the cluster [optional])\n        - shard_num (Shard number in the cluster [optional])\n        - level (message level, only messages with level less or equal Notice are\n          collected)\n    - fields:\n        - messages_last_10_min - gauge which show how many messages collected\n\n## Example Output\n\n```text\nclickhouse_events,cluster=test_cluster_two_shards_localhost,host=kshvakov,source=localhost,shard_num=1 read_compressed_bytes=212i,arena_alloc_chunks=35i,function_execute=85i,merge_tree_data_writer_rows=3i,rw_lock_acquired_read_locks=421i,file_open=46i,io_buffer_alloc_bytes=86451985i,inserted_bytes=196i,regexp_created=3i,real_time_microseconds=116832i,query=23i,network_receive_elapsed_microseconds=268i,merge_tree_data_writer_compressed_bytes=1080i,arena_alloc_bytes=212992i,disk_write_elapsed_microseconds=556i,inserted_rows=3i,compressed_read_buffer_bytes=81i,read_buffer_from_file_descriptor_read_bytes=148i,write_buffer_from_file_descriptor_write=47i,merge_tree_data_writer_blocks=3i,soft_page_faults=896i,hard_page_faults=7i,select_query=21i,merge_tree_data_writer_uncompressed_bytes=196i,merge_tree_data_writer_blocks_already_sorted=3i,user_time_microseconds=40196i,compressed_read_buffer_blocks=5i,write_buffer_from_file_descriptor_write_bytes=3246i,io_buffer_allocs=296i,created_write_buffer_ordinary=12i,disk_read_elapsed_microseconds=59347044i,network_send_elapsed_microseconds=1538i,context_lock=1040i,insert_query=1i,system_time_microseconds=14582i,read_buffer_from_file_descriptor_read=3i 1569421000000000000\nclickhouse_asynchronous_metrics,cluster=test_cluster_two_shards_localhost,host=kshvakov,source=localhost,shard_num=1 jemalloc.metadata_thp=0i,replicas_max_relative_delay=0i,jemalloc.mapped=1803177984i,jemalloc.allocated=1724839256i,jemalloc.background_thread.run_interval=0i,jemalloc.background_thread.num_threads=0i,uncompressed_cache_cells=0i,replicas_max_absolute_delay=0i,mark_cache_bytes=0i,compiled_expression_cache_count=0i,replicas_sum_queue_size=0i,number_of_tables=35i,replicas_max_merges_in_queue=0i,replicas_max_inserts_in_queue=0i,replicas_sum_merges_in_queue=0i,replicas_max_queue_size=0i,mark_cache_files=0i,jemalloc.background_thread.num_runs=0i,jemalloc.active=1726210048i,uptime=158i,jemalloc.retained=380481536i,replicas_sum_inserts_in_queue=0i,uncompressed_cache_bytes=0i,number_of_databases=2i,jemalloc.metadata=9207704i,max_part_count_for_partition=1i,jemalloc.resident=1742442496i 1569421000000000000\nclickhouse_metrics,cluster=test_cluster_two_shards_localhost,host=kshvakov,source=localhost,shard_num=1 replicated_send=0i,write=0i,ephemeral_node=0i,zoo_keeper_request=0i,distributed_files_to_insert=0i,replicated_fetch=0i,background_schedule_pool_task=0i,interserver_connection=0i,leader_replica=0i,delayed_inserts=0i,global_thread_active=41i,merge=0i,readonly_replica=0i,memory_tracking_in_background_schedule_pool=0i,memory_tracking_for_merges=0i,zoo_keeper_session=0i,context_lock_wait=0i,storage_buffer_bytes=0i,background_pool_task=0i,send_external_tables=0i,zoo_keeper_watch=0i,part_mutation=0i,disk_space_reserved_for_merge=0i,distributed_send=0i,version_integer=19014003i,local_thread=0i,replicated_checks=0i,memory_tracking=0i,memory_tracking_in_background_processing_pool=0i,leader_election=0i,revision=54425i,open_file_for_read=0i,open_file_for_write=0i,storage_buffer_rows=0i,rw_lock_waiting_readers=0i,rw_lock_waiting_writers=0i,rw_lock_active_writers=0i,local_thread_active=0i,query_preempted=0i,tcp_connection=1i,http_connection=1i,read=2i,query_thread=0i,dict_cache_requests=0i,rw_lock_active_readers=1i,global_thread=43i,query=1i 1569421000000000000\nclickhouse_tables,cluster=test_cluster_two_shards_localhost,database=system,host=kshvakov,source=localhost,shard_num=1,table=trace_log bytes=754i,parts=1i,rows=1i 1569421000000000000\nclickhouse_tables,cluster=test_cluster_two_shards_localhost,database=default,host=kshvakov,source=localhost,shard_num=1,table=example bytes=326i,parts=2i,rows=2i 1569421000000000000\n```\n\n[CONFIGURATION.md]: ../../../docs/CONFIGURATION.md#plugins\n[system.asynchronous_metrics]: https://clickhouse.tech/docs/en/operations/system-tables/asynchronous_metrics/\n[system.detached_parts]: https://clickhouse.tech/docs/en/operations/system-tables/detached_parts/\n[system.dictionaries]: https://clickhouse.tech/docs/en/operations/system-tables/dictionaries/\n[system.disks]: https://clickhouse.tech/docs/en/operations/system-tables/disks/\n[system.events]: https://clickhouse.tech/docs/en/operations/system-tables/events/\n[system.metrics]: https://clickhouse.tech/docs/en/operations/system-tables/metrics/\n[system.mutations]: https://clickhouse.tech/docs/en/operations/system-tables/mutations/\n[system.processes]: https://clickhouse.tech/docs/en/operations/system-tables/processes/\n[system.replication_queue]: https://clickhouse.com/docs/en/operations/system-tables/replication_queue/\n[system.text_log]: https://clickhouse.tech/docs/en/operations/system-tables/text_log/\n[system.zookeeper]: https://clickhouse.tech/docs/en/operations/system-tables/zookeeper/', 'ClickHouseç›‘æ§æŒ‡æ ‡é‡‡é›†', '1732765730', 'ç‹æ¨(822032277)', '1732765730', 'ç‹æ¨(822032277)');
INSERT INTO `plugin_tpl` VALUES ('99', 'Redis_Sentinel', '# # collect interval\n# interval = 15\n\n[[instances]]\n# [protocol://][:password]@address[:port]\n# e.g. servers = [\"tcp://localhost:26379\"]\nservers = []\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n# add some dimension data by labels\n# labels = {}\n\n## Optional TLS Config\n# use_tls = false\n# tls_min_version = \"1.2\"\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = true', '# Redis Sentinel Input Plugin\n\nA plugin for Redis Sentinel to monitor multiple Sentinel instances that are\nmonitoring multiple Redis servers and replicas.\n\n## Configuration\n\n```toml\n[[instances]]\n# [protocol://][:password]@address[:port]\n# e.g. servers = [\"tcp://localhost:26379\"]\nservers = []\n\n# # interval = global.interval * interval_times\n# interval_times = 1\n# add some dimension data by labels\n# labels = {}\n\n## Optional TLS Config\n# use_tls = false\n# tls_min_version = \"1.2\"\n# tls_ca = \"/etc/categraf/ca.pem\"\n# tls_cert = \"/etc/categraf/cert.pem\"\n# tls_key = \"/etc/categraf/key.pem\"\n## Use TLS but skip chain & host verification\n# insecure_skip_verify = true\n```\n\n## Measurements & Fields\n\nThe plugin gathers the results of these commands and measurements:\n\n* `sentinel masters` - `redis_sentinel_masters`\n* `sentinel sentinels` - `redis_sentinels`\n* `sentinel replicas` - `redis_replicas`\n* `info all` - `redis_sentinel`\n\nThe `has_quorum` field in `redis_sentinel_masters` is from calling the command\n`sentinels ckquorum`.\n\nThere are 5 remote network requests made for each server listed in the config.\n\n## Metrics\n\n* redis_sentinel_masters\n  * tags:\n    * host\n    * master\n    * port\n    * source\n\n  * fields:\n    * config_epoch (int)\n    * down_after_milliseconds (int)\n    * failover_timeout (int)\n    * flags (string)\n    * has_quorum (bool)\n    * info_refresh (int)\n    * ip (string)\n    * last_ok_ping_reply (int)\n    * last_ping_reply (int)\n    * last_ping_sent (int)\n    * link_pending_commands (int)\n    * link_refcount (int)\n    * num_other_sentinels (int)\n    * num_slaves (int)\n    * parallel_syncs (int)\n    * port (int)\n    * quorum (int)\n    * role_reported (string)\n    * role_reported_time (int)\n\n* redis_sentinel_sentinels\n  * tags:\n    * host\n    * master\n    * port\n    * sentinel_ip\n    * sentinel_port\n    * source\n\n  * fields:\n    * down_after_milliseconds (int)\n    * flags (string)\n    * last_hello_message (int)\n    * last_ok_ping_reply (int)\n    * last_ping_reply (int)\n    * last_ping_sent (int)\n    * link_pending_commands (int)\n    * link_refcount (int)\n    * name (string)\n    * voted_leader (string)\n    * voted_leader_epoch (int)\n\n* redis_sentinel_replicas\n  * tags:\n    * host\n    * master\n    * port\n    * replica_ip\n    * replica_port\n    * source\n\n  * fields:\n    * down_after_milliseconds (int)\n    * flags (string)\n    * info_refresh (int)\n    * last_ok_ping_reply (int)\n    * last_ping_reply (int)\n    * last_ping_sent (int)\n    * link_pending_commands (int)\n    * link_refcount (int)\n    * master_host (string)\n    * master_link_down_time (int)\n    * master_link_status (string)\n    * master_port (int)\n    * name (string)\n    * role_reported (string)\n    * role_reported_time (int)\n    * slave_priority (int)\n    * slave_repl_offset (int)\n\n* redis_sentinel\n  * tags:\n    * host\n    * port\n    * source\n\n  * fields:\n    * active_defrag_hits (int)\n    * active_defrag_key_hits (int)\n    * active_defrag_key_misses (int)\n    * active_defrag_misses (int)\n    * blocked_clients (int)\n    * client_recent_max_input_buffer (int)\n    * client_recent_max_output_buffer (int)\n    * clients (int)\n    * evicted_keys (int)\n    * expired_keys (int)\n    * expired_stale_perc (float)\n    * expired_time_cap_reached_count (int)\n    * instantaneous_input_kbps (float)\n    * instantaneous_ops_per_sec (int)\n    * instantaneous_output_kbps (float)\n    * keyspace_hits (int)\n    * keyspace_misses (int)\n    * latest_fork_usec (int)\n    * lru_clock (int)\n    * migrate_cached_sockets (int)\n    * pubsub_channels (int)\n    * pubsub_patterns (int)\n    * redis_version (string)\n    * rejected_connections (int)\n    * sentinel_masters (int)\n    * sentinel_running_scripts (int)\n    * sentinel_scripts_queue_length (int)\n    * sentinel_simulate_failure_flags (int)\n    * sentinel_tilt (int)\n    * slave_expires_tracked_keys (int)\n    * sync_full (int)\n    * sync_partial_err (int)\n    * sync_partial_ok (int)\n    * total_commands_processed (int)\n    * total_connections_received (int)\n    * total_net_input_bytes (int)\n    * total_net_output_bytes (int)\n    * uptime_ns (int, nanoseconds)\n    * used_cpu_sys (float)\n    * used_cpu_sys_children (float)\n    * used_cpu_user (float)\n    * used_cpu_user_children (float)\n\n', 'Redis_Sentinelç›‘æ§æŒ‡æ ‡é‡‡é›†', '1733126558', 'ç‹æ¨(822032277)', '1733126558', 'ç‹æ¨(822032277)');
